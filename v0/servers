{"servers":[{"name":"microsoft/markitdown","description":"Convert various file formats (PDF, Word, Excel, images, audio) to Markdown.","status":"active","repository":{"url":"https://github.com/microsoft/markitdown","source":"github","id":"888092115","readme":"# MarkItDown\n\n[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)\n![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)\n[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)\n\n\u003e [!TIP]\n\u003e MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.\n\n\u003e [!IMPORTANT]\n\u003e Breaking changes between 0.0.1 to 0.1.0:\n\u003e * Dependencies are now organized into optional feature-groups (further details below). Use `pip install 'markitdown[all]'` to have backward-compatible behavior. \n\u003e * convert\\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.\n\u003e * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.\n\nMarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.\n\nMarkItDown currently supports the conversion from:\n\n- PDF\n- PowerPoint\n- Word\n- Excel\n- Images (EXIF metadata and OCR)\n- Audio (EXIF metadata and speech transcription)\n- HTML\n- Text-based formats (CSV, JSON, XML)\n- ZIP files (iterates over contents)\n- Youtube URLs\n- EPubs\n- ... and more!\n\n## Why Markdown?\n\nMarkdown is extremely close to plain text, with minimal markup or formatting, but still\nprovides a way to represent important document structure. Mainstream LLMs, such as\nOpenAI's GPT-4o, natively \"_speak_\" Markdown, and often incorporate Markdown into their\nresponses unprompted. This suggests that they have been trained on vast amounts of\nMarkdown-formatted text, and understand it well. As a side benefit, Markdown conventions\nare also highly token-efficient.\n\n## Prerequisites\nMarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.\n\nWith the standard Python installation, you can create and activate a virtual environment using the following commands:\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\n```\n\nIf using `uv`, you can create a virtual environment with:\n\n```bash\nuv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment\n```\n\nIf you are using Anaconda, you can create a virtual environment with:\n\n```bash\nconda create -n markitdown python=3.12\nconda activate markitdown\n```\n\n## Installation\n\nTo install MarkItDown, use pip: `pip install 'markitdown[all]'`. Alternatively, you can install it from the source:\n\n```bash\ngit clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'\n```\n\n## Usage\n\n### Command-Line\n\n```bash\nmarkitdown path-to-file.pdf \u003e document.md\n```\n\nOr use `-o` to specify the output file:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md\n```\n\nYou can also pipe content:\n\n```bash\ncat path-to-file.pdf | markitdown\n```\n\n### Optional Dependencies\nMarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:\n\n```bash\npip install 'markitdown[pdf, docx, pptx]'\n```\n\nwill install only the dependencies for PDF, DOCX, and PPTX files.\n\nAt the moment, the following optional dependencies are available:\n\n* `[all]` Installs all optional dependencies\n* `[pptx]` Installs dependencies for PowerPoint files\n* `[docx]` Installs dependencies for Word files\n* `[xlsx]` Installs dependencies for Excel files\n* `[xls]` Installs dependencies for older Excel files\n* `[pdf]` Installs dependencies for PDF files\n* `[outlook]` Installs dependencies for Outlook messages\n* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence\n* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files\n* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription\n\n### Plugins\n\nMarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:\n\n```bash\nmarkitdown --list-plugins\n```\n\nTo enable plugins use:\n\n```bash\nmarkitdown --use-plugins path-to-file.pdf\n```\n\nTo find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.\n\n### Azure Document Intelligence\n\nTo use Microsoft Document Intelligence for conversion:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md -d -e \"\u003cdocument_intelligence_endpoint\u003e\"\n```\n\nMore information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)\n\n### Python API\n\nBasic usage in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False) # Set to True to enable plugins\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)\n```\n\nDocument Intelligence conversion in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"\u003cdocument_intelligence_endpoint\u003e\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)\n```\n\nTo use Large Language Models for image descriptions (currently only for pptx and image files), provide `llm_client` and `llm_model`:\n\n```python\nfrom markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)\n```\n\n### Docker\n\n```sh\ndocker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest \u003c ~/your-file.pdf \u003e output.md\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### How to Contribute\n\nYou can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.\n\n\u003cdiv align=\"center\"\u003e\n\n|            | All                                                          | Especially Needs Help from Community                                                                                                      |\n| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |\n| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |\n| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |\n\n\u003c/div\u003e\n\n### Running Tests and Checks\n\n- Navigate to the MarkItDown package:\n\n  ```sh\n  cd packages/markitdown\n  ```\n\n- Install `hatch` in your environment and run tests:\n\n  ```sh\n  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test\n  ```\n\n  (Alternative) Use the Devcontainer which has all the dependencies installed:\n\n  ```sh\n  # Reopen the project in Devcontainer and run:\n  hatch test\n  ```\n\n- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`\n\n### Contributing 3rd-party Plugins\n\nYou can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark \u0026 Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:03Z","updated_at":"2025-10-22T11:21:18Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"pypi","identifier":"markitdown-mcp","version":"0.0.1a4","runtime_hint":"binary"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"976a2f68-e16c-4e2b-9709-7133487f8c14","is_latest":true,"published_at":"2025-09-09T10:55:22.907101Z","updated_at":"2025-09-09T10:55:22.907101Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Markitdown","is_in_organization":true,"license":"MIT License","name":"markitdown","name_with_owner":"microsoft/markitdown","opengraph_image_url":"https://opengraph.githubassets.com/8d2ecce5906372369d51aad2174daf989ece5b7b69632353c517f602043ead2d/microsoft/markitdown","owner_avatar_url":"https://avatars.githubusercontent.com/u/6154722?v=4","preferred_image":"https://avatars.githubusercontent.com/u/6154722?v=4","primary_language":"Python","primary_language_color":"#3572A5","pushed_at":"2025-10-20T23:07:42Z","readme":"# MarkItDown\n\n[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)\n![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)\n[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)\n\n\u003e [!TIP]\n\u003e MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.\n\n\u003e [!IMPORTANT]\n\u003e Breaking changes between 0.0.1 to 0.1.0:\n\u003e * Dependencies are now organized into optional feature-groups (further details below). Use `pip install 'markitdown[all]'` to have backward-compatible behavior. \n\u003e * convert\\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.\n\u003e * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.\n\nMarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.\n\nMarkItDown currently supports the conversion from:\n\n- PDF\n- PowerPoint\n- Word\n- Excel\n- Images (EXIF metadata and OCR)\n- Audio (EXIF metadata and speech transcription)\n- HTML\n- Text-based formats (CSV, JSON, XML)\n- ZIP files (iterates over contents)\n- Youtube URLs\n- EPubs\n- ... and more!\n\n## Why Markdown?\n\nMarkdown is extremely close to plain text, with minimal markup or formatting, but still\nprovides a way to represent important document structure. Mainstream LLMs, such as\nOpenAI's GPT-4o, natively \"_speak_\" Markdown, and often incorporate Markdown into their\nresponses unprompted. This suggests that they have been trained on vast amounts of\nMarkdown-formatted text, and understand it well. As a side benefit, Markdown conventions\nare also highly token-efficient.\n\n## Prerequisites\nMarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.\n\nWith the standard Python installation, you can create and activate a virtual environment using the following commands:\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\n```\n\nIf using `uv`, you can create a virtual environment with:\n\n```bash\nuv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment\n```\n\nIf you are using Anaconda, you can create a virtual environment with:\n\n```bash\nconda create -n markitdown python=3.12\nconda activate markitdown\n```\n\n## Installation\n\nTo install MarkItDown, use pip: `pip install 'markitdown[all]'`. Alternatively, you can install it from the source:\n\n```bash\ngit clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'\n```\n\n## Usage\n\n### Command-Line\n\n```bash\nmarkitdown path-to-file.pdf \u003e document.md\n```\n\nOr use `-o` to specify the output file:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md\n```\n\nYou can also pipe content:\n\n```bash\ncat path-to-file.pdf | markitdown\n```\n\n### Optional Dependencies\nMarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:\n\n```bash\npip install 'markitdown[pdf, docx, pptx]'\n```\n\nwill install only the dependencies for PDF, DOCX, and PPTX files.\n\nAt the moment, the following optional dependencies are available:\n\n* `[all]` Installs all optional dependencies\n* `[pptx]` Installs dependencies for PowerPoint files\n* `[docx]` Installs dependencies for Word files\n* `[xlsx]` Installs dependencies for Excel files\n* `[xls]` Installs dependencies for older Excel files\n* `[pdf]` Installs dependencies for PDF files\n* `[outlook]` Installs dependencies for Outlook messages\n* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence\n* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files\n* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription\n\n### Plugins\n\nMarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:\n\n```bash\nmarkitdown --list-plugins\n```\n\nTo enable plugins use:\n\n```bash\nmarkitdown --use-plugins path-to-file.pdf\n```\n\nTo find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.\n\n### Azure Document Intelligence\n\nTo use Microsoft Document Intelligence for conversion:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md -d -e \"\u003cdocument_intelligence_endpoint\u003e\"\n```\n\nMore information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)\n\n### Python API\n\nBasic usage in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False) # Set to True to enable plugins\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)\n```\n\nDocument Intelligence conversion in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"\u003cdocument_intelligence_endpoint\u003e\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)\n```\n\nTo use Large Language Models for image descriptions (currently only for pptx and image files), provide `llm_client` and `llm_model`:\n\n```python\nfrom markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)\n```\n\n### Docker\n\n```sh\ndocker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest \u003c ~/your-file.pdf \u003e output.md\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### How to Contribute\n\nYou can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.\n\n\u003cdiv align=\"center\"\u003e\n\n|            | All                                                          | Especially Needs Help from Community                                                                                                      |\n| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |\n| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |\n| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |\n\n\u003c/div\u003e\n\n### Running Tests and Checks\n\n- Navigate to the MarkItDown package:\n\n  ```sh\n  cd packages/markitdown\n  ```\n\n- Install `hatch` in your environment and run tests:\n\n  ```sh\n  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test\n  ```\n\n  (Alternative) Use the Devcontainer which has all the dependencies installed:\n\n  ```sh\n  # Reopen the project in Devcontainer and run:\n  hatch test\n  ```\n\n- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`\n\n### Contributing 3rd-party Plugins\n\nYou can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark \u0026 Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n","stargazer_count":81964,"topics":["langchain","openai","autogen-extension","autogen","markdown","microsoft-office","pdf"],"uses_custom_opengraph_image":false}}}},{"name":"upstash/context7","description":"Get up-to-date, version-specific documentation and code examples from any library or framework.","status":"active","repository":{"url":"https://github.com/upstash/context7","source":"github","id":"955620917","readme":"![Cover](public/cover.png)\n\n# Context7 MCP - Up-to-date Code Docs For Any Prompt\n\n[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [![NPM Version](https://img.shields.io/npm/v/%40upstash%2Fcontext7-mcp?color=red)](https://www.npmjs.com/package/@upstash/context7-mcp) [![MIT licensed](https://img.shields.io/npm/l/%40upstash%2Fcontext7-mcp)](./LICENSE)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D) [\u003cimg alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/Install%20in%20VS%20Code-0098FF?style=for-the-badge\u0026logo=visualstudiocode\u0026logoColor=white\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\n[![繁體中文](https://img.shields.io/badge/docs-繁體中文-yellow)](./docs/README.zh-TW.md) [![简体中文](https://img.shields.io/badge/docs-简体中文-yellow)](./docs/README.zh-CN.md) [![日本語](https://img.shields.io/badge/docs-日本語-b7003a)](./docs/README.ja.md) [![한국어 문서](https://img.shields.io/badge/docs-한국어-green)](./docs/README.ko.md) [![Documentación en Español](https://img.shields.io/badge/docs-Español-orange)](./docs/README.es.md) [![Documentation en Français](https://img.shields.io/badge/docs-Français-blue)](./docs/README.fr.md) [![Documentação em Português (Brasil)](\u003chttps://img.shields.io/badge/docs-Português%20(Brasil)-purple\u003e)](./docs/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./docs/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![Документация на русском языке](https://img.shields.io/badge/docs-Русский-darkblue)](./docs/README.ru.md) [![Українська документація](https://img.shields.io/badge/docs-Українська-lightblue)](./docs/README.uk.md) [![Türkçe Doküman](https://img.shields.io/badge/docs-Türkçe-blue)](./docs/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./docs/README.ar.md) [![Tiếng Việt](https://img.shields.io/badge/docs-Tiếng%20Việt-red)](./docs/README.vi.md)\n\n## ❌ Without Context7\n\nLLMs rely on outdated or generic information about the libraries you use. You get:\n\n- ❌ Code examples are outdated and based on year-old training data\n- ❌ Hallucinated APIs that don't even exist\n- ❌ Generic answers for old package versions\n\n## ✅ With Context7\n\nContext7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source — and places them directly into your prompt.\n\nAdd `use context7` to your prompt in Cursor:\n\n```txt\nCreate a Next.js middleware that checks for a valid JWT in cookies and redirects unauthenticated users to `/login`. use context7\n```\n\n```txt\nConfigure a Cloudflare Worker script to cache JSON API responses for five minutes. use context7\n```\n\nContext7 fetches up-to-date code examples and documentation right into your LLM's context.\n\n- 1️⃣ Write your prompt naturally\n- 2️⃣ Tell the LLM to `use context7`\n- 3️⃣ Get working code answers\n\nNo tab-switching, no hallucinated APIs that don't exist, no outdated code generation.\n\n## 📚 Adding Projects\n\nCheck out our [project addition guide](./docs/adding-projects.md) to learn how to add (or update) your favorite libraries to Context7.\n\n## 🛠️ Installation\n\n### Requirements\n\n- Node.js \u003e= v18.0.0\n- Cursor, Claude Code, VSCode, Windsurf or another MCP Client\n- Context7 API Key (Optional) for higher rate limits and private repositories (Get yours by creating an account at [context7.com/dashboard](https://context7.com/dashboard))\n\n\u003e [!WARNING]\n\u003e **SSE Protocol Deprecation Notice**\n\u003e\n\u003e The Server-Sent Events (SSE) transport protocol is deprecated and its endpoint will be removed in upcoming releases. Please use HTTP or stdio transport methods instead.\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstalling via Smithery\u003c/b\u003e\u003c/summary\u003e\n\nTo install Context7 MCP Server for any client automatically via [Smithery](https://smithery.ai/server/@upstash/context7-mcp):\n\n```bash\nnpx -y @smithery/cli@latest install @upstash/context7-mcp --client \u003cCLIENT_NAME\u003e --key \u003cYOUR_SMITHERY_KEY\u003e\n```\n\nYou can find your Smithery key in the [Smithery.ai webpage](https://smithery.ai/server/@upstash/context7-mcp).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Cursor\u003c/b\u003e\u003c/summary\u003e\n\nGo to: `Settings` -\u003e `Cursor Settings` -\u003e `MCP` -\u003e `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n\u003e Since Cursor 1.0, you can click the install button below for instant one-click installation.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Claude Code\u003c/b\u003e\u003c/summary\u003e\n\nRun this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --transport http context7 https://mcp.context7.com/mcp --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n```\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Amp\u003c/b\u003e\u003c/summary\u003e\n\nRun this command in your terminal. See [Amp MCP docs](https://ampcode.com/manual#mcp) for more info.\n\n#### Without API Key (Basic Usage)\n\n```sh\namp mcp add context7 https://mcp.context7.com/mcp\n```\n\n#### With API Key (Higher Rate Limits \u0026 Private Repos)\n\n```sh\namp mcp add context7 --header \"CONTEXT7_API_KEY=YOUR_API_KEY\" https://mcp.context7.com/mcp\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Windsurf\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Windsurf Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"serverUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Windsurf Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in VS Code\u003c/b\u003e\u003c/summary\u003e\n\n[\u003cimg alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Context7%20MCP\u0026color=0098FF\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n[\u003cimg alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Context7%20MCP\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\n\u003cb\u003eInstall in Cline\u003c/b\u003e\n\u003c/summary\u003e\n\nYou can easily install Context7 through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (☰) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Context7_.\n4. Click the **Install** button.\n\nOr you can directly edit MCP servers configuration:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (☰) to enter the **MCP Servers** section.\n3. Choose **Remote Servers** tab.\n4. Click the **Edit Configuration** button.\n5. Add context7 to `mcpServers`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"type\": \"streamableHttp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Zed\u003c/b\u003e\u003c/summary\u003e\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Context7) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Context7\": {\n      \"source\": \"custom\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Augment Code\u003c/b\u003e\u003c/summary\u003e\n\nTo configure Context7 MCP in Augment Code, you can use either the graphical interface or manual configuration.\n\n### **A. Using the Augment Code UI**\n\n1. Click the hamburger menu.\n2. Select **Settings**.\n3. Navigate to the **Tools** section.\n4. Click the **+ Add MCP** button.\n5. Enter the following command:\n\n   ```\n   npx -y @upstash/context7-mcp@latest\n   ```\n\n6. Name the MCP: **Context7**.\n7. Click the **Add** button.\n\nOnce the MCP server is added, you can start using Context7's up-to-date code documentation features directly within Augment Code.\n\n---\n\n### **B. Manual Configuration**\n\n1. Press Cmd/Ctrl Shift P or go to the hamburger menu in the Augment panel\n2. Select Edit Settings\n3. Under Advanced, click Edit in settings.json\n4. Add the server configuration to the `mcpServers` array in the `augment.advanced` object\n\n```json\n\"augment.advanced\": {\n  \"mcpServers\": [\n    {\n      \"name\": \"context7\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  ]\n}\n```\n\nOnce the MCP server is added, restart your editor. If you receive any errors, check the syntax to make sure closing brackets or commas are not missing.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Roo Code\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Gemini CLI\u003c/b\u003e\u003c/summary\u003e\n\nSee [Gemini CLI Configuration](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for details.\n\n1.  Open the Gemini CLI settings file. The location is `~/.gemini/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Qwen Coder\u003c/b\u003e\u003c/summary\u003e\n\nSee [Qwen Coder MCP Configuration](https://qwenlm.github.io/qwen-code-docs/en/tools/mcp-server/#how-to-set-up-your-mcp-server) for details.\n\n1.  Open the Qwen Coder settings file. The location is `~/.qwen/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Claude Desktop\u003c/b\u003e\u003c/summary\u003e\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings \u003e Connectors \u003e Add Custom Connector. Enter the name as `Context7` and the remote MCP server URL as `https://mcp.context7.com/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Opencode\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Opencode configuration file. See [Opencode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### Opencode Remote Server Connection\n\n```json\n\"mcp\": {\n  \"context7\": {\n    \"type\": \"remote\",\n    \"url\": \"https://mcp.context7.com/mcp\",\n    \"headers\": {\n      \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n    },\n    \"enabled\": true\n  }\n}\n```\n\n#### Opencode Local Server Connection\n\n```json\n{\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in OpenAI Codex\u003c/b\u003e\u003c/summary\u003e\n\nSee [OpenAI Codex](https://github.com/openai/codex) for more information.\n\nAdd the following configuration to your OpenAI Codex MCP server settings:\n\n```toml\n[mcp_servers.context7]\nargs = [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\ncommand = \"npx\"\n```\n\n⚠️ Windows Notes\n\nOn Windows, some users may encounter request timed out errors with the default configuration.\nIn that case, explicitly configure the MCP server with the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"\nargs = [\n  \"C:\\\\Users\\\\yourname\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@upstash\\\\context7-mcp\\\\dist\\\\index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\n\nAlternatively, you can use the following configuration:\n\n```toml\n[mcp_servers.context7]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"@upstash/context7-mcp\",\n    \"--api-key\",\n    \"YOUR_API_KEY\"\n]\nenv = { SystemRoot=\"C:\\\\Windows\" }\nstartup_timeout_ms = 20_000\n```\n\nThis ensures Codex CLI works reliably on Windows.\n\n⚠️ MacOS Notes\n\nOn MacOS, some users may encounter the same request timed out errors like Windows,\nit also can be solved with the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"/Users/yourname/.nvm/versions/node/v22.14.0/bin/node\"  # Node.js full path\nargs = [\"/Users/yourname/.nvm/versions/node/v22.14.0/lib/node_modules/@upstash/context7-mcp/dist/index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\n\nThis ensures Codex CLI works reliably on MacOS.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003e\u003cb\u003eInstall in JetBrains AI Assistant\u003c/b\u003e\u003c/summary\u003e\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs, go to `Settings` -\u003e `Tools` -\u003e `AI Assistant` -\u003e `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way context7 could be added for JetBrains Junie in `Settings` -\u003e `Tools` -\u003e `Junie` -\u003e `MCP Settings`\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \n\u003csummary\u003e\u003cb\u003eInstall in Kiro\u003c/b\u003e\u003c/summary\u003e\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` \u003e `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Trae\u003c/b\u003e\u003c/summary\u003e\n\nUse the Add manually feature and fill in the JSON configuration information for that MCP server.\nFor more details, visit the [Trae documentation](https://docs.trae.ai/ide/model-context-protocol?_lang=en).\n\n#### Trae Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Trae Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eUsing Bun or Deno\u003c/b\u003e\u003c/summary\u003e\n\nUse these alternatives to run the local Context7 MCP server with other runtimes. These examples work for any client that supports launching a local MCP server via command + args.\n\n#### Bun\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Deno\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-env=NO_DEPRECATION,TRACE_DEPRECATION\",\n        \"--allow-net\",\n        \"npm:@upstash/context7-mcp\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eUsing Docker\u003c/b\u003e\u003c/summary\u003e\n\nIf you prefer to run the MCP server in a Docker container:\n\n1. **Build the Docker Image:**\n\n   First, create a `Dockerfile` in the project root (or anywhere you prefer):\n\n   \u003cdetails\u003e\n   \u003csummary\u003eClick to see Dockerfile content\u003c/summary\u003e\n\n   ```Dockerfile\n   FROM node:18-alpine\n\n   WORKDIR /app\n\n   # Install the latest version globally\n   RUN npm install -g @upstash/context7-mcp\n\n   # Expose default port if needed (optional, depends on MCP client interaction)\n   # EXPOSE 3000\n\n   # Default command to run the server\n   CMD [\"context7-mcp\"]\n   ```\n\n   \u003c/details\u003e\n\n   Then, build the image using a tag (e.g., `context7-mcp`). **Make sure Docker Desktop (or the Docker daemon) is running.** Run the following command in the same directory where you saved the `Dockerfile`:\n\n   ```bash\n   docker build -t context7-mcp .\n   ```\n\n2. **Configure Your MCP Client:**\n\n   Update your MCP client's configuration to use the Docker command.\n\n   _Example for a cline_mcp_settings.json:_\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"Сontext7\": {\n         \"autoApprove\": [],\n         \"disabled\": false,\n         \"timeout\": 60,\n         \"command\": \"docker\",\n         \"args\": [\"run\", \"-i\", \"--rm\", \"context7-mcp\"],\n         \"transportType\": \"stdio\"\n       }\n     }\n   }\n   ```\n\n   _Note: This is an example configuration. Please refer to the specific examples for your MCP client (like Cursor, VS Code, etc.) earlier in this README to adapt the structure (e.g., `mcpServers` vs `servers`). Also, ensure the image name in `args` matches the tag used during the `docker build` command._\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall Using the Desktop Extension\u003c/b\u003e\u003c/summary\u003e\n\nInstall the [context7.mcpb](mcpb/context7.mcpb) file under the mcpb folder and add it to your client. For more information, please check out [MCP bundles docs](https://github.com/anthropics/mcpb#mcp-bundles-mcpb).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Windows\u003c/b\u003e\u003c/summary\u003e\n\nThe configuration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/upstash/context7-mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Amazon Q Developer CLI\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Amazon Q Developer CLI configuration file. See [Amazon Q Developer CLI docs](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Warp\u003c/b\u003e\u003c/summary\u003e\n\nSee [Warp Model Context Protocol Documentation](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server) for details.\n\n1. Navigate `Settings` \u003e `AI` \u003e `Manage MCP servers`.\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"Context7\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n    \"env\": {},\n    \"working_directory\": null,\n    \"start_on_launch\": true\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003e\u003cb\u003eInstall in Copilot Coding Agent\u003c/b\u003e\u003c/summary\u003e\n\n## Using Context7 with Copilot Coding Agent\n\nAdd the following configuration to the `mcp` section of your Copilot Coding Agent configuration file Repository-\u003eSettings-\u003eCopilot-\u003eCoding agent-\u003eMCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      },\n      \"tools\": [\"get-library-docs\", \"resolve-library-id\"]\n    }\n  }\n}\n```\n\nFor more information, see the [official GitHub documentation](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in LM Studio\u003c/b\u003e\u003c/summary\u003e\n\nSee [LM Studio MCP Support](https://lmstudio.ai/blog/lmstudio-v0.3.17) for more information.\n\n#### One-click install:\n\n[![Add MCP Server context7 to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=context7\u0026config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkB1cHN0YXNoL2NvbnRleHQ3LW1jcCJdfQ%3D%3D)\n\n#### Manual set-up:\n\n1. Navigate to `Program` (right side) \u003e `Install` \u003e `Edit mcp.json`.\n2. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n3. Click `Save` to apply the changes.\n4. Toggle the MCP server on/off from the right hand side, under `Program`, or by clicking the plug icon at the bottom of the chat box.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Visual Studio 2022\u003c/b\u003e\u003c/summary\u003e\n\nYou can configure Context7 MCP in Visual Studio 2022 by following the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\nAdd this to your Visual Studio MCP config file (see the [Visual Studio docs](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022) for details):\n\n```json\n{\n  \"inputs\": [],\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"context7\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      }\n    }\n  }\n}\n```\n\nFor more information and troubleshooting, refer to the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Crush\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Crush configuration file. See [Crush MCP docs](https://github.com/charmbracelet/crush#mcps) for more info.\n\n#### Crush Remote Server Connection (HTTP)\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Crush Local Server Connection\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in BoltAI\u003c/b\u003e\u003c/summary\u003e\n\nOpen the \"Settings\" page of the app, navigate to \"Plugins,\" and enter the following JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nOnce saved, enter in the chat `get-library-docs` followed by your Context7 documentation ID (e.g., `get-library-docs /nuxt/ui`). More information is available on [BoltAI's Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Rovo Dev CLI\u003c/b\u003e\u003c/summary\u003e\n\nEdit your Rovo Dev CLI MCP config by running the command below -\n\n```bash\nacli rovodev mcp\n```\n\nExample config -\n\n#### Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Zencoder\u003c/b\u003e\u003c/summary\u003e\n\nTo configure Context7 MCP in Zencoder, follow these steps:\n\n1. Go to the Zencoder menu (...)\n2. From the dropdown menu, select Agent tools\n3. Click on the Add custom MCP\n4. Add the name and server configuration from below, and make sure to hit the Install button\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n}\n```\n\nOnce the MCP server is added, you can easily continue using it.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Qodo Gen\u003c/b\u003e\u003c/summary\u003e\n\nSee [Qodo Gen docs](https://docs.qodo.ai/qodo-documentation/qodo-gen/qodo-gen-chat/agentic-mode/agentic-tools-mcps) for more details.\n\n1. Open Qodo Gen chat panel in VSCode or IntelliJ.\n2. Click Connect more tools.\n3. Click + Add new MCP.\n4. Add the following configuration:\n\n#### Qodo Gen Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Qodo Gen Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Perplexity Desktop\u003c/b\u003e\u003c/summary\u003e\n\nSee [Local and Remote MCPs for Perplexity](https://www.perplexity.ai/help-center/en/articles/11502712-local-and-remote-mcps-for-perplexity) for more information.\n\n1. Navigate `Perplexity` \u003e `Settings`\n2. Select `Connectors`.\n3. Click `Add Connector`.\n4. Select `Advanced`.\n5. Enter Server Name: `Context7`\n6. Paste the following JSON in the text area:\n\n```json\n{\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n  \"command\": \"npx\",\n  \"env\": {}\n}\n```\n\n7. Click `Save`.\n\u003c/details\u003e\n\n## 🔨 Available Tools\n\nContext7 MCP provides the following tools that LLMs can use:\n\n- `resolve-library-id`: Resolves a general library name into a Context7-compatible library ID.\n  - `libraryName` (required): The name of the library to search for\n\n- `get-library-docs`: Fetches documentation for a library using a Context7-compatible library ID.\n  - `context7CompatibleLibraryID` (required): Exact Context7-compatible library ID (e.g., `/mongodb/docs`, `/vercel/next.js`)\n  - `topic` (optional): Focus the docs on a specific topic (e.g., \"routing\", \"hooks\")\n  - `tokens` (optional, default 5000): Max number of tokens to return. Values less than 1000 are automatically increased to 1000.\n\n## 🛟 Tips\n\n### Add a Rule\n\nIf you don’t want to add `use context7` to every prompt, you can define a simple rule in your MCP client's rule section:\n\n- For Windsurf, in `.windsurfrules` file\n- For Cursor, from `Cursor Settings \u003e Rules` section\n- For Claude Code, in `CLAUDE.md` file\n\nOr the equivalent in your MCP client to auto-invoke Context7 on any code question.\n\n#### Example Rule\n\n```txt\nAlways use context7 when I need code generation, setup or configuration steps, or\nlibrary/API documentation. This means you should automatically use the Context7 MCP\ntools to resolve library id and get library docs without me having to explicitly ask.\n```\n\nFrom then on, you’ll get Context7’s docs in any related conversation without typing anything extra. You can alter the rule to match your use cases.\n\n### Use Library Id\n\nIf you already know exactly which library you want to use, add its Context7 ID to your prompt. That way, Context7 MCP server can skip the library-matching step and directly continue with retrieving docs.\n\n```txt\nImplement basic authentication with Supabase. use library /supabase/supabase for API and docs.\n```\n\nThe slash syntax tells the MCP tool exactly which library to load docs for.\n\n### HTTPS Proxy\n\nIf you are behind an HTTP proxy, Context7 uses the standard `https_proxy` / `HTTPS_PROXY` environment variables.\n\n## 💻 Development\n\nClone the project and install dependencies:\n\n```bash\nbun i\n```\n\nBuild:\n\n```bash\nbun run build\n```\n\nRun the server:\n\n```bash\nbun run dist/index.js\n```\n\n### CLI Arguments\n\n`context7-mcp` accepts the following CLI flags:\n\n- `--transport \u003cstdio|http\u003e` – Transport to use (`stdio` by default). Note that HTTP transport automatically provides both HTTP and SSE endpoints.\n- `--port \u003cnumber\u003e` – Port to listen on when using `http` transport (default `3000`).\n- `--api-key \u003ckey\u003e` – API key for authentication (or set `CONTEXT7_API_KEY` env var). You can get your API key by creating an account at [context7.com/dashboard](https://context7.com/dashboard).\n\nExample with HTTP transport and port 8080:\n\n```bash\nbun run dist/index.js --transport http --port 8080\n```\n\nAnother example with stdio transport:\n\n```bash\nbun run dist/index.js --transport stdio --api-key YOUR_API_KEY\n```\n\n### Environment Variables\n\nYou can use the `CONTEXT7_API_KEY` environment variable instead of passing the `--api-key` flag. This is useful for:\n\n- Storing API keys securely in `.env` files\n- Integration with MCP server setups that use dotenv\n- Tools that prefer environment variable configuration\n\n**Note:** The `--api-key` CLI flag takes precedence over the environment variable when both are provided.\n\n**Example with .env file:**\n\n```bash\n# .env\nCONTEXT7_API_KEY=your_api_key_here\n```\n\n**Example MCP configuration using environment variable:**\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"],\n      \"env\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eLocal Configuration Example\u003c/b\u003e\u003c/summary\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"/path/to/folder/context7/src/index.ts\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTesting with MCP Inspector\u003c/b\u003e\u003c/summary\u003e\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx @upstash/context7-mcp\n```\n\n\u003c/details\u003e\n\n## 🚨 Troubleshooting\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eModule Not Found Errors\u003c/b\u003e\u003c/summary\u003e\n\nIf you encounter `ERR_MODULE_NOT_FOUND`, try using `bunx` instead of `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\nThis often resolves module resolution issues in environments where `npx` doesn't properly install or resolve packages.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eESM Resolution Issues\u003c/b\u003e\u003c/summary\u003e\n\nFor errors like `Error: Cannot find module 'uriTemplate.js'`, try the `--experimental-vm-modules` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-vm-modules\", \"@upstash/context7-mcp@1.0.6\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTLS/Certificate Issues\u003c/b\u003e\u003c/summary\u003e\n\nUse the `--experimental-fetch` flag to bypass TLS-related problems:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-fetch\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eGeneral MCP Client Errors\u003c/b\u003e\u003c/summary\u003e\n\n1. Try adding `@latest` to the package name\n2. Use `bunx` as an alternative to `npx`\n3. Consider using `deno` as another alternative\n4. Ensure you're using Node.js v18 or higher for native fetch support\n\n\u003c/details\u003e\n\n## ⚠️ Disclaimer\n\nContext7 projects are community-contributed and while we strive to maintain high quality, we cannot guarantee the accuracy, completeness, or security of all library documentation. Projects listed in Context7 are developed and maintained by their respective owners, not by Context7. If you encounter any suspicious, inappropriate, or potentially harmful content, please use the \"Report\" button on the project page to notify us immediately. We take all reports seriously and will review flagged content promptly to maintain the integrity and safety of our platform. By using Context7, you acknowledge that you do so at your own discretion and risk.\n\n## 🤝 Connect with Us\n\nStay updated and join our community:\n\n- 📢 Follow us on [X](https://x.com/context7ai) for the latest news and updates\n- 🌐 Visit our [Website](https://context7.com)\n- 💬 Join our [Discord Community](https://upstash.com/discord)\n\n## 📺 Context7 In Media\n\n- [Better Stack: \"Free Tool Makes Cursor 10x Smarter\"](https://youtu.be/52FC3qObp9E)\n- [Cole Medin: \"This is Hands Down the BEST MCP Server for AI Coding Assistants\"](https://www.youtube.com/watch?v=G7gK8H6u7Rs)\n- [Income Stream Surfers: \"Context7 + SequentialThinking MCPs: Is This AGI?\"](https://www.youtube.com/watch?v=-ggvzyLpK6o)\n- [Julian Goldie SEO: \"Context7: New MCP AI Agent Update\"](https://www.youtube.com/watch?v=CTZm6fBYisc)\n- [JeredBlu: \"Context 7 MCP: Get Documentation Instantly + VS Code Setup\"](https://www.youtube.com/watch?v=-ls0D-rtET4)\n- [Income Stream Surfers: \"Context7: The New MCP Server That Will CHANGE AI Coding\"](https://www.youtube.com/watch?v=PS-2Azb-C3M)\n- [AICodeKing: \"Context7 + Cline \u0026 RooCode: This MCP Server Makes CLINE 100X MORE EFFECTIVE!\"](https://www.youtube.com/watch?v=qZfENAPMnyo)\n- [Sean Kochel: \"5 MCP Servers For Vibe Coding Glory (Just Plug-In \u0026 Go)\"](https://www.youtube.com/watch?v=LqTQi8qexJM)\n\n## ⭐ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=upstash/context7\u0026type=Date)](https://www.star-history.com/#upstash/context7\u0026Date)\n\n## 📄 License\n\nMIT\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:50Z","updated_at":"2025-10-22T11:21:06Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.context7.com/mcp","headers":[{"value":"{context7_api_key}","variables":{"context7_api_key":{"description":"Context7 API key (optional; increases rate limits). Get one at https://context7.com/dashboard","is_secret":true}},"name":"CONTEXT7_API_KEY"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"dcec7705-b81b-4e0f-8615-8032604be7ad","is_latest":true,"published_at":"2025-09-09T10:55:22.907116Z","updated_at":"2025-09-09T10:55:22.907116Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Context7","homepage_url":"https://context7.com","is_in_organization":true,"license":"MIT License","name":"context7","name_with_owner":"upstash/context7","opengraph_image_url":"https://opengraph.githubassets.com/2c711fcd5c975d1b7f91fa55460c2d3e0881b45bfaf46c1ddf4c0506d70b2e04/upstash/context7","owner_avatar_url":"https://avatars.githubusercontent.com/u/74989412?v=4","preferred_image":"https://avatars.githubusercontent.com/u/74989412?v=4","primary_language":"JavaScript","primary_language_color":"#f1e05a","pushed_at":"2025-10-20T14:41:08Z","readme":"![Cover](public/cover.png)\n\n# Context7 MCP - Up-to-date Code Docs For Any Prompt\n\n[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [![NPM Version](https://img.shields.io/npm/v/%40upstash%2Fcontext7-mcp?color=red)](https://www.npmjs.com/package/@upstash/context7-mcp) [![MIT licensed](https://img.shields.io/npm/l/%40upstash%2Fcontext7-mcp)](./LICENSE)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D) [\u003cimg alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/Install%20in%20VS%20Code-0098FF?style=for-the-badge\u0026logo=visualstudiocode\u0026logoColor=white\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\n[![繁體中文](https://img.shields.io/badge/docs-繁體中文-yellow)](./docs/README.zh-TW.md) [![简体中文](https://img.shields.io/badge/docs-简体中文-yellow)](./docs/README.zh-CN.md) [![日本語](https://img.shields.io/badge/docs-日本語-b7003a)](./docs/README.ja.md) [![한국어 문서](https://img.shields.io/badge/docs-한국어-green)](./docs/README.ko.md) [![Documentación en Español](https://img.shields.io/badge/docs-Español-orange)](./docs/README.es.md) [![Documentation en Français](https://img.shields.io/badge/docs-Français-blue)](./docs/README.fr.md) [![Documentação em Português (Brasil)](\u003chttps://img.shields.io/badge/docs-Português%20(Brasil)-purple\u003e)](./docs/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./docs/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![Документация на русском языке](https://img.shields.io/badge/docs-Русский-darkblue)](./docs/README.ru.md) [![Українська документація](https://img.shields.io/badge/docs-Українська-lightblue)](./docs/README.uk.md) [![Türkçe Doküman](https://img.shields.io/badge/docs-Türkçe-blue)](./docs/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./docs/README.ar.md) [![Tiếng Việt](https://img.shields.io/badge/docs-Tiếng%20Việt-red)](./docs/README.vi.md)\n\n## ❌ Without Context7\n\nLLMs rely on outdated or generic information about the libraries you use. You get:\n\n- ❌ Code examples are outdated and based on year-old training data\n- ❌ Hallucinated APIs that don't even exist\n- ❌ Generic answers for old package versions\n\n## ✅ With Context7\n\nContext7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source — and places them directly into your prompt.\n\nAdd `use context7` to your prompt in Cursor:\n\n```txt\nCreate a Next.js middleware that checks for a valid JWT in cookies and redirects unauthenticated users to `/login`. use context7\n```\n\n```txt\nConfigure a Cloudflare Worker script to cache JSON API responses for five minutes. use context7\n```\n\nContext7 fetches up-to-date code examples and documentation right into your LLM's context.\n\n- 1️⃣ Write your prompt naturally\n- 2️⃣ Tell the LLM to `use context7`\n- 3️⃣ Get working code answers\n\nNo tab-switching, no hallucinated APIs that don't exist, no outdated code generation.\n\n## 📚 Adding Projects\n\nCheck out our [project addition guide](./docs/adding-projects.md) to learn how to add (or update) your favorite libraries to Context7.\n\n## 🛠️ Installation\n\n### Requirements\n\n- Node.js \u003e= v18.0.0\n- Cursor, Claude Code, VSCode, Windsurf or another MCP Client\n- Context7 API Key (Optional) for higher rate limits and private repositories (Get yours by creating an account at [context7.com/dashboard](https://context7.com/dashboard))\n\n\u003e [!WARNING]\n\u003e **SSE Protocol Deprecation Notice**\n\u003e\n\u003e The Server-Sent Events (SSE) transport protocol is deprecated and its endpoint will be removed in upcoming releases. Please use HTTP or stdio transport methods instead.\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstalling via Smithery\u003c/b\u003e\u003c/summary\u003e\n\nTo install Context7 MCP Server for any client automatically via [Smithery](https://smithery.ai/server/@upstash/context7-mcp):\n\n```bash\nnpx -y @smithery/cli@latest install @upstash/context7-mcp --client \u003cCLIENT_NAME\u003e --key \u003cYOUR_SMITHERY_KEY\u003e\n```\n\nYou can find your Smithery key in the [Smithery.ai webpage](https://smithery.ai/server/@upstash/context7-mcp).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Cursor\u003c/b\u003e\u003c/summary\u003e\n\nGo to: `Settings` -\u003e `Cursor Settings` -\u003e `MCP` -\u003e `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n\u003e Since Cursor 1.0, you can click the install button below for instant one-click installation.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7\u0026config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Claude Code\u003c/b\u003e\u003c/summary\u003e\n\nRun this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --transport http context7 https://mcp.context7.com/mcp --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n```\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Amp\u003c/b\u003e\u003c/summary\u003e\n\nRun this command in your terminal. See [Amp MCP docs](https://ampcode.com/manual#mcp) for more info.\n\n#### Without API Key (Basic Usage)\n\n```sh\namp mcp add context7 https://mcp.context7.com/mcp\n```\n\n#### With API Key (Higher Rate Limits \u0026 Private Repos)\n\n```sh\namp mcp add context7 --header \"CONTEXT7_API_KEY=YOUR_API_KEY\" https://mcp.context7.com/mcp\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Windsurf\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Windsurf Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"serverUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Windsurf Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in VS Code\u003c/b\u003e\u003c/summary\u003e\n\n[\u003cimg alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Context7%20MCP\u0026color=0098FF\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n[\u003cimg alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Context7%20MCP\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\n\u003cb\u003eInstall in Cline\u003c/b\u003e\n\u003c/summary\u003e\n\nYou can easily install Context7 through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (☰) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Context7_.\n4. Click the **Install** button.\n\nOr you can directly edit MCP servers configuration:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (☰) to enter the **MCP Servers** section.\n3. Choose **Remote Servers** tab.\n4. Click the **Edit Configuration** button.\n5. Add context7 to `mcpServers`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"type\": \"streamableHttp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Zed\u003c/b\u003e\u003c/summary\u003e\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Context7) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Context7\": {\n      \"source\": \"custom\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Augment Code\u003c/b\u003e\u003c/summary\u003e\n\nTo configure Context7 MCP in Augment Code, you can use either the graphical interface or manual configuration.\n\n### **A. Using the Augment Code UI**\n\n1. Click the hamburger menu.\n2. Select **Settings**.\n3. Navigate to the **Tools** section.\n4. Click the **+ Add MCP** button.\n5. Enter the following command:\n\n   ```\n   npx -y @upstash/context7-mcp@latest\n   ```\n\n6. Name the MCP: **Context7**.\n7. Click the **Add** button.\n\nOnce the MCP server is added, you can start using Context7's up-to-date code documentation features directly within Augment Code.\n\n---\n\n### **B. Manual Configuration**\n\n1. Press Cmd/Ctrl Shift P or go to the hamburger menu in the Augment panel\n2. Select Edit Settings\n3. Under Advanced, click Edit in settings.json\n4. Add the server configuration to the `mcpServers` array in the `augment.advanced` object\n\n```json\n\"augment.advanced\": {\n  \"mcpServers\": [\n    {\n      \"name\": \"context7\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  ]\n}\n```\n\nOnce the MCP server is added, restart your editor. If you receive any errors, check the syntax to make sure closing brackets or commas are not missing.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Roo Code\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Gemini CLI\u003c/b\u003e\u003c/summary\u003e\n\nSee [Gemini CLI Configuration](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for details.\n\n1.  Open the Gemini CLI settings file. The location is `~/.gemini/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Qwen Coder\u003c/b\u003e\u003c/summary\u003e\n\nSee [Qwen Coder MCP Configuration](https://qwenlm.github.io/qwen-code-docs/en/tools/mcp-server/#how-to-set-up-your-mcp-server) for details.\n\n1.  Open the Qwen Coder settings file. The location is `~/.qwen/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Claude Desktop\u003c/b\u003e\u003c/summary\u003e\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings \u003e Connectors \u003e Add Custom Connector. Enter the name as `Context7` and the remote MCP server URL as `https://mcp.context7.com/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Opencode\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Opencode configuration file. See [Opencode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### Opencode Remote Server Connection\n\n```json\n\"mcp\": {\n  \"context7\": {\n    \"type\": \"remote\",\n    \"url\": \"https://mcp.context7.com/mcp\",\n    \"headers\": {\n      \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n    },\n    \"enabled\": true\n  }\n}\n```\n\n#### Opencode Local Server Connection\n\n```json\n{\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in OpenAI Codex\u003c/b\u003e\u003c/summary\u003e\n\nSee [OpenAI Codex](https://github.com/openai/codex) for more information.\n\nAdd the following configuration to your OpenAI Codex MCP server settings:\n\n```toml\n[mcp_servers.context7]\nargs = [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\ncommand = \"npx\"\n```\n\n⚠️ Windows Notes\n\nOn Windows, some users may encounter request timed out errors with the default configuration.\nIn that case, explicitly configure the MCP server with the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"\nargs = [\n  \"C:\\\\Users\\\\yourname\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@upstash\\\\context7-mcp\\\\dist\\\\index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\n\nAlternatively, you can use the following configuration:\n\n```toml\n[mcp_servers.context7]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"@upstash/context7-mcp\",\n    \"--api-key\",\n    \"YOUR_API_KEY\"\n]\nenv = { SystemRoot=\"C:\\\\Windows\" }\nstartup_timeout_ms = 20_000\n```\n\nThis ensures Codex CLI works reliably on Windows.\n\n⚠️ MacOS Notes\n\nOn MacOS, some users may encounter the same request timed out errors like Windows,\nit also can be solved with the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"/Users/yourname/.nvm/versions/node/v22.14.0/bin/node\"  # Node.js full path\nargs = [\"/Users/yourname/.nvm/versions/node/v22.14.0/lib/node_modules/@upstash/context7-mcp/dist/index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\n\nThis ensures Codex CLI works reliably on MacOS.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003e\u003cb\u003eInstall in JetBrains AI Assistant\u003c/b\u003e\u003c/summary\u003e\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs, go to `Settings` -\u003e `Tools` -\u003e `AI Assistant` -\u003e `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way context7 could be added for JetBrains Junie in `Settings` -\u003e `Tools` -\u003e `Junie` -\u003e `MCP Settings`\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \n\u003csummary\u003e\u003cb\u003eInstall in Kiro\u003c/b\u003e\u003c/summary\u003e\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` \u003e `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Trae\u003c/b\u003e\u003c/summary\u003e\n\nUse the Add manually feature and fill in the JSON configuration information for that MCP server.\nFor more details, visit the [Trae documentation](https://docs.trae.ai/ide/model-context-protocol?_lang=en).\n\n#### Trae Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Trae Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eUsing Bun or Deno\u003c/b\u003e\u003c/summary\u003e\n\nUse these alternatives to run the local Context7 MCP server with other runtimes. These examples work for any client that supports launching a local MCP server via command + args.\n\n#### Bun\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Deno\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-env=NO_DEPRECATION,TRACE_DEPRECATION\",\n        \"--allow-net\",\n        \"npm:@upstash/context7-mcp\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eUsing Docker\u003c/b\u003e\u003c/summary\u003e\n\nIf you prefer to run the MCP server in a Docker container:\n\n1. **Build the Docker Image:**\n\n   First, create a `Dockerfile` in the project root (or anywhere you prefer):\n\n   \u003cdetails\u003e\n   \u003csummary\u003eClick to see Dockerfile content\u003c/summary\u003e\n\n   ```Dockerfile\n   FROM node:18-alpine\n\n   WORKDIR /app\n\n   # Install the latest version globally\n   RUN npm install -g @upstash/context7-mcp\n\n   # Expose default port if needed (optional, depends on MCP client interaction)\n   # EXPOSE 3000\n\n   # Default command to run the server\n   CMD [\"context7-mcp\"]\n   ```\n\n   \u003c/details\u003e\n\n   Then, build the image using a tag (e.g., `context7-mcp`). **Make sure Docker Desktop (or the Docker daemon) is running.** Run the following command in the same directory where you saved the `Dockerfile`:\n\n   ```bash\n   docker build -t context7-mcp .\n   ```\n\n2. **Configure Your MCP Client:**\n\n   Update your MCP client's configuration to use the Docker command.\n\n   _Example for a cline_mcp_settings.json:_\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"Сontext7\": {\n         \"autoApprove\": [],\n         \"disabled\": false,\n         \"timeout\": 60,\n         \"command\": \"docker\",\n         \"args\": [\"run\", \"-i\", \"--rm\", \"context7-mcp\"],\n         \"transportType\": \"stdio\"\n       }\n     }\n   }\n   ```\n\n   _Note: This is an example configuration. Please refer to the specific examples for your MCP client (like Cursor, VS Code, etc.) earlier in this README to adapt the structure (e.g., `mcpServers` vs `servers`). Also, ensure the image name in `args` matches the tag used during the `docker build` command._\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall Using the Desktop Extension\u003c/b\u003e\u003c/summary\u003e\n\nInstall the [context7.mcpb](mcpb/context7.mcpb) file under the mcpb folder and add it to your client. For more information, please check out [MCP bundles docs](https://github.com/anthropics/mcpb#mcp-bundles-mcpb).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Windows\u003c/b\u003e\u003c/summary\u003e\n\nThe configuration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/upstash/context7-mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Amazon Q Developer CLI\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Amazon Q Developer CLI configuration file. See [Amazon Q Developer CLI docs](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Warp\u003c/b\u003e\u003c/summary\u003e\n\nSee [Warp Model Context Protocol Documentation](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server) for details.\n\n1. Navigate `Settings` \u003e `AI` \u003e `Manage MCP servers`.\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"Context7\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n    \"env\": {},\n    \"working_directory\": null,\n    \"start_on_launch\": true\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003e\u003cb\u003eInstall in Copilot Coding Agent\u003c/b\u003e\u003c/summary\u003e\n\n## Using Context7 with Copilot Coding Agent\n\nAdd the following configuration to the `mcp` section of your Copilot Coding Agent configuration file Repository-\u003eSettings-\u003eCopilot-\u003eCoding agent-\u003eMCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      },\n      \"tools\": [\"get-library-docs\", \"resolve-library-id\"]\n    }\n  }\n}\n```\n\nFor more information, see the [official GitHub documentation](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in LM Studio\u003c/b\u003e\u003c/summary\u003e\n\nSee [LM Studio MCP Support](https://lmstudio.ai/blog/lmstudio-v0.3.17) for more information.\n\n#### One-click install:\n\n[![Add MCP Server context7 to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=context7\u0026config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkB1cHN0YXNoL2NvbnRleHQ3LW1jcCJdfQ%3D%3D)\n\n#### Manual set-up:\n\n1. Navigate to `Program` (right side) \u003e `Install` \u003e `Edit mcp.json`.\n2. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n3. Click `Save` to apply the changes.\n4. Toggle the MCP server on/off from the right hand side, under `Program`, or by clicking the plug icon at the bottom of the chat box.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Visual Studio 2022\u003c/b\u003e\u003c/summary\u003e\n\nYou can configure Context7 MCP in Visual Studio 2022 by following the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\nAdd this to your Visual Studio MCP config file (see the [Visual Studio docs](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022) for details):\n\n```json\n{\n  \"inputs\": [],\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"context7\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      }\n    }\n  }\n}\n```\n\nFor more information and troubleshooting, refer to the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Crush\u003c/b\u003e\u003c/summary\u003e\n\nAdd this to your Crush configuration file. See [Crush MCP docs](https://github.com/charmbracelet/crush#mcps) for more info.\n\n#### Crush Remote Server Connection (HTTP)\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Crush Local Server Connection\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in BoltAI\u003c/b\u003e\u003c/summary\u003e\n\nOpen the \"Settings\" page of the app, navigate to \"Plugins,\" and enter the following JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nOnce saved, enter in the chat `get-library-docs` followed by your Context7 documentation ID (e.g., `get-library-docs /nuxt/ui`). More information is available on [BoltAI's Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Rovo Dev CLI\u003c/b\u003e\u003c/summary\u003e\n\nEdit your Rovo Dev CLI MCP config by running the command below -\n\n```bash\nacli rovodev mcp\n```\n\nExample config -\n\n#### Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Zencoder\u003c/b\u003e\u003c/summary\u003e\n\nTo configure Context7 MCP in Zencoder, follow these steps:\n\n1. Go to the Zencoder menu (...)\n2. From the dropdown menu, select Agent tools\n3. Click on the Add custom MCP\n4. Add the name and server configuration from below, and make sure to hit the Install button\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n}\n```\n\nOnce the MCP server is added, you can easily continue using it.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Qodo Gen\u003c/b\u003e\u003c/summary\u003e\n\nSee [Qodo Gen docs](https://docs.qodo.ai/qodo-documentation/qodo-gen/qodo-gen-chat/agentic-mode/agentic-tools-mcps) for more details.\n\n1. Open Qodo Gen chat panel in VSCode or IntelliJ.\n2. Click Connect more tools.\n3. Click + Add new MCP.\n4. Add the following configuration:\n\n#### Qodo Gen Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Qodo Gen Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eInstall in Perplexity Desktop\u003c/b\u003e\u003c/summary\u003e\n\nSee [Local and Remote MCPs for Perplexity](https://www.perplexity.ai/help-center/en/articles/11502712-local-and-remote-mcps-for-perplexity) for more information.\n\n1. Navigate `Perplexity` \u003e `Settings`\n2. Select `Connectors`.\n3. Click `Add Connector`.\n4. Select `Advanced`.\n5. Enter Server Name: `Context7`\n6. Paste the following JSON in the text area:\n\n```json\n{\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n  \"command\": \"npx\",\n  \"env\": {}\n}\n```\n\n7. Click `Save`.\n\u003c/details\u003e\n\n## 🔨 Available Tools\n\nContext7 MCP provides the following tools that LLMs can use:\n\n- `resolve-library-id`: Resolves a general library name into a Context7-compatible library ID.\n  - `libraryName` (required): The name of the library to search for\n\n- `get-library-docs`: Fetches documentation for a library using a Context7-compatible library ID.\n  - `context7CompatibleLibraryID` (required): Exact Context7-compatible library ID (e.g., `/mongodb/docs`, `/vercel/next.js`)\n  - `topic` (optional): Focus the docs on a specific topic (e.g., \"routing\", \"hooks\")\n  - `tokens` (optional, default 5000): Max number of tokens to return. Values less than 1000 are automatically increased to 1000.\n\n## 🛟 Tips\n\n### Add a Rule\n\nIf you don’t want to add `use context7` to every prompt, you can define a simple rule in your MCP client's rule section:\n\n- For Windsurf, in `.windsurfrules` file\n- For Cursor, from `Cursor Settings \u003e Rules` section\n- For Claude Code, in `CLAUDE.md` file\n\nOr the equivalent in your MCP client to auto-invoke Context7 on any code question.\n\n#### Example Rule\n\n```txt\nAlways use context7 when I need code generation, setup or configuration steps, or\nlibrary/API documentation. This means you should automatically use the Context7 MCP\ntools to resolve library id and get library docs without me having to explicitly ask.\n```\n\nFrom then on, you’ll get Context7’s docs in any related conversation without typing anything extra. You can alter the rule to match your use cases.\n\n### Use Library Id\n\nIf you already know exactly which library you want to use, add its Context7 ID to your prompt. That way, Context7 MCP server can skip the library-matching step and directly continue with retrieving docs.\n\n```txt\nImplement basic authentication with Supabase. use library /supabase/supabase for API and docs.\n```\n\nThe slash syntax tells the MCP tool exactly which library to load docs for.\n\n### HTTPS Proxy\n\nIf you are behind an HTTP proxy, Context7 uses the standard `https_proxy` / `HTTPS_PROXY` environment variables.\n\n## 💻 Development\n\nClone the project and install dependencies:\n\n```bash\nbun i\n```\n\nBuild:\n\n```bash\nbun run build\n```\n\nRun the server:\n\n```bash\nbun run dist/index.js\n```\n\n### CLI Arguments\n\n`context7-mcp` accepts the following CLI flags:\n\n- `--transport \u003cstdio|http\u003e` – Transport to use (`stdio` by default). Note that HTTP transport automatically provides both HTTP and SSE endpoints.\n- `--port \u003cnumber\u003e` – Port to listen on when using `http` transport (default `3000`).\n- `--api-key \u003ckey\u003e` – API key for authentication (or set `CONTEXT7_API_KEY` env var). You can get your API key by creating an account at [context7.com/dashboard](https://context7.com/dashboard).\n\nExample with HTTP transport and port 8080:\n\n```bash\nbun run dist/index.js --transport http --port 8080\n```\n\nAnother example with stdio transport:\n\n```bash\nbun run dist/index.js --transport stdio --api-key YOUR_API_KEY\n```\n\n### Environment Variables\n\nYou can use the `CONTEXT7_API_KEY` environment variable instead of passing the `--api-key` flag. This is useful for:\n\n- Storing API keys securely in `.env` files\n- Integration with MCP server setups that use dotenv\n- Tools that prefer environment variable configuration\n\n**Note:** The `--api-key` CLI flag takes precedence over the environment variable when both are provided.\n\n**Example with .env file:**\n\n```bash\n# .env\nCONTEXT7_API_KEY=your_api_key_here\n```\n\n**Example MCP configuration using environment variable:**\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"],\n      \"env\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eLocal Configuration Example\u003c/b\u003e\u003c/summary\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"/path/to/folder/context7/src/index.ts\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTesting with MCP Inspector\u003c/b\u003e\u003c/summary\u003e\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx @upstash/context7-mcp\n```\n\n\u003c/details\u003e\n\n## 🚨 Troubleshooting\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eModule Not Found Errors\u003c/b\u003e\u003c/summary\u003e\n\nIf you encounter `ERR_MODULE_NOT_FOUND`, try using `bunx` instead of `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\nThis often resolves module resolution issues in environments where `npx` doesn't properly install or resolve packages.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eESM Resolution Issues\u003c/b\u003e\u003c/summary\u003e\n\nFor errors like `Error: Cannot find module 'uriTemplate.js'`, try the `--experimental-vm-modules` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-vm-modules\", \"@upstash/context7-mcp@1.0.6\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTLS/Certificate Issues\u003c/b\u003e\u003c/summary\u003e\n\nUse the `--experimental-fetch` flag to bypass TLS-related problems:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-fetch\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eGeneral MCP Client Errors\u003c/b\u003e\u003c/summary\u003e\n\n1. Try adding `@latest` to the package name\n2. Use `bunx` as an alternative to `npx`\n3. Consider using `deno` as another alternative\n4. Ensure you're using Node.js v18 or higher for native fetch support\n\n\u003c/details\u003e\n\n## ⚠️ Disclaimer\n\nContext7 projects are community-contributed and while we strive to maintain high quality, we cannot guarantee the accuracy, completeness, or security of all library documentation. Projects listed in Context7 are developed and maintained by their respective owners, not by Context7. If you encounter any suspicious, inappropriate, or potentially harmful content, please use the \"Report\" button on the project page to notify us immediately. We take all reports seriously and will review flagged content promptly to maintain the integrity and safety of our platform. By using Context7, you acknowledge that you do so at your own discretion and risk.\n\n## 🤝 Connect with Us\n\nStay updated and join our community:\n\n- 📢 Follow us on [X](https://x.com/context7ai) for the latest news and updates\n- 🌐 Visit our [Website](https://context7.com)\n- 💬 Join our [Discord Community](https://upstash.com/discord)\n\n## 📺 Context7 In Media\n\n- [Better Stack: \"Free Tool Makes Cursor 10x Smarter\"](https://youtu.be/52FC3qObp9E)\n- [Cole Medin: \"This is Hands Down the BEST MCP Server for AI Coding Assistants\"](https://www.youtube.com/watch?v=G7gK8H6u7Rs)\n- [Income Stream Surfers: \"Context7 + SequentialThinking MCPs: Is This AGI?\"](https://www.youtube.com/watch?v=-ggvzyLpK6o)\n- [Julian Goldie SEO: \"Context7: New MCP AI Agent Update\"](https://www.youtube.com/watch?v=CTZm6fBYisc)\n- [JeredBlu: \"Context 7 MCP: Get Documentation Instantly + VS Code Setup\"](https://www.youtube.com/watch?v=-ls0D-rtET4)\n- [Income Stream Surfers: \"Context7: The New MCP Server That Will CHANGE AI Coding\"](https://www.youtube.com/watch?v=PS-2Azb-C3M)\n- [AICodeKing: \"Context7 + Cline \u0026 RooCode: This MCP Server Makes CLINE 100X MORE EFFECTIVE!\"](https://www.youtube.com/watch?v=qZfENAPMnyo)\n- [Sean Kochel: \"5 MCP Servers For Vibe Coding Glory (Just Plug-In \u0026 Go)\"](https://www.youtube.com/watch?v=LqTQi8qexJM)\n\n## ⭐ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=upstash/context7\u0026type=Date)](https://www.star-history.com/#upstash/context7\u0026Date)\n\n## 📄 License\n\nMIT\n","stargazer_count":34640,"topics":["llm","mcp","mcp-server","vibe-coding"],"uses_custom_opengraph_image":false}}}},{"name":"github/github-mcp-server","description":"Official GitHub MCP Server that connects AI tools directly to GitHub's platform. Enables AI agents to read repositories, manage issues and PRs, analyze code, and automate workflows through natural language interactions.","status":"active","repository":{"url":"https://github.com/github/github-mcp-server","source":"github","id":"942771284","readme":"# GitHub MCP Server\n\nThe GitHub MCP Server connects AI tools directly to GitHub's platform. This gives AI agents, assistants, and chatbots the ability to read repositories and code files, manage issues and PRs, analyze code, and automate workflows. All through natural language interactions.\n\n### Use Cases\n\n- Repository Management: Browse and query code, search files, analyze commits, and understand project structure across any repository you have access to.\n- Issue \u0026 PR Automation: Create, update, and manage issues and pull requests. Let AI help triage bugs, review code changes, and maintain project boards.\n- CI/CD \u0026 Workflow Intelligence: Monitor GitHub Actions workflow runs, analyze build failures, manage releases, and get insights into your development pipeline.\n- Code Analysis: Examine security findings, review Dependabot alerts, understand code patterns, and get comprehensive insights into your codebase.\n- Team Collaboration: Access discussions, manage notifications, analyze team activity, and streamline processes for your team.\n\nBuilt for developers who want to connect their AI tools to GitHub context and capabilities, from simple natural language queries to complex multi-step agent workflows.\n\n---\n\n## Remote GitHub MCP Server\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D\u0026quality=insiders)\n\nThe remote GitHub MCP Server is hosted by GitHub and provides the easiest method for getting up and running. If your MCP host does not support remote MCP servers, don't worry! You can use the [local version of the GitHub MCP Server](https://github.com/github/github-mcp-server?tab=readme-ov-file#local-github-mcp-server) instead.\n\n### Prerequisites\n\n1. A compatible MCP host with remote server support (VS Code 1.101+, Claude Desktop, Cursor, Windsurf, etc.)\n2. Any applicable [policies enabled](https://github.com/github/github-mcp-server/blob/main/docs/policies-and-governance.md)\n\n### Install in VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start. Make sure you're using [VS Code 1.101](https://code.visualstudio.com/updates/v1_101) or [later](https://code.visualstudio.com/updates) for remote MCP and OAuth support.\n\nAlternatively, to manually configure VS Code, choose the appropriate JSON block from the examples below and add it to your host configuration:\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eUsing OAuth\u003c/th\u003e\u003cth\u003eUsing a GitHub PAT\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003cth align=left colspan=2\u003eVS Code (version 1.101 or greater)\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${input:github_mcp_pat}\"\n      }\n    }\n  },\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_mcp_pat\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ]\n}\n```\n\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n### Install in other MCP hosts\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Applications](/docs/installation-guides/install-claude.md)** - Installation guide for Claude Web, Claude Desktop and Claude Code CLI\n- **[Cursor](/docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Windsurf](/docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n\n\u003e **Note:** Each MCP host application needs to configure a GitHub App or OAuth App to support remote access via OAuth. Any host application that supports remote MCP servers should support the remote GitHub server with PAT authentication. Configuration details and support levels vary by host. Make sure to refer to the host application's documentation for more info.\n\n### Configuration\n\n#### Toolset configuration\n\nSee [Remote Server Documentation](docs/remote-server.md) for full details on remote server configuration, toolsets, headers, and advanced usage. This file provides comprehensive instructions and examples for connecting, customizing, and installing the remote GitHub MCP Server in VS Code and other MCP hosts.\n\nWhen no toolsets are specified, [default toolsets](#default-toolset) are used.\n\n#### Enterprise Cloud with data residency (ghe.com)\n\nGitHub Enterprise Cloud can also make use of the remote server.\n\nExample for `https://octocorp.ghe.com`:\n```\n{\n    ...\n    \"proxima-github\": {\n      \"type\": \"http\",\n      \"url\": \"https://copilot-api.octocorp.ghe.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${input:github_mcp_pat}\"\n      }\n    },\n    ...\n}\n```\n\nGitHub Enterprise Server does not support remote server hosting. Please refer to [GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)](#github-enterprise-server-and-enterprise-cloud-with-data-residency-ghecom) from the local server configuration.\n\n---\n\n## Local GitHub MCP Server\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D\u0026quality=insiders)\n\n### Prerequisites\n\n1. To run the server in a container, you will need to have [Docker](https://www.docker.com/) installed.\n2. Once Docker is installed, you will also need to ensure Docker is running. The image is public; if you get errors on pull, you may have an expired token and need to `docker logout ghcr.io`.\n3. Lastly you will need to [Create a GitHub Personal Access Token](https://github.com/settings/personal-access-tokens/new).\nThe MCP server can use many of the GitHub APIs, so enable the permissions that you feel comfortable granting your AI tools (to learn more about access tokens, please check out the [documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)).\n\n\u003cdetails\u003e\u003csummary\u003e\u003cb\u003eHandling PATs Securely\u003c/b\u003e\u003c/summary\u003e\n\n### Environment Variables (Recommended)\nTo keep your GitHub PAT secure and reusable across different MCP hosts:\n\n1. **Store your PAT in environment variables**\n   ```bash\n   export GITHUB_PAT=your_token_here\n   ```\n   Or create a `.env` file:\n   ```env\n   GITHUB_PAT=your_token_here\n   ```\n\n2. **Protect your `.env` file**\n   ```bash\n   # Add to .gitignore to prevent accidental commits\n   echo \".env\" \u003e\u003e .gitignore\n   ```\n\n3. **Reference the token in configurations**\n   ```bash\n   # CLI usage\n   claude mcp update github -e GITHUB_PERSONAL_ACCESS_TOKEN=$GITHUB_PAT\n\n   # In config files (where supported)\n   \"env\": {\n     \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"$GITHUB_PAT\"\n   }\n   ```\n\n\u003e **Note**: Environment variable support varies by host app and IDE. Some applications (like Windsurf) require hardcoded tokens in config files.\n\n### Token Security Best Practices\n\n- **Minimum scopes**: Only grant necessary permissions\n  - `repo` - Repository operations\n  - `read:packages` - Docker image access\n  - `read:org` - Organization team access\n- **Separate tokens**: Use different PATs for different projects/environments\n- **Regular rotation**: Update tokens periodically\n- **Never commit**: Keep tokens out of version control\n- **File permissions**: Restrict access to config files containing tokens\n  ```bash\n  chmod 600 ~/.your-app/config.json\n  ```\n\n\u003c/details\u003e\n\n### GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)\n\nThe flag `--gh-host` and the environment variable `GITHUB_HOST` can be used to set\nthe hostname for GitHub Enterprise Server or GitHub Enterprise Cloud with data residency.\n\n- For GitHub Enterprise Server, prefix the hostname with the `https://` URI scheme, as it otherwise defaults to `http://`, which GitHub Enterprise Server does not support.\n- For GitHub Enterprise Cloud with data residency, use `https://YOURSUBDOMAIN.ghe.com` as the hostname.\n``` json\n\"github\": {\n    \"command\": \"docker\",\n    \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"-e\",\n    \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n    \"-e\",\n    \"GITHUB_HOST\",\n    \"ghcr.io/github/github-mcp-server\"\n    ],\n    \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\",\n        \"GITHUB_HOST\": \"https://\u003cyour GHES or ghe.com domain name\u003e\"\n    }\n}\n```\n\n## Installation\n\n### Install in GitHub Copilot on VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start.\n\nMore about using MCP server tools in VS Code's [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\nInstall in GitHub Copilot on other IDEs (JetBrains, Visual Studio, Eclipse, etc.)\n\nAdd the following JSON block to your IDE's MCP settings.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"github_token\",\n        \"description\": \"GitHub Personal Access Token\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"github\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n          \"ghcr.io/github/github-mcp-server\"\n        ],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with other host applications that accept the same format.\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eExample JSON block without the MCP key included\u003c/b\u003e\u003c/summary\u003e\n\u003cbr\u003e\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_token\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"github\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n        \"ghcr.io/github/github-mcp-server\"\n      ],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n      }\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n### Install in Other MCP Hosts\n\nFor other MCP host applications, please refer to our installation guides:\n\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Code \u0026 Claude Desktop](docs/installation-guides/install-claude.md)** - Installation guide for Claude Code and Claude Desktop\n- **[Cursor](docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Google Gemini CLI](docs/installation-guides/install-gemini-cli.md)** - Installation guide for Google Gemini CLI\n- **[Windsurf](docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n\nFor a complete overview of all installation options, see our **[Installation Guides Index](docs/installation-guides)**.\n\n\u003e **Note:** Any host application that supports local MCP servers should be able to access the local GitHub MCP server. However, the specific configuration process, syntax and stability of the integration will vary by host application. While many may follow a similar format to the examples above, this is not guaranteed. Please refer to your host application's documentation for the correct MCP configuration syntax and setup process.\n\n### Build from source\n\nIf you don't have Docker, you can use `go build` to build the binary in the\n`cmd/github-mcp-server` directory, and use the `github-mcp-server stdio` command with the `GITHUB_PERSONAL_ACCESS_TOKEN` environment variable set to your token. To specify the output location of the build, use the `-o` flag. You should configure your server to use the built executable as its `command`. For example:\n\n```JSON\n{\n  \"mcp\": {\n    \"servers\": {\n      \"github\": {\n        \"command\": \"/path/to/github-mcp-server\",\n        \"args\": [\"stdio\"],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"\u003cYOUR_TOKEN\u003e\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Tool Configuration\n\nThe GitHub MCP Server supports enabling or disabling specific groups of functionalities via the `--toolsets` flag. This allows you to control which GitHub API capabilities are available to your AI tools. Enabling only the toolsets that you need can help the LLM with tool choice and reduce the context size.\n\n_Toolsets are not limited to Tools. Relevant MCP Resources and Prompts are also included where applicable._\n\nWhen no toolsets are specified, [default toolsets](#default-toolset) are used.\n\n#### Specifying Toolsets\n\nTo specify toolsets you want available to the LLM, you can pass an allow-list in two ways:\n\n1. **Using Command Line Argument**:\n\n   ```bash\n   github-mcp-server --toolsets repos,issues,pull_requests,actions,code_security\n   ```\n\n2. **Using Environment Variable**:\n   ```bash\n   GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security\" ./github-mcp-server\n   ```\n\nThe environment variable `GITHUB_TOOLSETS` takes precedence over the command line argument if both are provided.\n\n### Using Toolsets With Docker\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=\u003cyour-token\u003e \\\n  -e GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security,experiments\" \\\n  ghcr.io/github/github-mcp-server\n```\n\n### Special toolsets\n\n#### \"all\" toolset\n\nThe special toolset `all` can be provided to enable all available toolsets regardless of any other configuration:\n\n```bash\n./github-mcp-server --toolsets all\n```\n\nOr using the environment variable:\n\n```bash\nGITHUB_TOOLSETS=\"all\" ./github-mcp-server\n```\n\n#### \"default\" toolset\nThe default toolset `default` is the configuration that gets passed to the server if no toolsets are specified.\n\nThe default configuration is:\n- context\n- repos\n- issues\n- pull_requests\n- users\n\nTo keep the default configuration and add additional toolsets:\n\n```bash\nGITHUB_TOOLSETS=\"default,stargazers\" ./github-mcp-server\n```\n\n### Available Toolsets\n\nThe following sets of tools are available:\n\n\u003c!-- START AUTOMATED TOOLSETS --\u003e\n| Toolset                 | Description                                                   |\n| ----------------------- | ------------------------------------------------------------- |\n| `context`               | **Strongly recommended**: Tools that provide context about the current user and GitHub context you are operating in |\n| `actions` | GitHub Actions workflows and CI/CD operations |\n| `code_security` | Code security related tools, such as GitHub Code Scanning |\n| `dependabot` | Dependabot tools |\n| `discussions` | GitHub Discussions related tools |\n| `experiments` | Experimental features that are not considered stable yet |\n| `gists` | GitHub Gist related tools |\n| `issues` | GitHub Issues related tools |\n| `labels` | GitHub Labels related tools |\n| `notifications` | GitHub Notifications related tools |\n| `orgs` | GitHub Organization related tools |\n| `projects` | GitHub Projects related tools |\n| `pull_requests` | GitHub Pull Request related tools |\n| `repos` | GitHub Repository related tools |\n| `secret_protection` | Secret protection related tools, such as GitHub Secret Scanning |\n| `security_advisories` | Security advisories related tools |\n| `stargazers` | GitHub Stargazers related tools |\n| `users` | GitHub User related tools |\n\u003c!-- END AUTOMATED TOOLSETS --\u003e\n\n### Additional Toolsets in Remote Github MCP Server\n\n| Toolset                 | Description                                                   |\n| ----------------------- | ------------------------------------------------------------- |\n| `copilot` | Copilot related tools (e.g. Copilot Coding Agent) |\n| `copilot_spaces` | Copilot Spaces related tools |\n| `github_support_docs_search` | Search docs to answer GitHub product and support questions |\n\n## Tools\n\n\u003c!-- START AUTOMATED TOOLS --\u003e\n\u003cdetails\u003e\n\n\u003csummary\u003eActions\u003c/summary\u003e\n\n- **cancel_workflow_run** - Cancel workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **delete_workflow_run_logs** - Delete workflow logs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **download_workflow_run_artifact** - Download workflow artifact\n  - `artifact_id`: The unique identifier of the artifact (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_job_logs** - Get job logs\n  - `failed_only`: When true, gets logs for all failed jobs in run_id (boolean, optional)\n  - `job_id`: The unique identifier of the workflow job (required for single job logs) (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `return_content`: Returns actual log content instead of URLs (boolean, optional)\n  - `run_id`: Workflow run ID (required when using failed_only) (number, optional)\n  - `tail_lines`: Number of lines to return from the end of the log (number, optional)\n\n- **get_workflow_run** - Get workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **get_workflow_run_logs** - Get workflow run logs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **get_workflow_run_usage** - Get workflow usage\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_jobs** - List workflow jobs\n  - `filter`: Filters jobs by their completed_at timestamp (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_run_artifacts** - List workflow artifacts\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_runs** - List workflow runs\n  - `actor`: Returns someone's workflow runs. Use the login for the user who created the workflow run. (string, optional)\n  - `branch`: Returns workflow runs associated with a branch. Use the name of the branch. (string, optional)\n  - `event`: Returns workflow runs for a specific event type (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `status`: Returns workflow runs with the check run status (string, optional)\n  - `workflow_id`: The workflow ID or workflow file name (string, required)\n\n- **list_workflows** - List workflows\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **rerun_failed_jobs** - Rerun failed jobs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **rerun_workflow_run** - Rerun workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **run_workflow** - Run workflow\n  - `inputs`: Inputs the workflow accepts (object, optional)\n  - `owner`: Repository owner (string, required)\n  - `ref`: The git reference for the workflow. The reference can be a branch or tag name. (string, required)\n  - `repo`: Repository name (string, required)\n  - `workflow_id`: The workflow ID (numeric) or workflow file name (e.g., main.yml, ci.yaml) (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eCode Security\u003c/summary\u003e\n\n- **get_code_scanning_alert** - Get code scanning alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_code_scanning_alerts** - List code scanning alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `ref`: The Git reference for the results you want to list. (string, optional)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter code scanning alerts by severity (string, optional)\n  - `state`: Filter code scanning alerts by state. Defaults to open (string, optional)\n  - `tool_name`: The name of the tool used for code scanning. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eContext\u003c/summary\u003e\n\n- **get_me** - Get my user profile\n  - No parameters required\n\n- **get_team_members** - Get team members\n  - `org`: Organization login (owner) that contains the team. (string, required)\n  - `team_slug`: Team slug (string, required)\n\n- **get_teams** - Get teams\n  - `user`: Username to get teams for. If not provided, uses the authenticated user. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eDependabot\u003c/summary\u003e\n\n- **get_dependabot_alert** - Get dependabot alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_dependabot_alerts** - List dependabot alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter dependabot alerts by severity (string, optional)\n  - `state`: Filter dependabot alerts by state. Defaults to open (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eDiscussions\u003c/summary\u003e\n\n- **get_discussion** - Get discussion\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_discussion_comments** - Get discussion comments\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_discussion_categories** - List discussion categories\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name. If not provided, discussion categories will be queried at the organisation level. (string, optional)\n\n- **list_discussions** - List discussions\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `category`: Optional filter by discussion category ID. If provided, only discussions with this category are listed. (string, optional)\n  - `direction`: Order direction. (string, optional)\n  - `orderBy`: Order discussions by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name. If not provided, discussions will be queried at the organisation level. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eGists\u003c/summary\u003e\n\n- **create_gist** - Create Gist\n  - `content`: Content for simple single-file gist creation (string, required)\n  - `description`: Description of the gist (string, optional)\n  - `filename`: Filename for simple single-file gist creation (string, required)\n  - `public`: Whether the gist is public (boolean, optional)\n\n- **list_gists** - List Gists\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `since`: Only gists updated after this time (ISO 8601 timestamp) (string, optional)\n  - `username`: GitHub username (omit for authenticated user's gists) (string, optional)\n\n- **update_gist** - Update Gist\n  - `content`: Content for the file (string, required)\n  - `description`: Updated description of the gist (string, optional)\n  - `filename`: Filename to update or create (string, required)\n  - `gist_id`: ID of the gist to update (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eIssues\u003c/summary\u003e\n\n- **add_issue_comment** - Add comment to issue\n  - `body`: Comment content (string, required)\n  - `issue_number`: Issue number to comment on (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **add_sub_issue** - Add sub-issue\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `replace_parent`: When true, replaces the sub-issue's current parent issue (boolean, optional)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to add. ID is not the same as issue number (number, required)\n\n- **assign_copilot_to_issue** - Assign Copilot to issue\n  - `issueNumber`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **create_issue** - Open new issue\n  - `assignees`: Usernames to assign to this issue (string[], optional)\n  - `body`: Issue body content (string, optional)\n  - `labels`: Labels to apply to this issue (string[], optional)\n  - `milestone`: Milestone number (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `title`: Issue title (string, required)\n  - `type`: Type of this issue (string, optional)\n\n- **get_issue** - Get issue details\n  - `issue_number`: The number of the issue (number, required)\n  - `owner`: The owner of the repository (string, required)\n  - `repo`: The name of the repository (string, required)\n\n- **get_issue_comments** - Get issue comments\n  - `issue_number`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **get_label** - Get a specific label from a repository.\n  - `name`: Label name. (string, required)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **list_issue_types** - List available issue types\n  - `owner`: The organization owner of the repository (string, required)\n\n- **list_issues** - List issues\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `direction`: Order direction. If provided, the 'orderBy' also needs to be provided. (string, optional)\n  - `labels`: Filter by labels (string[], optional)\n  - `orderBy`: Order issues by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `since`: Filter by date (ISO 8601 timestamp) (string, optional)\n  - `state`: Filter by state, by default both open and closed issues are returned when not provided (string, optional)\n\n- **list_label** - List labels from a repository or an issue\n  - `issue_number`: Issue number - if provided, lists labels on the specific issue (number, optional)\n  - `owner`: Repository owner (username or organization name) - required for all operations (string, required)\n  - `repo`: Repository name - required for all operations (string, required)\n\n- **list_sub_issues** - List sub-issues\n  - `issue_number`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (default: 1) (number, optional)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **remove_sub_issue** - Remove sub-issue\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to remove. ID is not the same as issue number (number, required)\n\n- **reprioritize_sub_issue** - Reprioritize sub-issue\n  - `after_id`: The ID of the sub-issue to be prioritized after (either after_id OR before_id should be specified) (number, optional)\n  - `before_id`: The ID of the sub-issue to be prioritized before (either after_id OR before_id should be specified) (number, optional)\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to reprioritize. ID is not the same as issue number (number, required)\n\n- **search_issues** - Search issues\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only issues for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub issues search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only issues for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **update_issue** - Edit issue\n  - `assignees`: New assignees (string[], optional)\n  - `body`: New description (string, optional)\n  - `duplicate_of`: Issue number that this issue is a duplicate of. Only used when state_reason is 'duplicate'. (number, optional)\n  - `issue_number`: Issue number to update (number, required)\n  - `labels`: New labels (string[], optional)\n  - `milestone`: New milestone number (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `state`: New state (string, optional)\n  - `state_reason`: Reason for the state change. Ignored unless state is changed. (string, optional)\n  - `title`: New title (string, optional)\n  - `type`: New issue type (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eLabels\u003c/summary\u003e\n\n- **get_label** - Get a specific label from a repository.\n  - `name`: Label name. (string, required)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **label_write** - Write operations on repository labels.\n  - `color`: Label color as 6-character hex code without '#' prefix (e.g., 'f29513'). Required for 'create', optional for 'update'. (string, optional)\n  - `description`: Label description text. Optional for 'create' and 'update'. (string, optional)\n  - `method`: Operation to perform: 'create', 'update', or 'delete' (string, required)\n  - `name`: Label name - required for all operations (string, required)\n  - `new_name`: New name for the label (used only with 'update' method to rename) (string, optional)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **list_label** - List labels from a repository or an issue\n  - `issue_number`: Issue number - if provided, lists labels on the specific issue (number, optional)\n  - `owner`: Repository owner (username or organization name) - required for all operations (string, required)\n  - `repo`: Repository name - required for all operations (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eNotifications\u003c/summary\u003e\n\n- **dismiss_notification** - Dismiss notification\n  - `state`: The new state of the notification (read/done) (string, optional)\n  - `threadID`: The ID of the notification thread (string, required)\n\n- **get_notification_details** - Get notification details\n  - `notificationID`: The ID of the notification (string, required)\n\n- **list_notifications** - List notifications\n  - `before`: Only show notifications updated before the given time (ISO 8601 format) (string, optional)\n  - `filter`: Filter notifications to, use default unless specified. Read notifications are ones that have already been acknowledged by the user. Participating notifications are those that the user is directly involved in, such as issues or pull requests they have commented on or created. (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are listed. (string, optional)\n  - `since`: Only show notifications updated after the given time (ISO 8601 format) (string, optional)\n\n- **manage_notification_subscription** - Manage notification subscription\n  - `action`: Action to perform: ignore, watch, or delete the notification subscription. (string, required)\n  - `notificationID`: The ID of the notification thread. (string, required)\n\n- **manage_repository_notification_subscription** - Manage repository notification subscription\n  - `action`: Action to perform: ignore, watch, or delete the repository notification subscription. (string, required)\n  - `owner`: The account owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **mark_all_notifications_read** - Mark all notifications as read\n  - `lastReadAt`: Describes the last point that notifications were checked (optional). Default: Now (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are marked as read. (string, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are marked as read. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eOrganizations\u003c/summary\u003e\n\n- **search_orgs** - Search organizations\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Organization search query. Examples: 'microsoft', 'location:california', 'created:\u003e=2025-01-01'. Search is automatically scoped to type:org. (string, required)\n  - `sort`: Sort field by category (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eProjects\u003c/summary\u003e\n\n- **add_project_item** - Add project item\n  - `item_id`: The numeric ID of the issue or pull request to add to the project. (number, required)\n  - `item_type`: The item's type, either issue or pull_request. (string, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **delete_project_item** - Delete project item\n  - `item_id`: The internal project item ID to delete from the project (not the issue or pull request ID). (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **get_project** - Get project\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number (number, required)\n\n- **get_project_field** - Get project field\n  - `field_id`: The field's id. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **get_project_item** - Get project item\n  - `item_id`: The item's ID. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **list_project_fields** - List project fields\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `project_number`: The project's number. (number, required)\n\n- **list_project_items** - List project items\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `project_number`: The project's number. (number, required)\n  - `query`: Search query to filter items (string, optional)\n\n- **list_projects** - List projects\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `query`: Filter projects by a search query (matches title and description) (string, optional)\n\n- **update_project_item** - Update project item\n  - `item_id`: The unique identifier of the project item. This is not the issue or pull request ID. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n  - `updated_field`: Object consisting of the ID of the project field to update and the new value for the field. To clear the field, set \"value\" to null. Example: {\"id\": 123456, \"value\": \"New Value\"} (object, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003ePull Requests\u003c/summary\u003e\n\n- **add_comment_to_pending_review** - Add review comment to the requester's latest pending pull request review\n  - `body`: The text of the review comment (string, required)\n  - `line`: The line of the blob in the pull request diff that the comment applies to. For multi-line comments, the last line of the range (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `path`: The relative path to the file that necessitates a comment (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n  - `side`: The side of the diff to comment on. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `startLine`: For multi-line comments, the first line of the range that the comment applies to (number, optional)\n  - `startSide`: For multi-line comments, the starting side of the diff that the comment applies to. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `subjectType`: The level at which the comment is targeted (string, required)\n\n- **create_pull_request** - Open new pull request\n  - `base`: Branch to merge into (string, required)\n  - `body`: PR description (string, optional)\n  - `draft`: Create as draft PR (boolean, optional)\n  - `head`: Branch containing changes (string, required)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `title`: PR title (string, required)\n\n- **list_pull_requests** - List pull requests\n  - `base`: Filter by base branch (string, optional)\n  - `direction`: Sort direction (string, optional)\n  - `head`: Filter by head user/org and branch (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sort`: Sort by (string, optional)\n  - `state`: Filter by state (string, optional)\n\n- **merge_pull_request** - Merge pull request\n  - `commit_message`: Extra detail for merge commit (string, optional)\n  - `commit_title`: Title for merge commit (string, optional)\n  - `merge_method`: Merge method (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **pull_request_read** - Get details for a single pull request\n  - `method`: Action to specify what pull request data needs to be retrieved from GitHub. \nPossible options: \n 1. get - Get details of a specific pull request.\n 2. get_diff - Get the diff of a pull request.\n 3. get_status - Get status of a head commit in a pull request. This reflects status of builds and checks.\n 4. get_files - Get the list of files changed in a pull request. Use with pagination parameters to control the number of results returned.\n 5. get_review_comments - Get the review comments on a pull request. Use with pagination parameters to control the number of results returned.\n 6. get_reviews - Get the reviews on a pull request. When asked for review comments, use get_review_comments method.\n (string, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **pull_request_review_write** - Write operations (create, submit, delete) on pull request reviews.\n  - `body`: Review comment text (string, optional)\n  - `commitID`: SHA of commit to review (string, optional)\n  - `event`: Review action to perform. (string, optional)\n  - `method`: The write operation to perform on pull request review. (string, required)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **request_copilot_review** - Request Copilot review\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **search_pull_requests** - Search pull requests\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only pull requests for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub pull request search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only pull requests for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **update_pull_request** - Edit pull request\n  - `base`: New base branch name (string, optional)\n  - `body`: New description (string, optional)\n  - `draft`: Mark pull request as draft (true) or ready for review (false) (boolean, optional)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number to update (number, required)\n  - `repo`: Repository name (string, required)\n  - `reviewers`: GitHub usernames to request reviews from (string[], optional)\n  - `state`: New state (string, optional)\n  - `title`: New title (string, optional)\n\n- **update_pull_request_branch** - Update pull request branch\n  - `expectedHeadSha`: The expected SHA of the pull request's HEAD ref (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eRepositories\u003c/summary\u003e\n\n- **create_branch** - Create branch\n  - `branch`: Name for new branch (string, required)\n  - `from_branch`: Source branch (defaults to repo default) (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **create_or_update_file** - Create or update file\n  - `branch`: Branch to create/update the file in (string, required)\n  - `content`: Content of the file (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path where to create/update the file (string, required)\n  - `repo`: Repository name (string, required)\n  - `sha`: Required if updating an existing file. The blob SHA of the file being replaced. (string, optional)\n\n- **create_repository** - Create repository\n  - `autoInit`: Initialize with README (boolean, optional)\n  - `description`: Repository description (string, optional)\n  - `name`: Repository name (string, required)\n  - `organization`: Organization to create the repository in (omit to create in your personal account) (string, optional)\n  - `private`: Whether repo should be private (boolean, optional)\n\n- **delete_file** - Delete file\n  - `branch`: Branch to delete the file from (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to the file to delete (string, required)\n  - `repo`: Repository name (string, required)\n\n- **fork_repository** - Fork repository\n  - `organization`: Organization to fork to (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_commit** - Get commit details\n  - `include_diff`: Whether to include file diffs and stats in the response. Default is true. (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch name, or tag name (string, required)\n\n- **get_file_contents** - Get file or directory contents\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to file/directory (directories must end with a slash '/') (string, optional)\n  - `ref`: Accepts optional git refs such as `refs/tags/{tag}`, `refs/heads/{branch}` or `refs/pull/{pr_number}/head` (string, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Accepts optional commit SHA. If specified, it will be used instead of ref (string, optional)\n\n- **get_latest_release** - Get latest release\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_release_by_tag** - Get a release by tag name\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (e.g., 'v1.0.0') (string, required)\n\n- **get_tag** - Get tag details\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (string, required)\n\n- **list_branches** - List branches\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_commits** - List commits\n  - `author`: Author username or email address to filter commits by (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch or tag name to list commits of. If not provided, uses the default branch of the repository. If a commit SHA is provided, will list commits up to that SHA. (string, optional)\n\n- **list_releases** - List releases\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_tags** - List tags\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **push_files** - Push files to repository\n  - `branch`: Branch to push to (string, required)\n  - `files`: Array of file objects to push, each object with path (string) and content (string) (object[], required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **search_code** - Search code\n  - `order`: Sort order for results (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub's powerful code search syntax. Examples: 'content:Skill language:Java org:github', 'NOT is:archived language:Python OR language:go', 'repo:github/github-mcp-server'. Supports exact matching, language filters, path filters, and more. (string, required)\n  - `sort`: Sort field ('indexed' only) (string, optional)\n\n- **search_repositories** - Search repositories\n  - `minimal_output`: Return minimal repository information (default: true). When false, returns full GitHub API repository objects. (boolean, optional)\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Repository search query. Examples: 'machine learning in:name stars:\u003e1000 language:python', 'topic:react', 'user:facebook'. Supports advanced search syntax for precise filtering. (string, required)\n  - `sort`: Sort repositories by field, defaults to best match (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eSecret Protection\u003c/summary\u003e\n\n- **get_secret_scanning_alert** - Get secret scanning alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_secret_scanning_alerts** - List secret scanning alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `resolution`: Filter by resolution (string, optional)\n  - `secret_type`: A comma-separated list of secret types to return. All default secret patterns are returned. To return generic patterns, pass the token name(s) in the parameter. (string, optional)\n  - `state`: Filter by state (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eSecurity Advisories\u003c/summary\u003e\n\n- **get_global_security_advisory** - Get a global security advisory\n  - `ghsaId`: GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, required)\n\n- **list_global_security_advisories** - List global security advisories\n  - `affects`: Filter advisories by affected package or version (e.g. \"package1,package2@1.0.0\"). (string, optional)\n  - `cveId`: Filter by CVE ID. (string, optional)\n  - `cwes`: Filter by Common Weakness Enumeration IDs (e.g. [\"79\", \"284\", \"22\"]). (string[], optional)\n  - `ecosystem`: Filter by package ecosystem. (string, optional)\n  - `ghsaId`: Filter by GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, optional)\n  - `isWithdrawn`: Whether to only return withdrawn advisories. (boolean, optional)\n  - `modified`: Filter by publish or update date or date range (ISO 8601 date or range). (string, optional)\n  - `published`: Filter by publish date or date range (ISO 8601 date or range). (string, optional)\n  - `severity`: Filter by severity. (string, optional)\n  - `type`: Advisory type. (string, optional)\n  - `updated`: Filter by update date or date range (ISO 8601 date or range). (string, optional)\n\n- **list_org_repository_security_advisories** - List org repository security advisories\n  - `direction`: Sort direction. (string, optional)\n  - `org`: The organization login. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n- **list_repository_security_advisories** - List repository security advisories\n  - `direction`: Sort direction. (string, optional)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eStargazers\u003c/summary\u003e\n\n- **list_starred_repositories** - List starred repositories\n  - `direction`: The direction to sort the results by. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `sort`: How to sort the results. Can be either 'created' (when the repository was starred) or 'updated' (when the repository was last pushed to). (string, optional)\n  - `username`: Username to list starred repositories for. Defaults to the authenticated user. (string, optional)\n\n- **star_repository** - Star repository\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **unstar_repository** - Unstar repository\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eUsers\u003c/summary\u003e\n\n- **search_users** - Search users\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: User search query. Examples: 'john smith', 'location:seattle', 'followers:\u003e100'. Search is automatically scoped to type:user. (string, required)\n  - `sort`: Sort users by number of followers or repositories, or when the person joined GitHub. (string, optional)\n\n\u003c/details\u003e\n\u003c!-- END AUTOMATED TOOLS --\u003e\n\n### Additional Tools in Remote Github MCP Server\n\n\u003cdetails\u003e\n\n\u003csummary\u003eCopilot\u003c/summary\u003e\n\n-   **create_pull_request_with_copilot** - Perform task with GitHub Copilot coding agent\n    -   `owner`: Repository owner. You can guess the owner, but confirm it with the user before proceeding. (string, required)\n    -   `repo`: Repository name. You can guess the repository name, but confirm it with the user before proceeding. (string, required)\n    -   `problem_statement`: Detailed description of the task to be performed (e.g., 'Implement a feature that does X', 'Fix bug Y', etc.) (string, required)\n    -   `title`: Title for the pull request that will be created (string, required)\n    -   `base_ref`: Git reference (e.g., branch) that the agent will start its work from. If not specified, defaults to the repository's default branch (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eCopilot Spaces\u003c/summary\u003e\n\n-   **get_copilot_space** - Get Copilot Space\n    -   `owner`: The owner of the space. (string, required)\n    -   `name`: The name of the space. (string, required)\n\n-   **list_copilot_spaces** - List Copilot Spaces\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eGitHub Support Docs Search\u003c/summary\u003e\n\n-   **github_support_docs_search** - Retrieve documentation relevant to answer GitHub product and support questions. Support topics include: GitHub Actions Workflows, Authentication, GitHub Support Inquiries, Pull Request Practices, Repository Maintenance, GitHub Pages, GitHub Packages, GitHub Discussions, Copilot Spaces\n    -   `query`: Input from the user about the question they need answered. This is the latest raw unedited user message. You should ALWAYS leave the user message as it is, you should never modify it. (string, required)\n\u003c/details\u003e\n\n## Dynamic Tool Discovery\n\n**Note**: This feature is currently in beta and may not be available in all environments. Please test it out and let us know if you encounter any issues.\n\nInstead of starting with all tools enabled, you can turn on dynamic toolset discovery. Dynamic toolsets allow the MCP host to list and enable toolsets in response to a user prompt. This should help to avoid situations where the model gets confused by the sheer number of tools available.\n\n### Using Dynamic Tool Discovery\n\nWhen using the binary, you can pass the `--dynamic-toolsets` flag.\n\n```bash\n./github-mcp-server --dynamic-toolsets\n```\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=\u003cyour-token\u003e \\\n  -e GITHUB_DYNAMIC_TOOLSETS=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## Read-Only Mode\n\nTo run the server in read-only mode, you can use the `--read-only` flag. This will only offer read-only tools, preventing any modifications to repositories, issues, pull requests, etc.\n\n```bash\n./github-mcp-server --read-only\n```\n\nWhen using Docker, you can pass the read-only mode as an environment variable:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=\u003cyour-token\u003e \\\n  -e GITHUB_READ_ONLY=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## i18n / Overriding Descriptions\n\nThe descriptions of the tools can be overridden by creating a\n`github-mcp-server-config.json` file in the same directory as the binary.\n\nThe file should contain a JSON object with the tool names as keys and the new\ndescriptions as values. For example:\n\n```json\n{\n  \"TOOL_ADD_ISSUE_COMMENT_DESCRIPTION\": \"an alternative description\",\n  \"TOOL_CREATE_BRANCH_DESCRIPTION\": \"Create a new branch in a GitHub repository\"\n}\n```\n\nYou can create an export of the current translations by running the binary with\nthe `--export-translations` flag.\n\nThis flag will preserve any translations/overrides you have made, while adding\nany new translations that have been added to the binary since the last time you\nexported.\n\n```sh\n./github-mcp-server --export-translations\ncat github-mcp-server-config.json\n```\n\nYou can also use ENV vars to override the descriptions. The environment\nvariable names are the same as the keys in the JSON file, prefixed with\n`GITHUB_MCP_` and all uppercase.\n\nFor example, to override the `TOOL_ADD_ISSUE_COMMENT_DESCRIPTION` tool, you can\nset the following environment variable:\n\n```sh\nexport GITHUB_MCP_TOOL_ADD_ISSUE_COMMENT_DESCRIPTION=\"an alternative description\"\n```\n\n## Library Usage\n\nThe exported Go API of this module should currently be considered unstable, and subject to breaking changes. In the future, we may offer stability; please file an issue if there is a use case where this would be valuable.\n\n## License\n\nThis project is licensed under the terms of the MIT open source license. Please refer to [MIT](./LICENSE) for the full terms.\n"},"version":"0.13.0","created_at":"2025-09-23T17:41:15Z","updated_at":"2025-10-22T11:21:30Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://api.githubcopilot.com/mcp/"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"ab12cd34-5678-90ef-1234-567890abcdef","is_latest":true,"published_at":"2025-08-22T00:00:00Z","updated_at":"2025-08-22T00:00:00Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"GitHub","is_in_organization":true,"license":"MIT License","name":"github-mcp-server","name_with_owner":"github/github-mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/8ef88f4a4fa88fad7f1209acf6ccb6cfeb047b3597d0f8f5090a781e46bc2728/github/github-mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","preferred_image":"https://avatars.githubusercontent.com/u/9919?v=4","primary_language":"Go","primary_language_color":"#00ADD8","pushed_at":"2025-10-21T15:08:09Z","readme":"# GitHub MCP Server\n\nThe GitHub MCP Server connects AI tools directly to GitHub's platform. This gives AI agents, assistants, and chatbots the ability to read repositories and code files, manage issues and PRs, analyze code, and automate workflows. All through natural language interactions.\n\n### Use Cases\n\n- Repository Management: Browse and query code, search files, analyze commits, and understand project structure across any repository you have access to.\n- Issue \u0026 PR Automation: Create, update, and manage issues and pull requests. Let AI help triage bugs, review code changes, and maintain project boards.\n- CI/CD \u0026 Workflow Intelligence: Monitor GitHub Actions workflow runs, analyze build failures, manage releases, and get insights into your development pipeline.\n- Code Analysis: Examine security findings, review Dependabot alerts, understand code patterns, and get comprehensive insights into your codebase.\n- Team Collaboration: Access discussions, manage notifications, analyze team activity, and streamline processes for your team.\n\nBuilt for developers who want to connect their AI tools to GitHub context and capabilities, from simple natural language queries to complex multi-step agent workflows.\n\n---\n\n## Remote GitHub MCP Server\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D\u0026quality=insiders)\n\nThe remote GitHub MCP Server is hosted by GitHub and provides the easiest method for getting up and running. If your MCP host does not support remote MCP servers, don't worry! You can use the [local version of the GitHub MCP Server](https://github.com/github/github-mcp-server?tab=readme-ov-file#local-github-mcp-server) instead.\n\n### Prerequisites\n\n1. A compatible MCP host with remote server support (VS Code 1.101+, Claude Desktop, Cursor, Windsurf, etc.)\n2. Any applicable [policies enabled](https://github.com/github/github-mcp-server/blob/main/docs/policies-and-governance.md)\n\n### Install in VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start. Make sure you're using [VS Code 1.101](https://code.visualstudio.com/updates/v1_101) or [later](https://code.visualstudio.com/updates) for remote MCP and OAuth support.\n\nAlternatively, to manually configure VS Code, choose the appropriate JSON block from the examples below and add it to your host configuration:\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eUsing OAuth\u003c/th\u003e\u003cth\u003eUsing a GitHub PAT\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003cth align=left colspan=2\u003eVS Code (version 1.101 or greater)\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${input:github_mcp_pat}\"\n      }\n    }\n  },\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_mcp_pat\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ]\n}\n```\n\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n### Install in other MCP hosts\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Applications](/docs/installation-guides/install-claude.md)** - Installation guide for Claude Web, Claude Desktop and Claude Code CLI\n- **[Cursor](/docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Windsurf](/docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n\n\u003e **Note:** Each MCP host application needs to configure a GitHub App or OAuth App to support remote access via OAuth. Any host application that supports remote MCP servers should support the remote GitHub server with PAT authentication. Configuration details and support levels vary by host. Make sure to refer to the host application's documentation for more info.\n\n### Configuration\n\n#### Toolset configuration\n\nSee [Remote Server Documentation](docs/remote-server.md) for full details on remote server configuration, toolsets, headers, and advanced usage. This file provides comprehensive instructions and examples for connecting, customizing, and installing the remote GitHub MCP Server in VS Code and other MCP hosts.\n\nWhen no toolsets are specified, [default toolsets](#default-toolset) are used.\n\n#### Enterprise Cloud with data residency (ghe.com)\n\nGitHub Enterprise Cloud can also make use of the remote server.\n\nExample for `https://octocorp.ghe.com`:\n```\n{\n    ...\n    \"proxima-github\": {\n      \"type\": \"http\",\n      \"url\": \"https://copilot-api.octocorp.ghe.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${input:github_mcp_pat}\"\n      }\n    },\n    ...\n}\n```\n\nGitHub Enterprise Server does not support remote server hosting. Please refer to [GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)](#github-enterprise-server-and-enterprise-cloud-with-data-residency-ghecom) from the local server configuration.\n\n---\n\n## Local GitHub MCP Server\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github\u0026inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D\u0026quality=insiders)\n\n### Prerequisites\n\n1. To run the server in a container, you will need to have [Docker](https://www.docker.com/) installed.\n2. Once Docker is installed, you will also need to ensure Docker is running. The image is public; if you get errors on pull, you may have an expired token and need to `docker logout ghcr.io`.\n3. Lastly you will need to [Create a GitHub Personal Access Token](https://github.com/settings/personal-access-tokens/new).\nThe MCP server can use many of the GitHub APIs, so enable the permissions that you feel comfortable granting your AI tools (to learn more about access tokens, please check out the [documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)).\n\n\u003cdetails\u003e\u003csummary\u003e\u003cb\u003eHandling PATs Securely\u003c/b\u003e\u003c/summary\u003e\n\n### Environment Variables (Recommended)\nTo keep your GitHub PAT secure and reusable across different MCP hosts:\n\n1. **Store your PAT in environment variables**\n   ```bash\n   export GITHUB_PAT=your_token_here\n   ```\n   Or create a `.env` file:\n   ```env\n   GITHUB_PAT=your_token_here\n   ```\n\n2. **Protect your `.env` file**\n   ```bash\n   # Add to .gitignore to prevent accidental commits\n   echo \".env\" \u003e\u003e .gitignore\n   ```\n\n3. **Reference the token in configurations**\n   ```bash\n   # CLI usage\n   claude mcp update github -e GITHUB_PERSONAL_ACCESS_TOKEN=$GITHUB_PAT\n\n   # In config files (where supported)\n   \"env\": {\n     \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"$GITHUB_PAT\"\n   }\n   ```\n\n\u003e **Note**: Environment variable support varies by host app and IDE. Some applications (like Windsurf) require hardcoded tokens in config files.\n\n### Token Security Best Practices\n\n- **Minimum scopes**: Only grant necessary permissions\n  - `repo` - Repository operations\n  - `read:packages` - Docker image access\n  - `read:org` - Organization team access\n- **Separate tokens**: Use different PATs for different projects/environments\n- **Regular rotation**: Update tokens periodically\n- **Never commit**: Keep tokens out of version control\n- **File permissions**: Restrict access to config files containing tokens\n  ```bash\n  chmod 600 ~/.your-app/config.json\n  ```\n\n\u003c/details\u003e\n\n### GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)\n\nThe flag `--gh-host` and the environment variable `GITHUB_HOST` can be used to set\nthe hostname for GitHub Enterprise Server or GitHub Enterprise Cloud with data residency.\n\n- For GitHub Enterprise Server, prefix the hostname with the `https://` URI scheme, as it otherwise defaults to `http://`, which GitHub Enterprise Server does not support.\n- For GitHub Enterprise Cloud with data residency, use `https://YOURSUBDOMAIN.ghe.com` as the hostname.\n``` json\n\"github\": {\n    \"command\": \"docker\",\n    \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"-e\",\n    \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n    \"-e\",\n    \"GITHUB_HOST\",\n    \"ghcr.io/github/github-mcp-server\"\n    ],\n    \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\",\n        \"GITHUB_HOST\": \"https://\u003cyour GHES or ghe.com domain name\u003e\"\n    }\n}\n```\n\n## Installation\n\n### Install in GitHub Copilot on VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start.\n\nMore about using MCP server tools in VS Code's [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\nInstall in GitHub Copilot on other IDEs (JetBrains, Visual Studio, Eclipse, etc.)\n\nAdd the following JSON block to your IDE's MCP settings.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"github_token\",\n        \"description\": \"GitHub Personal Access Token\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"github\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n          \"ghcr.io/github/github-mcp-server\"\n        ],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with other host applications that accept the same format.\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eExample JSON block without the MCP key included\u003c/b\u003e\u003c/summary\u003e\n\u003cbr\u003e\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_token\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"github\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n        \"ghcr.io/github/github-mcp-server\"\n      ],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n      }\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n### Install in Other MCP Hosts\n\nFor other MCP host applications, please refer to our installation guides:\n\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Code \u0026 Claude Desktop](docs/installation-guides/install-claude.md)** - Installation guide for Claude Code and Claude Desktop\n- **[Cursor](docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Google Gemini CLI](docs/installation-guides/install-gemini-cli.md)** - Installation guide for Google Gemini CLI\n- **[Windsurf](docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n\nFor a complete overview of all installation options, see our **[Installation Guides Index](docs/installation-guides)**.\n\n\u003e **Note:** Any host application that supports local MCP servers should be able to access the local GitHub MCP server. However, the specific configuration process, syntax and stability of the integration will vary by host application. While many may follow a similar format to the examples above, this is not guaranteed. Please refer to your host application's documentation for the correct MCP configuration syntax and setup process.\n\n### Build from source\n\nIf you don't have Docker, you can use `go build` to build the binary in the\n`cmd/github-mcp-server` directory, and use the `github-mcp-server stdio` command with the `GITHUB_PERSONAL_ACCESS_TOKEN` environment variable set to your token. To specify the output location of the build, use the `-o` flag. You should configure your server to use the built executable as its `command`. For example:\n\n```JSON\n{\n  \"mcp\": {\n    \"servers\": {\n      \"github\": {\n        \"command\": \"/path/to/github-mcp-server\",\n        \"args\": [\"stdio\"],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"\u003cYOUR_TOKEN\u003e\"\n        }\n      }\n    }\n  }\n}\n```\n\n## Tool Configuration\n\nThe GitHub MCP Server supports enabling or disabling specific groups of functionalities via the `--toolsets` flag. This allows you to control which GitHub API capabilities are available to your AI tools. Enabling only the toolsets that you need can help the LLM with tool choice and reduce the context size.\n\n_Toolsets are not limited to Tools. Relevant MCP Resources and Prompts are also included where applicable._\n\nWhen no toolsets are specified, [default toolsets](#default-toolset) are used.\n\n#### Specifying Toolsets\n\nTo specify toolsets you want available to the LLM, you can pass an allow-list in two ways:\n\n1. **Using Command Line Argument**:\n\n   ```bash\n   github-mcp-server --toolsets repos,issues,pull_requests,actions,code_security\n   ```\n\n2. **Using Environment Variable**:\n   ```bash\n   GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security\" ./github-mcp-server\n   ```\n\nThe environment variable `GITHUB_TOOLSETS` takes precedence over the command line argument if both are provided.\n\n### Using Toolsets With Docker\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=\u003cyour-token\u003e \\\n  -e GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security,experiments\" \\\n  ghcr.io/github/github-mcp-server\n```\n\n### Special toolsets\n\n#### \"all\" toolset\n\nThe special toolset `all` can be provided to enable all available toolsets regardless of any other configuration:\n\n```bash\n./github-mcp-server --toolsets all\n```\n\nOr using the environment variable:\n\n```bash\nGITHUB_TOOLSETS=\"all\" ./github-mcp-server\n```\n\n#### \"default\" toolset\nThe default toolset `default` is the configuration that gets passed to the server if no toolsets are specified.\n\nThe default configuration is:\n- context\n- repos\n- issues\n- pull_requests\n- users\n\nTo keep the default configuration and add additional toolsets:\n\n```bash\nGITHUB_TOOLSETS=\"default,stargazers\" ./github-mcp-server\n```\n\n### Available Toolsets\n\nThe following sets of tools are available:\n\n\u003c!-- START AUTOMATED TOOLSETS --\u003e\n| Toolset                 | Description                                                   |\n| ----------------------- | ------------------------------------------------------------- |\n| `context`               | **Strongly recommended**: Tools that provide context about the current user and GitHub context you are operating in |\n| `actions` | GitHub Actions workflows and CI/CD operations |\n| `code_security` | Code security related tools, such as GitHub Code Scanning |\n| `dependabot` | Dependabot tools |\n| `discussions` | GitHub Discussions related tools |\n| `experiments` | Experimental features that are not considered stable yet |\n| `gists` | GitHub Gist related tools |\n| `issues` | GitHub Issues related tools |\n| `labels` | GitHub Labels related tools |\n| `notifications` | GitHub Notifications related tools |\n| `orgs` | GitHub Organization related tools |\n| `projects` | GitHub Projects related tools |\n| `pull_requests` | GitHub Pull Request related tools |\n| `repos` | GitHub Repository related tools |\n| `secret_protection` | Secret protection related tools, such as GitHub Secret Scanning |\n| `security_advisories` | Security advisories related tools |\n| `stargazers` | GitHub Stargazers related tools |\n| `users` | GitHub User related tools |\n\u003c!-- END AUTOMATED TOOLSETS --\u003e\n\n### Additional Toolsets in Remote Github MCP Server\n\n| Toolset                 | Description                                                   |\n| ----------------------- | ------------------------------------------------------------- |\n| `copilot` | Copilot related tools (e.g. Copilot Coding Agent) |\n| `copilot_spaces` | Copilot Spaces related tools |\n| `github_support_docs_search` | Search docs to answer GitHub product and support questions |\n\n## Tools\n\n\u003c!-- START AUTOMATED TOOLS --\u003e\n\u003cdetails\u003e\n\n\u003csummary\u003eActions\u003c/summary\u003e\n\n- **cancel_workflow_run** - Cancel workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **delete_workflow_run_logs** - Delete workflow logs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **download_workflow_run_artifact** - Download workflow artifact\n  - `artifact_id`: The unique identifier of the artifact (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_job_logs** - Get job logs\n  - `failed_only`: When true, gets logs for all failed jobs in run_id (boolean, optional)\n  - `job_id`: The unique identifier of the workflow job (required for single job logs) (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `return_content`: Returns actual log content instead of URLs (boolean, optional)\n  - `run_id`: Workflow run ID (required when using failed_only) (number, optional)\n  - `tail_lines`: Number of lines to return from the end of the log (number, optional)\n\n- **get_workflow_run** - Get workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **get_workflow_run_logs** - Get workflow run logs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **get_workflow_run_usage** - Get workflow usage\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_jobs** - List workflow jobs\n  - `filter`: Filters jobs by their completed_at timestamp (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_run_artifacts** - List workflow artifacts\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **list_workflow_runs** - List workflow runs\n  - `actor`: Returns someone's workflow runs. Use the login for the user who created the workflow run. (string, optional)\n  - `branch`: Returns workflow runs associated with a branch. Use the name of the branch. (string, optional)\n  - `event`: Returns workflow runs for a specific event type (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `status`: Returns workflow runs with the check run status (string, optional)\n  - `workflow_id`: The workflow ID or workflow file name (string, required)\n\n- **list_workflows** - List workflows\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **rerun_failed_jobs** - Rerun failed jobs\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **rerun_workflow_run** - Rerun workflow run\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The unique identifier of the workflow run (number, required)\n\n- **run_workflow** - Run workflow\n  - `inputs`: Inputs the workflow accepts (object, optional)\n  - `owner`: Repository owner (string, required)\n  - `ref`: The git reference for the workflow. The reference can be a branch or tag name. (string, required)\n  - `repo`: Repository name (string, required)\n  - `workflow_id`: The workflow ID (numeric) or workflow file name (e.g., main.yml, ci.yaml) (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eCode Security\u003c/summary\u003e\n\n- **get_code_scanning_alert** - Get code scanning alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_code_scanning_alerts** - List code scanning alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `ref`: The Git reference for the results you want to list. (string, optional)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter code scanning alerts by severity (string, optional)\n  - `state`: Filter code scanning alerts by state. Defaults to open (string, optional)\n  - `tool_name`: The name of the tool used for code scanning. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eContext\u003c/summary\u003e\n\n- **get_me** - Get my user profile\n  - No parameters required\n\n- **get_team_members** - Get team members\n  - `org`: Organization login (owner) that contains the team. (string, required)\n  - `team_slug`: Team slug (string, required)\n\n- **get_teams** - Get teams\n  - `user`: Username to get teams for. If not provided, uses the authenticated user. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eDependabot\u003c/summary\u003e\n\n- **get_dependabot_alert** - Get dependabot alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_dependabot_alerts** - List dependabot alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter dependabot alerts by severity (string, optional)\n  - `state`: Filter dependabot alerts by state. Defaults to open (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eDiscussions\u003c/summary\u003e\n\n- **get_discussion** - Get discussion\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_discussion_comments** - Get discussion comments\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_discussion_categories** - List discussion categories\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name. If not provided, discussion categories will be queried at the organisation level. (string, optional)\n\n- **list_discussions** - List discussions\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `category`: Optional filter by discussion category ID. If provided, only discussions with this category are listed. (string, optional)\n  - `direction`: Order direction. (string, optional)\n  - `orderBy`: Order discussions by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name. If not provided, discussions will be queried at the organisation level. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eGists\u003c/summary\u003e\n\n- **create_gist** - Create Gist\n  - `content`: Content for simple single-file gist creation (string, required)\n  - `description`: Description of the gist (string, optional)\n  - `filename`: Filename for simple single-file gist creation (string, required)\n  - `public`: Whether the gist is public (boolean, optional)\n\n- **list_gists** - List Gists\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `since`: Only gists updated after this time (ISO 8601 timestamp) (string, optional)\n  - `username`: GitHub username (omit for authenticated user's gists) (string, optional)\n\n- **update_gist** - Update Gist\n  - `content`: Content for the file (string, required)\n  - `description`: Updated description of the gist (string, optional)\n  - `filename`: Filename to update or create (string, required)\n  - `gist_id`: ID of the gist to update (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eIssues\u003c/summary\u003e\n\n- **add_issue_comment** - Add comment to issue\n  - `body`: Comment content (string, required)\n  - `issue_number`: Issue number to comment on (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **add_sub_issue** - Add sub-issue\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `replace_parent`: When true, replaces the sub-issue's current parent issue (boolean, optional)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to add. ID is not the same as issue number (number, required)\n\n- **assign_copilot_to_issue** - Assign Copilot to issue\n  - `issueNumber`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **create_issue** - Open new issue\n  - `assignees`: Usernames to assign to this issue (string[], optional)\n  - `body`: Issue body content (string, optional)\n  - `labels`: Labels to apply to this issue (string[], optional)\n  - `milestone`: Milestone number (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `title`: Issue title (string, required)\n  - `type`: Type of this issue (string, optional)\n\n- **get_issue** - Get issue details\n  - `issue_number`: The number of the issue (number, required)\n  - `owner`: The owner of the repository (string, required)\n  - `repo`: The name of the repository (string, required)\n\n- **get_issue_comments** - Get issue comments\n  - `issue_number`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **get_label** - Get a specific label from a repository.\n  - `name`: Label name. (string, required)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **list_issue_types** - List available issue types\n  - `owner`: The organization owner of the repository (string, required)\n\n- **list_issues** - List issues\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `direction`: Order direction. If provided, the 'orderBy' also needs to be provided. (string, optional)\n  - `labels`: Filter by labels (string[], optional)\n  - `orderBy`: Order issues by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `since`: Filter by date (ISO 8601 timestamp) (string, optional)\n  - `state`: Filter by state, by default both open and closed issues are returned when not provided (string, optional)\n\n- **list_label** - List labels from a repository or an issue\n  - `issue_number`: Issue number - if provided, lists labels on the specific issue (number, optional)\n  - `owner`: Repository owner (username or organization name) - required for all operations (string, required)\n  - `repo`: Repository name - required for all operations (string, required)\n\n- **list_sub_issues** - List sub-issues\n  - `issue_number`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (default: 1) (number, optional)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **remove_sub_issue** - Remove sub-issue\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to remove. ID is not the same as issue number (number, required)\n\n- **reprioritize_sub_issue** - Reprioritize sub-issue\n  - `after_id`: The ID of the sub-issue to be prioritized after (either after_id OR before_id should be specified) (number, optional)\n  - `before_id`: The ID of the sub-issue to be prioritized before (either after_id OR before_id should be specified) (number, optional)\n  - `issue_number`: The number of the parent issue (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to reprioritize. ID is not the same as issue number (number, required)\n\n- **search_issues** - Search issues\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only issues for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub issues search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only issues for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **update_issue** - Edit issue\n  - `assignees`: New assignees (string[], optional)\n  - `body`: New description (string, optional)\n  - `duplicate_of`: Issue number that this issue is a duplicate of. Only used when state_reason is 'duplicate'. (number, optional)\n  - `issue_number`: Issue number to update (number, required)\n  - `labels`: New labels (string[], optional)\n  - `milestone`: New milestone number (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `state`: New state (string, optional)\n  - `state_reason`: Reason for the state change. Ignored unless state is changed. (string, optional)\n  - `title`: New title (string, optional)\n  - `type`: New issue type (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eLabels\u003c/summary\u003e\n\n- **get_label** - Get a specific label from a repository.\n  - `name`: Label name. (string, required)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **label_write** - Write operations on repository labels.\n  - `color`: Label color as 6-character hex code without '#' prefix (e.g., 'f29513'). Required for 'create', optional for 'update'. (string, optional)\n  - `description`: Label description text. Optional for 'create' and 'update'. (string, optional)\n  - `method`: Operation to perform: 'create', 'update', or 'delete' (string, required)\n  - `name`: Label name - required for all operations (string, required)\n  - `new_name`: New name for the label (used only with 'update' method to rename) (string, optional)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **list_label** - List labels from a repository or an issue\n  - `issue_number`: Issue number - if provided, lists labels on the specific issue (number, optional)\n  - `owner`: Repository owner (username or organization name) - required for all operations (string, required)\n  - `repo`: Repository name - required for all operations (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eNotifications\u003c/summary\u003e\n\n- **dismiss_notification** - Dismiss notification\n  - `state`: The new state of the notification (read/done) (string, optional)\n  - `threadID`: The ID of the notification thread (string, required)\n\n- **get_notification_details** - Get notification details\n  - `notificationID`: The ID of the notification (string, required)\n\n- **list_notifications** - List notifications\n  - `before`: Only show notifications updated before the given time (ISO 8601 format) (string, optional)\n  - `filter`: Filter notifications to, use default unless specified. Read notifications are ones that have already been acknowledged by the user. Participating notifications are those that the user is directly involved in, such as issues or pull requests they have commented on or created. (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are listed. (string, optional)\n  - `since`: Only show notifications updated after the given time (ISO 8601 format) (string, optional)\n\n- **manage_notification_subscription** - Manage notification subscription\n  - `action`: Action to perform: ignore, watch, or delete the notification subscription. (string, required)\n  - `notificationID`: The ID of the notification thread. (string, required)\n\n- **manage_repository_notification_subscription** - Manage repository notification subscription\n  - `action`: Action to perform: ignore, watch, or delete the repository notification subscription. (string, required)\n  - `owner`: The account owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **mark_all_notifications_read** - Mark all notifications as read\n  - `lastReadAt`: Describes the last point that notifications were checked (optional). Default: Now (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are marked as read. (string, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are marked as read. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eOrganizations\u003c/summary\u003e\n\n- **search_orgs** - Search organizations\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Organization search query. Examples: 'microsoft', 'location:california', 'created:\u003e=2025-01-01'. Search is automatically scoped to type:org. (string, required)\n  - `sort`: Sort field by category (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eProjects\u003c/summary\u003e\n\n- **add_project_item** - Add project item\n  - `item_id`: The numeric ID of the issue or pull request to add to the project. (number, required)\n  - `item_type`: The item's type, either issue or pull_request. (string, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **delete_project_item** - Delete project item\n  - `item_id`: The internal project item ID to delete from the project (not the issue or pull request ID). (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **get_project** - Get project\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number (number, required)\n\n- **get_project_field** - Get project field\n  - `field_id`: The field's id. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **get_project_item** - Get project item\n  - `item_id`: The item's ID. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n\n- **list_project_fields** - List project fields\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `project_number`: The project's number. (number, required)\n\n- **list_project_items** - List project items\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `project_number`: The project's number. (number, required)\n  - `query`: Search query to filter items (string, optional)\n\n- **list_projects** - List projects\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `per_page`: Number of results per page (max 100, default: 30) (number, optional)\n  - `query`: Filter projects by a search query (matches title and description) (string, optional)\n\n- **update_project_item** - Update project item\n  - `item_id`: The unique identifier of the project item. This is not the issue or pull request ID. (number, required)\n  - `owner`: If owner_type == user it is the handle for the GitHub user account. If owner_type == org it is the name of the organization. The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (string, required)\n  - `project_number`: The project's number. (number, required)\n  - `updated_field`: Object consisting of the ID of the project field to update and the new value for the field. To clear the field, set \"value\" to null. Example: {\"id\": 123456, \"value\": \"New Value\"} (object, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003ePull Requests\u003c/summary\u003e\n\n- **add_comment_to_pending_review** - Add review comment to the requester's latest pending pull request review\n  - `body`: The text of the review comment (string, required)\n  - `line`: The line of the blob in the pull request diff that the comment applies to. For multi-line comments, the last line of the range (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `path`: The relative path to the file that necessitates a comment (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n  - `side`: The side of the diff to comment on. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `startLine`: For multi-line comments, the first line of the range that the comment applies to (number, optional)\n  - `startSide`: For multi-line comments, the starting side of the diff that the comment applies to. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `subjectType`: The level at which the comment is targeted (string, required)\n\n- **create_pull_request** - Open new pull request\n  - `base`: Branch to merge into (string, required)\n  - `body`: PR description (string, optional)\n  - `draft`: Create as draft PR (boolean, optional)\n  - `head`: Branch containing changes (string, required)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `title`: PR title (string, required)\n\n- **list_pull_requests** - List pull requests\n  - `base`: Filter by base branch (string, optional)\n  - `direction`: Sort direction (string, optional)\n  - `head`: Filter by head user/org and branch (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sort`: Sort by (string, optional)\n  - `state`: Filter by state (string, optional)\n\n- **merge_pull_request** - Merge pull request\n  - `commit_message`: Extra detail for merge commit (string, optional)\n  - `commit_title`: Title for merge commit (string, optional)\n  - `merge_method`: Merge method (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **pull_request_read** - Get details for a single pull request\n  - `method`: Action to specify what pull request data needs to be retrieved from GitHub. \nPossible options: \n 1. get - Get details of a specific pull request.\n 2. get_diff - Get the diff of a pull request.\n 3. get_status - Get status of a head commit in a pull request. This reflects status of builds and checks.\n 4. get_files - Get the list of files changed in a pull request. Use with pagination parameters to control the number of results returned.\n 5. get_review_comments - Get the review comments on a pull request. Use with pagination parameters to control the number of results returned.\n 6. get_reviews - Get the reviews on a pull request. When asked for review comments, use get_review_comments method.\n (string, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **pull_request_review_write** - Write operations (create, submit, delete) on pull request reviews.\n  - `body`: Review comment text (string, optional)\n  - `commitID`: SHA of commit to review (string, optional)\n  - `event`: Review action to perform. (string, optional)\n  - `method`: The write operation to perform on pull request review. (string, required)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **request_copilot_review** - Request Copilot review\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **search_pull_requests** - Search pull requests\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only pull requests for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub pull request search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only pull requests for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **update_pull_request** - Edit pull request\n  - `base`: New base branch name (string, optional)\n  - `body`: New description (string, optional)\n  - `draft`: Mark pull request as draft (true) or ready for review (false) (boolean, optional)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number to update (number, required)\n  - `repo`: Repository name (string, required)\n  - `reviewers`: GitHub usernames to request reviews from (string[], optional)\n  - `state`: New state (string, optional)\n  - `title`: New title (string, optional)\n\n- **update_pull_request_branch** - Update pull request branch\n  - `expectedHeadSha`: The expected SHA of the pull request's HEAD ref (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eRepositories\u003c/summary\u003e\n\n- **create_branch** - Create branch\n  - `branch`: Name for new branch (string, required)\n  - `from_branch`: Source branch (defaults to repo default) (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **create_or_update_file** - Create or update file\n  - `branch`: Branch to create/update the file in (string, required)\n  - `content`: Content of the file (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path where to create/update the file (string, required)\n  - `repo`: Repository name (string, required)\n  - `sha`: Required if updating an existing file. The blob SHA of the file being replaced. (string, optional)\n\n- **create_repository** - Create repository\n  - `autoInit`: Initialize with README (boolean, optional)\n  - `description`: Repository description (string, optional)\n  - `name`: Repository name (string, required)\n  - `organization`: Organization to create the repository in (omit to create in your personal account) (string, optional)\n  - `private`: Whether repo should be private (boolean, optional)\n\n- **delete_file** - Delete file\n  - `branch`: Branch to delete the file from (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to the file to delete (string, required)\n  - `repo`: Repository name (string, required)\n\n- **fork_repository** - Fork repository\n  - `organization`: Organization to fork to (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_commit** - Get commit details\n  - `include_diff`: Whether to include file diffs and stats in the response. Default is true. (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch name, or tag name (string, required)\n\n- **get_file_contents** - Get file or directory contents\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to file/directory (directories must end with a slash '/') (string, optional)\n  - `ref`: Accepts optional git refs such as `refs/tags/{tag}`, `refs/heads/{branch}` or `refs/pull/{pr_number}/head` (string, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Accepts optional commit SHA. If specified, it will be used instead of ref (string, optional)\n\n- **get_latest_release** - Get latest release\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_release_by_tag** - Get a release by tag name\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (e.g., 'v1.0.0') (string, required)\n\n- **get_tag** - Get tag details\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (string, required)\n\n- **list_branches** - List branches\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_commits** - List commits\n  - `author`: Author username or email address to filter commits by (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch or tag name to list commits of. If not provided, uses the default branch of the repository. If a commit SHA is provided, will list commits up to that SHA. (string, optional)\n\n- **list_releases** - List releases\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_tags** - List tags\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **push_files** - Push files to repository\n  - `branch`: Branch to push to (string, required)\n  - `files`: Array of file objects to push, each object with path (string) and content (string) (object[], required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **search_code** - Search code\n  - `order`: Sort order for results (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub's powerful code search syntax. Examples: 'content:Skill language:Java org:github', 'NOT is:archived language:Python OR language:go', 'repo:github/github-mcp-server'. Supports exact matching, language filters, path filters, and more. (string, required)\n  - `sort`: Sort field ('indexed' only) (string, optional)\n\n- **search_repositories** - Search repositories\n  - `minimal_output`: Return minimal repository information (default: true). When false, returns full GitHub API repository objects. (boolean, optional)\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Repository search query. Examples: 'machine learning in:name stars:\u003e1000 language:python', 'topic:react', 'user:facebook'. Supports advanced search syntax for precise filtering. (string, required)\n  - `sort`: Sort repositories by field, defaults to best match (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eSecret Protection\u003c/summary\u003e\n\n- **get_secret_scanning_alert** - Get secret scanning alert\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_secret_scanning_alerts** - List secret scanning alerts\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `resolution`: Filter by resolution (string, optional)\n  - `secret_type`: A comma-separated list of secret types to return. All default secret patterns are returned. To return generic patterns, pass the token name(s) in the parameter. (string, optional)\n  - `state`: Filter by state (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eSecurity Advisories\u003c/summary\u003e\n\n- **get_global_security_advisory** - Get a global security advisory\n  - `ghsaId`: GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, required)\n\n- **list_global_security_advisories** - List global security advisories\n  - `affects`: Filter advisories by affected package or version (e.g. \"package1,package2@1.0.0\"). (string, optional)\n  - `cveId`: Filter by CVE ID. (string, optional)\n  - `cwes`: Filter by Common Weakness Enumeration IDs (e.g. [\"79\", \"284\", \"22\"]). (string[], optional)\n  - `ecosystem`: Filter by package ecosystem. (string, optional)\n  - `ghsaId`: Filter by GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, optional)\n  - `isWithdrawn`: Whether to only return withdrawn advisories. (boolean, optional)\n  - `modified`: Filter by publish or update date or date range (ISO 8601 date or range). (string, optional)\n  - `published`: Filter by publish date or date range (ISO 8601 date or range). (string, optional)\n  - `severity`: Filter by severity. (string, optional)\n  - `type`: Advisory type. (string, optional)\n  - `updated`: Filter by update date or date range (ISO 8601 date or range). (string, optional)\n\n- **list_org_repository_security_advisories** - List org repository security advisories\n  - `direction`: Sort direction. (string, optional)\n  - `org`: The organization login. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n- **list_repository_security_advisories** - List repository security advisories\n  - `direction`: Sort direction. (string, optional)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eStargazers\u003c/summary\u003e\n\n- **list_starred_repositories** - List starred repositories\n  - `direction`: The direction to sort the results by. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `sort`: How to sort the results. Can be either 'created' (when the repository was starred) or 'updated' (when the repository was last pushed to). (string, optional)\n  - `username`: Username to list starred repositories for. Defaults to the authenticated user. (string, optional)\n\n- **star_repository** - Star repository\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **unstar_repository** - Unstar repository\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eUsers\u003c/summary\u003e\n\n- **search_users** - Search users\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: User search query. Examples: 'john smith', 'location:seattle', 'followers:\u003e100'. Search is automatically scoped to type:user. (string, required)\n  - `sort`: Sort users by number of followers or repositories, or when the person joined GitHub. (string, optional)\n\n\u003c/details\u003e\n\u003c!-- END AUTOMATED TOOLS --\u003e\n\n### Additional Tools in Remote Github MCP Server\n\n\u003cdetails\u003e\n\n\u003csummary\u003eCopilot\u003c/summary\u003e\n\n-   **create_pull_request_with_copilot** - Perform task with GitHub Copilot coding agent\n    -   `owner`: Repository owner. You can guess the owner, but confirm it with the user before proceeding. (string, required)\n    -   `repo`: Repository name. You can guess the repository name, but confirm it with the user before proceeding. (string, required)\n    -   `problem_statement`: Detailed description of the task to be performed (e.g., 'Implement a feature that does X', 'Fix bug Y', etc.) (string, required)\n    -   `title`: Title for the pull request that will be created (string, required)\n    -   `base_ref`: Git reference (e.g., branch) that the agent will start its work from. If not specified, defaults to the repository's default branch (string, optional)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eCopilot Spaces\u003c/summary\u003e\n\n-   **get_copilot_space** - Get Copilot Space\n    -   `owner`: The owner of the space. (string, required)\n    -   `name`: The name of the space. (string, required)\n\n-   **list_copilot_spaces** - List Copilot Spaces\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eGitHub Support Docs Search\u003c/summary\u003e\n\n-   **github_support_docs_search** - Retrieve documentation relevant to answer GitHub product and support questions. Support topics include: GitHub Actions Workflows, Authentication, GitHub Support Inquiries, Pull Request Practices, Repository Maintenance, GitHub Pages, GitHub Packages, GitHub Discussions, Copilot Spaces\n    -   `query`: Input from the user about the question they need answered. This is the latest raw unedited user message. You should ALWAYS leave the user message as it is, you should never modify it. (string, required)\n\u003c/details\u003e\n\n## Dynamic Tool Discovery\n\n**Note**: This feature is currently in beta and may not be available in all environments. Please test it out and let us know if you encounter any issues.\n\nInstead of starting with all tools enabled, you can turn on dynamic toolset discovery. Dynamic toolsets allow the MCP host to list and enable toolsets in response to a user prompt. This should help to avoid situations where the model gets confused by the sheer number of tools available.\n\n### Using Dynamic Tool Discovery\n\nWhen using the binary, you can pass the `--dynamic-toolsets` flag.\n\n```bash\n./github-mcp-server --dynamic-toolsets\n```\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=\u003cyour-token\u003e \\\n  -e GITHUB_DYNAMIC_TOOLSETS=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## Read-Only Mode\n\nTo run the server in read-only mode, you can use the `--read-only` flag. This will only offer read-only tools, preventing any modifications to repositories, issues, pull requests, etc.\n\n```bash\n./github-mcp-server --read-only\n```\n\nWhen using Docker, you can pass the read-only mode as an environment variable:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=\u003cyour-token\u003e \\\n  -e GITHUB_READ_ONLY=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## i18n / Overriding Descriptions\n\nThe descriptions of the tools can be overridden by creating a\n`github-mcp-server-config.json` file in the same directory as the binary.\n\nThe file should contain a JSON object with the tool names as keys and the new\ndescriptions as values. For example:\n\n```json\n{\n  \"TOOL_ADD_ISSUE_COMMENT_DESCRIPTION\": \"an alternative description\",\n  \"TOOL_CREATE_BRANCH_DESCRIPTION\": \"Create a new branch in a GitHub repository\"\n}\n```\n\nYou can create an export of the current translations by running the binary with\nthe `--export-translations` flag.\n\nThis flag will preserve any translations/overrides you have made, while adding\nany new translations that have been added to the binary since the last time you\nexported.\n\n```sh\n./github-mcp-server --export-translations\ncat github-mcp-server-config.json\n```\n\nYou can also use ENV vars to override the descriptions. The environment\nvariable names are the same as the keys in the JSON file, prefixed with\n`GITHUB_MCP_` and all uppercase.\n\nFor example, to override the `TOOL_ADD_ISSUE_COMMENT_DESCRIPTION` tool, you can\nset the following environment variable:\n\n```sh\nexport GITHUB_MCP_TOOL_ADD_ISSUE_COMMENT_DESCRIPTION=\"an alternative description\"\n```\n\n## Library Usage\n\nThe exported Go API of this module should currently be considered unstable, and subject to breaking changes. In the future, we may offer stability; please file an issue if there is a use case where this would be valuable.\n\n## License\n\nThis project is licensed under the terms of the MIT open source license. Please refer to [MIT](./LICENSE) for the full terms.\n","stargazer_count":23800,"topics":["github","mcp","mcp-server"],"uses_custom_opengraph_image":false}}}},{"name":"microsoft/playwright-mcp","description":"Automate web browsers using accessibility trees for testing and data extraction.","status":"active","repository":{"url":"https://github.com/microsoft/playwright-mcp","source":"github","id":"952688112","readme":"## Playwright MCP\n\nA Model Context Protocol (MCP) server that provides browser automation capabilities using [Playwright](https://playwright.dev). This server enables LLMs to interact with web pages through structured accessibility snapshots, bypassing the need for screenshots or visually-tuned models.\n\n### Key Features\n\n- **Fast and lightweight**. Uses Playwright's accessibility tree, not pixel-based input.\n- **LLM-friendly**. No vision models needed, operates purely on structured data.\n- **Deterministic tool application**. Avoids ambiguity common with screenshot-based approaches.\n\n### Requirements\n- Node.js 18 or newer\n- VS Code, Cursor, Windsurf, Claude Desktop, Goose or any other MCP client\n\n\u003c!--\n// Generate using:\nnode utils/generate-links.js\n--\u003e\n\n### Getting started\n\nFirst, install the Playwright MCP server with your client.\n\n**Standard config** works in most of the tools:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n[\u003cimg src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Server\u0026color=0098FF\" alt=\"Install in VS Code\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [\u003cimg alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Server\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n\u003cdetails\u003e\n\u003csummary\u003eAmp\u003c/summary\u003e\n\nAdd via the Amp VS Code extension settings screen or by updating your settings.json file:\n\n```json\n\"amp.mcpServers\": {\n  \"playwright\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"@playwright/mcp@latest\"\n    ]\n  }\n}\n```\n\n**Amp CLI Setup:**\n\nAdd via the `amp mcp add`command below\n\n```bash\namp mcp add playwright -- npx @playwright/mcp@latest\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eClaude Code\u003c/summary\u003e\n\nUse the Claude Code CLI to add the Playwright MCP server:\n\n```bash\nclaude mcp add playwright npx @playwright/mcp@latest\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eClaude Desktop\u003c/summary\u003e\n\nFollow the MCP install [guide](https://modelcontextprotocol.io/quickstart/user), use the standard config above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eCodex\u003c/summary\u003e\n\nCreate or edit the configuration file `~/.codex/config.toml` and add:\n\n```toml\n[mcp_servers.playwright]\ncommand = \"npx\"\nargs = [\"@playwright/mcp@latest\"]\n```\n\nFor more information, see the [Codex MCP documentation](https://github.com/openai/codex/blob/main/codex-rs/config.md#mcp_servers).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eCursor\u003c/summary\u003e\n\n#### Click the button to install:\n\n[\u003cimg src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\"\u003e](https://cursor.com/en/install-mcp?name=Playwright\u0026config=eyJjb21tYW5kIjoibnB4IEBwbGF5d3JpZ2h0L21jcEBsYXRlc3QifQ%3D%3D)\n\n#### Or install manually:\n\nGo to `Cursor Settings` -\u003e `MCP` -\u003e `Add new MCP Server`. Name to your liking, use `command` type with the command `npx @playwright/mcp@latest`. You can also verify config or add command like arguments via clicking `Edit`.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eGemini CLI\u003c/summary\u003e\n\nFollow the MCP install [guide](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#configure-the-mcp-server-in-settingsjson), use the standard config above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eGoose\u003c/summary\u003e\n\n#### Click the button to install:\n\n[![Install in Goose](https://block.github.io/goose/img/extension-install-dark.svg)](https://block.github.io/goose/extension?cmd=npx\u0026arg=%40playwright%2Fmcp%40latest\u0026id=playwright\u0026name=Playwright\u0026description=Interact%20with%20web%20pages%20through%20structured%20accessibility%20snapshots%20using%20Playwright)\n\n#### Or install manually:\n\nGo to `Advanced settings` -\u003e `Extensions` -\u003e `Add custom extension`. Name to your liking, use type `STDIO`, and set the `command` to `npx @playwright/mcp`. Click \"Add Extension\".\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eKiro\u003c/summary\u003e\n\nFollow the MCP Servers [documentation](https://kiro.dev/docs/mcp/). For example in `.kiro/settings/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eLM Studio\u003c/summary\u003e\n\n#### Click the button to install:\n\n[![Add MCP Server playwright to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=playwright\u0026config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyJAcGxheXdyaWdodC9tY3BAbGF0ZXN0Il19)\n\n#### Or install manually:\n\nGo to `Program` in the right sidebar -\u003e `Install` -\u003e `Edit mcp.json`. Use the standard config above.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eopencode\u003c/summary\u003e\n\nFollow the MCP Servers [documentation](https://opencode.ai/docs/mcp-servers/). For example in `~/.config/opencode/opencode.json`:\n\n```json\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"playwright\": {\n      \"type\": \"local\",\n      \"command\": [\n        \"npx\",\n        \"@playwright/mcp@latest\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eQodo Gen\u003c/summary\u003e\n\nOpen [Qodo Gen](https://docs.qodo.ai/qodo-documentation/qodo-gen) chat panel in VSCode or IntelliJ → Connect more tools → + Add new MCP → Paste the standard config above.\n\nClick \u003ccode\u003eSave\u003c/code\u003e.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eVS Code\u003c/summary\u003e\n\n#### Click the button to install:\n\n[\u003cimg src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Server\u0026color=0098FF\" alt=\"Install in VS Code\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [\u003cimg alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Server\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n#### Or install manually:\n\nFollow the MCP install [guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server), use the standard config above. You can also install the Playwright MCP server using the VS Code CLI:\n\n```bash\n# For VS Code\ncode --add-mcp '{\"name\":\"playwright\",\"command\":\"npx\",\"args\":[\"@playwright/mcp@latest\"]}'\n```\n\nAfter installation, the Playwright MCP server will be available for use with your GitHub Copilot agent in VS Code.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eWarp\u003c/summary\u003e\n\nGo to `Settings` -\u003e `AI` -\u003e `Manage MCP Servers` -\u003e `+ Add` to [add an MCP Server](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server). Use the standard config above.\n\nAlternatively, use the slash command `/add-mcp` in the Warp prompt and paste the standard config from above:\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eWindsurf\u003c/summary\u003e\n\nFollow Windsurf MCP [documentation](https://docs.windsurf.com/windsurf/cascade/mcp). Use the standard config above.\n\n\u003c/details\u003e\n\n### Configuration\n\nPlaywright MCP server supports following arguments. They can be provided in the JSON configuration above, as a part of the `\"args\"` list:\n\n\u003c!--- Options generated by update-readme.js --\u003e\n\n```\n\u003e npx @playwright/mcp@latest --help\n  --allowed-hosts \u003chosts...\u003e            comma-separated list of hosts this\n                                        server is allowed to serve from.\n                                        Defaults to the host the server is bound\n                                        to. Pass '*' to disable the host check.\n  --allowed-origins \u003corigins\u003e           semicolon-separated list of origins to\n                                        allow the browser to request. Default is\n                                        to allow all.\n  --blocked-origins \u003corigins\u003e           semicolon-separated list of origins to\n                                        block the browser from requesting.\n                                        Blocklist is evaluated before allowlist.\n                                        If used without the allowlist, requests\n                                        not matching the blocklist are still\n                                        allowed.\n  --block-service-workers               block service workers\n  --browser \u003cbrowser\u003e                   browser or chrome channel to use,\n                                        possible values: chrome, firefox,\n                                        webkit, msedge.\n  --caps \u003ccaps\u003e                         comma-separated list of additional\n                                        capabilities to enable, possible values:\n                                        vision, pdf.\n  --cdp-endpoint \u003cendpoint\u003e             CDP endpoint to connect to.\n  --cdp-header \u003cheaders...\u003e             CDP headers to send with the connect\n                                        request, multiple can be specified.\n  --config \u003cpath\u003e                       path to the configuration file.\n  --device \u003cdevice\u003e                     device to emulate, for example: \"iPhone\n                                        15\"\n  --executable-path \u003cpath\u003e              path to the browser executable.\n  --extension                           Connect to a running browser instance\n                                        (Edge/Chrome only). Requires the\n                                        \"Playwright MCP Bridge\" browser\n                                        extension to be installed.\n  --grant-permissions \u003cpermissions...\u003e  List of permissions to grant to the\n                                        browser context, for example\n                                        \"geolocation\", \"clipboard-read\",\n                                        \"clipboard-write\".\n  --headless                            run browser in headless mode, headed by\n                                        default\n  --host \u003chost\u003e                         host to bind server to. Default is\n                                        localhost. Use 0.0.0.0 to bind to all\n                                        interfaces.\n  --ignore-https-errors                 ignore https errors\n  --init-script \u003cpath...\u003e               path to JavaScript file to add as an\n                                        initialization script. The script will\n                                        be evaluated in every page before any of\n                                        the page's scripts. Can be specified\n                                        multiple times.\n  --isolated                            keep the browser profile in memory, do\n                                        not save it to disk.\n  --image-responses \u003cmode\u003e              whether to send image responses to the\n                                        client. Can be \"allow\" or \"omit\",\n                                        Defaults to \"allow\".\n  --no-sandbox                          disable the sandbox for all process\n                                        types that are normally sandboxed.\n  --output-dir \u003cpath\u003e                   path to the directory for output files.\n  --port \u003cport\u003e                         port to listen on for SSE transport.\n  --proxy-bypass \u003cbypass\u003e               comma-separated domains to bypass proxy,\n                                        for example\n                                        \".com,chromium.org,.domain.com\"\n  --proxy-server \u003cproxy\u003e                specify proxy server, for example\n                                        \"http://myproxy:3128\" or\n                                        \"socks5://myproxy:8080\"\n  --save-session                        Whether to save the Playwright MCP\n                                        session into the output directory.\n  --save-trace                          Whether to save the Playwright Trace of\n                                        the session into the output directory.\n  --save-video \u003csize\u003e                   Whether to save the video of the session\n                                        into the output directory. For example\n                                        \"--save-video=800x600\"\n  --secrets \u003cpath\u003e                      path to a file containing secrets in the\n                                        dotenv format\n  --shared-browser-context              reuse the same browser context between\n                                        all connected HTTP clients.\n  --storage-state \u003cpath\u003e                path to the storage state file for\n                                        isolated sessions.\n  --test-id-attribute \u003cattribute\u003e       specify the attribute to use for test\n                                        ids, defaults to \"data-testid\"\n  --timeout-action \u003ctimeout\u003e            specify action timeout in milliseconds,\n                                        defaults to 5000ms\n  --timeout-navigation \u003ctimeout\u003e        specify navigation timeout in\n                                        milliseconds, defaults to 60000ms\n  --user-agent \u003cua string\u003e              specify user agent string\n  --user-data-dir \u003cpath\u003e                path to the user data directory. If not\n                                        specified, a temporary directory will be\n                                        created.\n  --viewport-size \u003csize\u003e                specify browser viewport size in pixels,\n                                        for example \"1280x720\"\n```\n\n\u003c!--- End of options generated section --\u003e\n\n### User profile\n\nYou can run Playwright MCP with persistent profile like a regular browser (default), in isolated contexts for testing sessions, or connect to your existing browser using the browser extension.\n\n**Persistent profile**\n\nAll the logged in information will be stored in the persistent profile, you can delete it between sessions if you'd like to clear the offline state.\nPersistent profile is located at the following locations and you can override it with the `--user-data-dir` argument.\n\n```bash\n# Windows\n%USERPROFILE%\\AppData\\Local\\ms-playwright\\mcp-{channel}-profile\n\n# macOS\n- ~/Library/Caches/ms-playwright/mcp-{channel}-profile\n\n# Linux\n- ~/.cache/ms-playwright/mcp-{channel}-profile\n```\n\n**Isolated**\n\nIn the isolated mode, each session is started in the isolated profile. Every time you ask MCP to close the browser,\nthe session is closed and all the storage state for this session is lost. You can provide initial storage state\nto the browser via the config's `contextOptions` or via the `--storage-state` argument. Learn more about the storage\nstate [here](https://playwright.dev/docs/auth).\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--isolated\",\n        \"--storage-state={path/to/storage.json}\"\n      ]\n    }\n  }\n}\n```\n\n**Browser Extension**\n\nThe Playwright MCP Chrome Extension allows you to connect to existing browser tabs and leverage your logged-in sessions and browser state. See [extension/README.md](extension/README.md) for installation and setup instructions.\n\n### Configuration file\n\nThe Playwright MCP server can be configured using a JSON configuration file. You can specify the configuration file\nusing the `--config` command line option:\n\n```bash\nnpx @playwright/mcp@latest --config path/to/config.json\n```\n\n\u003cdetails\u003e\n\u003csummary\u003eConfiguration file schema\u003c/summary\u003e\n\n```typescript\n{\n  // Browser configuration\n  browser?: {\n    // Browser type to use (chromium, firefox, or webkit)\n    browserName?: 'chromium' | 'firefox' | 'webkit';\n\n    // Keep the browser profile in memory, do not save it to disk.\n    isolated?: boolean;\n\n    // Path to user data directory for browser profile persistence\n    userDataDir?: string;\n\n    // Browser launch options (see Playwright docs)\n    // @see https://playwright.dev/docs/api/class-browsertype#browser-type-launch\n    launchOptions?: {\n      channel?: string;        // Browser channel (e.g. 'chrome')\n      headless?: boolean;      // Run in headless mode\n      executablePath?: string; // Path to browser executable\n      // ... other Playwright launch options\n    };\n\n    // Browser context options\n    // @see https://playwright.dev/docs/api/class-browser#browser-new-context\n    contextOptions?: {\n      viewport?: { width: number, height: number };\n      // ... other Playwright context options\n    };\n\n    // CDP endpoint for connecting to existing browser\n    cdpEndpoint?: string;\n\n    // Remote Playwright server endpoint\n    remoteEndpoint?: string;\n  },\n\n  // Server configuration\n  server?: {\n    port?: number;  // Port to listen on\n    host?: string;  // Host to bind to (default: localhost)\n  },\n\n  // List of additional capabilities\n  capabilities?: Array\u003c\n    'tabs' |    // Tab management\n    'install' | // Browser installation\n    'pdf' |     // PDF generation\n    'vision' |  // Coordinate-based interactions\n  \u003e;\n\n  // Directory for output files\n  outputDir?: string;\n\n  // Network configuration\n  network?: {\n    // List of origins to allow the browser to request. Default is to allow all. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    allowedOrigins?: string[];\n\n    // List of origins to block the browser to request. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    blockedOrigins?: string[];\n  };\n \n  /**\n   * Whether to send image responses to the client. Can be \"allow\" or \"omit\". \n   * Defaults to \"allow\".\n   */\n  imageResponses?: 'allow' | 'omit';\n}\n```\n\u003c/details\u003e\n\n### Standalone MCP server\n\nWhen running headed browser on system w/o display or from worker processes of the IDEs,\nrun the MCP server from environment with the DISPLAY and pass the `--port` flag to enable HTTP transport.\n\n```bash\nnpx @playwright/mcp@latest --port 8931\n```\n\nAnd then in MCP client config, set the `url` to the HTTP endpoint:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"url\": \"http://localhost:8931/mcp\"\n    }\n  }\n}\n```\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eDocker\u003c/b\u003e\u003c/summary\u003e\n\n**NOTE:** The Docker implementation only supports headless chromium at the moment.\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"--pull=always\", \"mcr.microsoft.com/playwright/mcp\"]\n    }\n  }\n}\n```\n\nOr If you prefer to run the container as a long-lived service instead of letting the MCP client spawn it, use:\n\n```\ndocker run -d -i --rm --init --pull=always \\\n  --entrypoint node \\\n  --name playwright \\\n  -p 8931:8931 \\\n  mcr.microsoft.com/playwright/mcp \\\n  cli.js --headless --browser chromium --no-sandbox --port 8931\n```\n\nThe server will listen on host port **8931** and can be reached by any MCP client.  \n\nYou can build the Docker image yourself.\n\n```\ndocker build -t mcr.microsoft.com/playwright/mcp .\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eProgrammatic usage\u003c/b\u003e\u003c/summary\u003e\n\n```js\nimport http from 'http';\n\nimport { createConnection } from '@playwright/mcp';\nimport { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';\n\nhttp.createServer(async (req, res) =\u003e {\n  // ...\n\n  // Creates a headless Playwright MCP server with SSE transport\n  const connection = await createConnection({ browser: { launchOptions: { headless: true } } });\n  const transport = new SSEServerTransport('/messages', res);\n  await connection.connect(transport);\n\n  // ...\n});\n```\n\u003c/details\u003e\n\n### Tools\n\n\u003c!--- Tools generated by update-readme.js --\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eCore automation\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_click**\n  - Title: Click\n  - Description: Perform click on a web page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `doubleClick` (boolean, optional): Whether to perform a double click instead of a single click\n    - `button` (string, optional): Button to click, defaults to left\n    - `modifiers` (array, optional): Modifier keys to press\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_close**\n  - Title: Close browser\n  - Description: Close the page\n  - Parameters: None\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_console_messages**\n  - Title: Get console messages\n  - Description: Returns all console messages\n  - Parameters:\n    - `onlyErrors` (boolean, optional): Only return error messages\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_drag**\n  - Title: Drag mouse\n  - Description: Perform drag and drop between two elements\n  - Parameters:\n    - `startElement` (string): Human-readable source element description used to obtain the permission to interact with the element\n    - `startRef` (string): Exact source element reference from the page snapshot\n    - `endElement` (string): Human-readable target element description used to obtain the permission to interact with the element\n    - `endRef` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_evaluate**\n  - Title: Evaluate JavaScript\n  - Description: Evaluate JavaScript expression on page or element\n  - Parameters:\n    - `function` (string): () =\u003e { /* code */ } or (element) =\u003e { /* code */ } when element is provided\n    - `element` (string, optional): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string, optional): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_file_upload**\n  - Title: Upload files\n  - Description: Upload one or multiple files\n  - Parameters:\n    - `paths` (array, optional): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_fill_form**\n  - Title: Fill form\n  - Description: Fill multiple form fields\n  - Parameters:\n    - `fields` (array): Fields to fill in\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_handle_dialog**\n  - Title: Handle a dialog\n  - Description: Handle a dialog\n  - Parameters:\n    - `accept` (boolean): Whether to accept the dialog.\n    - `promptText` (string, optional): The text of the prompt in case of a prompt dialog.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_hover**\n  - Title: Hover mouse\n  - Description: Hover over element on page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_navigate**\n  - Title: Navigate to a URL\n  - Description: Navigate to a URL\n  - Parameters:\n    - `url` (string): The URL to navigate to\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_navigate_back**\n  - Title: Go back\n  - Description: Go back to the previous page\n  - Parameters: None\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_network_requests**\n  - Title: List network requests\n  - Description: Returns all network requests since loading the page\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_press_key**\n  - Title: Press a key\n  - Description: Press a key on the keyboard\n  - Parameters:\n    - `key` (string): Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_resize**\n  - Title: Resize browser window\n  - Description: Resize the browser window\n  - Parameters:\n    - `width` (number): Width of the browser window\n    - `height` (number): Height of the browser window\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_select_option**\n  - Title: Select option\n  - Description: Select an option in a dropdown\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `values` (array): Array of values to select in the dropdown. This can be a single value or multiple values.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_snapshot**\n  - Title: Page snapshot\n  - Description: Capture accessibility snapshot of the current page, this is better than screenshot\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_take_screenshot**\n  - Title: Take a screenshot\n  - Description: Take a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n  - Parameters:\n    - `type` (string, optional): Image format for the screenshot. Default is png.\n    - `filename` (string, optional): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\n    - `element` (string, optional): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n    - `ref` (string, optional): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n    - `fullPage` (boolean, optional): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_type**\n  - Title: Type text\n  - Description: Type text into editable element\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `text` (string): Text to type into the element\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n    - `slowly` (boolean, optional): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_wait_for**\n  - Title: Wait for\n  - Description: Wait for text to appear or disappear or a specified time to pass\n  - Parameters:\n    - `time` (number, optional): The time to wait in seconds\n    - `text` (string, optional): The text to wait for\n    - `textGone` (string, optional): The text to wait for to disappear\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTab management\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_tabs**\n  - Title: Manage tabs\n  - Description: List, create, close, or select a browser tab.\n  - Parameters:\n    - `action` (string): Operation to perform\n    - `index` (number, optional): Tab index, used for close/select. If omitted for close, current tab is closed.\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eBrowser installation\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_install**\n  - Title: Install the browser specified in the config\n  - Description: Install the browser specified in the config. Call this if you get an error about the browser not being installed.\n  - Parameters: None\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eCoordinate-based (opt-in via --caps=vision)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_mouse_click_xy**\n  - Title: Click\n  - Description: Click left mouse button at a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_mouse_drag_xy**\n  - Title: Drag mouse\n  - Description: Drag left mouse button to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `startX` (number): Start X coordinate\n    - `startY` (number): Start Y coordinate\n    - `endX` (number): End X coordinate\n    - `endY` (number): End Y coordinate\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_mouse_move_xy**\n  - Title: Move mouse\n  - Description: Move mouse to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003ePDF generation (opt-in via --caps=pdf)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_pdf_save**\n  - Title: Save as PDF\n  - Description: Save page as PDF\n  - Parameters:\n    - `filename` (string, optional): File name to save the pdf to. Defaults to `page-{timestamp}.pdf` if not specified. Prefer relative file names to stay within the output directory.\n  - Read-only: **true**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTest assertions (opt-in via --caps=testing)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_generate_locator**\n  - Title: Create locator for element\n  - Description: Generate locator for the given element to use in tests\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_element_visible**\n  - Title: Verify element visible\n  - Description: Verify element is visible on the page\n  - Parameters:\n    - `role` (string): ROLE of the element. Can be found in the snapshot like this: `- {ROLE} \"Accessible Name\":`\n    - `accessibleName` (string): ACCESSIBLE_NAME of the element. Can be found in the snapshot like this: `- role \"{ACCESSIBLE_NAME}\"`\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_list_visible**\n  - Title: Verify list visible\n  - Description: Verify list is visible on the page\n  - Parameters:\n    - `element` (string): Human-readable list description\n    - `ref` (string): Exact target element reference that points to the list\n    - `items` (array): Items to verify\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_text_visible**\n  - Title: Verify text visible\n  - Description: Verify text is visible on the page. Prefer browser_verify_element_visible if possible.\n  - Parameters:\n    - `text` (string): TEXT to verify. Can be found in the snapshot like this: `- role \"Accessible Name\": {TEXT}` or like this: `- text: {TEXT}`\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_value**\n  - Title: Verify value\n  - Description: Verify element value\n  - Parameters:\n    - `type` (string): Type of the element\n    - `element` (string): Human-readable element description\n    - `ref` (string): Exact target element reference that points to the element\n    - `value` (string): Value to verify. For checkbox, use \"true\" or \"false\".\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTracing (opt-in via --caps=tracing)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_start_tracing**\n  - Title: Start tracing\n  - Description: Start trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_stop_tracing**\n  - Title: Stop tracing\n  - Description: Stop trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c/details\u003e\n\n\n\u003c!--- End of tools generated section --\u003e\n"},"version":"0.0.1-seed","created_at":"2025-09-23T17:40:57Z","updated_at":"2025-10-22T11:21:13Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"@playwright/mcp","version":"latest","runtime_hint":"npx"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"41b79849-7e6c-4fc7-82c0-5a611ea21523","is_latest":true,"published_at":"2025-09-09T10:55:22.907061Z","updated_at":"2025-09-09T10:55:22.907061Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Playwright","homepage_url":"https://www.npmjs.com/package/@playwright/mcp","is_in_organization":true,"license":"Apache License 2.0","name":"playwright-mcp","name_with_owner":"microsoft/playwright-mcp","opengraph_image_url":"https://opengraph.githubassets.com/eee41135954b9c1b5d4ea9a4852fd7036e80a5c9648222e68e73b575cf8e494f/microsoft/playwright-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/6154722?v=4","preferred_image":"https://avatars.githubusercontent.com/u/6154722?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-22T01:40:09Z","readme":"## Playwright MCP\n\nA Model Context Protocol (MCP) server that provides browser automation capabilities using [Playwright](https://playwright.dev). This server enables LLMs to interact with web pages through structured accessibility snapshots, bypassing the need for screenshots or visually-tuned models.\n\n### Key Features\n\n- **Fast and lightweight**. Uses Playwright's accessibility tree, not pixel-based input.\n- **LLM-friendly**. No vision models needed, operates purely on structured data.\n- **Deterministic tool application**. Avoids ambiguity common with screenshot-based approaches.\n\n### Requirements\n- Node.js 18 or newer\n- VS Code, Cursor, Windsurf, Claude Desktop, Goose or any other MCP client\n\n\u003c!--\n// Generate using:\nnode utils/generate-links.js\n--\u003e\n\n### Getting started\n\nFirst, install the Playwright MCP server with your client.\n\n**Standard config** works in most of the tools:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n[\u003cimg src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Server\u0026color=0098FF\" alt=\"Install in VS Code\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [\u003cimg alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Server\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n\u003cdetails\u003e\n\u003csummary\u003eAmp\u003c/summary\u003e\n\nAdd via the Amp VS Code extension settings screen or by updating your settings.json file:\n\n```json\n\"amp.mcpServers\": {\n  \"playwright\": {\n    \"command\": \"npx\",\n    \"args\": [\n      \"@playwright/mcp@latest\"\n    ]\n  }\n}\n```\n\n**Amp CLI Setup:**\n\nAdd via the `amp mcp add`command below\n\n```bash\namp mcp add playwright -- npx @playwright/mcp@latest\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eClaude Code\u003c/summary\u003e\n\nUse the Claude Code CLI to add the Playwright MCP server:\n\n```bash\nclaude mcp add playwright npx @playwright/mcp@latest\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eClaude Desktop\u003c/summary\u003e\n\nFollow the MCP install [guide](https://modelcontextprotocol.io/quickstart/user), use the standard config above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eCodex\u003c/summary\u003e\n\nCreate or edit the configuration file `~/.codex/config.toml` and add:\n\n```toml\n[mcp_servers.playwright]\ncommand = \"npx\"\nargs = [\"@playwright/mcp@latest\"]\n```\n\nFor more information, see the [Codex MCP documentation](https://github.com/openai/codex/blob/main/codex-rs/config.md#mcp_servers).\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eCursor\u003c/summary\u003e\n\n#### Click the button to install:\n\n[\u003cimg src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\"\u003e](https://cursor.com/en/install-mcp?name=Playwright\u0026config=eyJjb21tYW5kIjoibnB4IEBwbGF5d3JpZ2h0L21jcEBsYXRlc3QifQ%3D%3D)\n\n#### Or install manually:\n\nGo to `Cursor Settings` -\u003e `MCP` -\u003e `Add new MCP Server`. Name to your liking, use `command` type with the command `npx @playwright/mcp@latest`. You can also verify config or add command like arguments via clicking `Edit`.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eGemini CLI\u003c/summary\u003e\n\nFollow the MCP install [guide](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#configure-the-mcp-server-in-settingsjson), use the standard config above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eGoose\u003c/summary\u003e\n\n#### Click the button to install:\n\n[![Install in Goose](https://block.github.io/goose/img/extension-install-dark.svg)](https://block.github.io/goose/extension?cmd=npx\u0026arg=%40playwright%2Fmcp%40latest\u0026id=playwright\u0026name=Playwright\u0026description=Interact%20with%20web%20pages%20through%20structured%20accessibility%20snapshots%20using%20Playwright)\n\n#### Or install manually:\n\nGo to `Advanced settings` -\u003e `Extensions` -\u003e `Add custom extension`. Name to your liking, use type `STDIO`, and set the `command` to `npx @playwright/mcp`. Click \"Add Extension\".\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eKiro\u003c/summary\u003e\n\nFollow the MCP Servers [documentation](https://kiro.dev/docs/mcp/). For example in `.kiro/settings/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eLM Studio\u003c/summary\u003e\n\n#### Click the button to install:\n\n[![Add MCP Server playwright to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=playwright\u0026config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyJAcGxheXdyaWdodC9tY3BAbGF0ZXN0Il19)\n\n#### Or install manually:\n\nGo to `Program` in the right sidebar -\u003e `Install` -\u003e `Edit mcp.json`. Use the standard config above.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eopencode\u003c/summary\u003e\n\nFollow the MCP Servers [documentation](https://opencode.ai/docs/mcp-servers/). For example in `~/.config/opencode/opencode.json`:\n\n```json\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"playwright\": {\n      \"type\": \"local\",\n      \"command\": [\n        \"npx\",\n        \"@playwright/mcp@latest\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eQodo Gen\u003c/summary\u003e\n\nOpen [Qodo Gen](https://docs.qodo.ai/qodo-documentation/qodo-gen) chat panel in VSCode or IntelliJ → Connect more tools → + Add new MCP → Paste the standard config above.\n\nClick \u003ccode\u003eSave\u003c/code\u003e.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eVS Code\u003c/summary\u003e\n\n#### Click the button to install:\n\n[\u003cimg src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Server\u0026color=0098FF\" alt=\"Install in VS Code\"\u003e](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [\u003cimg alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Server\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n#### Or install manually:\n\nFollow the MCP install [guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server), use the standard config above. You can also install the Playwright MCP server using the VS Code CLI:\n\n```bash\n# For VS Code\ncode --add-mcp '{\"name\":\"playwright\",\"command\":\"npx\",\"args\":[\"@playwright/mcp@latest\"]}'\n```\n\nAfter installation, the Playwright MCP server will be available for use with your GitHub Copilot agent in VS Code.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eWarp\u003c/summary\u003e\n\nGo to `Settings` -\u003e `AI` -\u003e `Manage MCP Servers` -\u003e `+ Add` to [add an MCP Server](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server). Use the standard config above.\n\nAlternatively, use the slash command `/add-mcp` in the Warp prompt and paste the standard config from above:\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eWindsurf\u003c/summary\u003e\n\nFollow Windsurf MCP [documentation](https://docs.windsurf.com/windsurf/cascade/mcp). Use the standard config above.\n\n\u003c/details\u003e\n\n### Configuration\n\nPlaywright MCP server supports following arguments. They can be provided in the JSON configuration above, as a part of the `\"args\"` list:\n\n\u003c!--- Options generated by update-readme.js --\u003e\n\n```\n\u003e npx @playwright/mcp@latest --help\n  --allowed-hosts \u003chosts...\u003e            comma-separated list of hosts this\n                                        server is allowed to serve from.\n                                        Defaults to the host the server is bound\n                                        to. Pass '*' to disable the host check.\n  --allowed-origins \u003corigins\u003e           semicolon-separated list of origins to\n                                        allow the browser to request. Default is\n                                        to allow all.\n  --blocked-origins \u003corigins\u003e           semicolon-separated list of origins to\n                                        block the browser from requesting.\n                                        Blocklist is evaluated before allowlist.\n                                        If used without the allowlist, requests\n                                        not matching the blocklist are still\n                                        allowed.\n  --block-service-workers               block service workers\n  --browser \u003cbrowser\u003e                   browser or chrome channel to use,\n                                        possible values: chrome, firefox,\n                                        webkit, msedge.\n  --caps \u003ccaps\u003e                         comma-separated list of additional\n                                        capabilities to enable, possible values:\n                                        vision, pdf.\n  --cdp-endpoint \u003cendpoint\u003e             CDP endpoint to connect to.\n  --cdp-header \u003cheaders...\u003e             CDP headers to send with the connect\n                                        request, multiple can be specified.\n  --config \u003cpath\u003e                       path to the configuration file.\n  --device \u003cdevice\u003e                     device to emulate, for example: \"iPhone\n                                        15\"\n  --executable-path \u003cpath\u003e              path to the browser executable.\n  --extension                           Connect to a running browser instance\n                                        (Edge/Chrome only). Requires the\n                                        \"Playwright MCP Bridge\" browser\n                                        extension to be installed.\n  --grant-permissions \u003cpermissions...\u003e  List of permissions to grant to the\n                                        browser context, for example\n                                        \"geolocation\", \"clipboard-read\",\n                                        \"clipboard-write\".\n  --headless                            run browser in headless mode, headed by\n                                        default\n  --host \u003chost\u003e                         host to bind server to. Default is\n                                        localhost. Use 0.0.0.0 to bind to all\n                                        interfaces.\n  --ignore-https-errors                 ignore https errors\n  --init-script \u003cpath...\u003e               path to JavaScript file to add as an\n                                        initialization script. The script will\n                                        be evaluated in every page before any of\n                                        the page's scripts. Can be specified\n                                        multiple times.\n  --isolated                            keep the browser profile in memory, do\n                                        not save it to disk.\n  --image-responses \u003cmode\u003e              whether to send image responses to the\n                                        client. Can be \"allow\" or \"omit\",\n                                        Defaults to \"allow\".\n  --no-sandbox                          disable the sandbox for all process\n                                        types that are normally sandboxed.\n  --output-dir \u003cpath\u003e                   path to the directory for output files.\n  --port \u003cport\u003e                         port to listen on for SSE transport.\n  --proxy-bypass \u003cbypass\u003e               comma-separated domains to bypass proxy,\n                                        for example\n                                        \".com,chromium.org,.domain.com\"\n  --proxy-server \u003cproxy\u003e                specify proxy server, for example\n                                        \"http://myproxy:3128\" or\n                                        \"socks5://myproxy:8080\"\n  --save-session                        Whether to save the Playwright MCP\n                                        session into the output directory.\n  --save-trace                          Whether to save the Playwright Trace of\n                                        the session into the output directory.\n  --save-video \u003csize\u003e                   Whether to save the video of the session\n                                        into the output directory. For example\n                                        \"--save-video=800x600\"\n  --secrets \u003cpath\u003e                      path to a file containing secrets in the\n                                        dotenv format\n  --shared-browser-context              reuse the same browser context between\n                                        all connected HTTP clients.\n  --storage-state \u003cpath\u003e                path to the storage state file for\n                                        isolated sessions.\n  --test-id-attribute \u003cattribute\u003e       specify the attribute to use for test\n                                        ids, defaults to \"data-testid\"\n  --timeout-action \u003ctimeout\u003e            specify action timeout in milliseconds,\n                                        defaults to 5000ms\n  --timeout-navigation \u003ctimeout\u003e        specify navigation timeout in\n                                        milliseconds, defaults to 60000ms\n  --user-agent \u003cua string\u003e              specify user agent string\n  --user-data-dir \u003cpath\u003e                path to the user data directory. If not\n                                        specified, a temporary directory will be\n                                        created.\n  --viewport-size \u003csize\u003e                specify browser viewport size in pixels,\n                                        for example \"1280x720\"\n```\n\n\u003c!--- End of options generated section --\u003e\n\n### User profile\n\nYou can run Playwright MCP with persistent profile like a regular browser (default), in isolated contexts for testing sessions, or connect to your existing browser using the browser extension.\n\n**Persistent profile**\n\nAll the logged in information will be stored in the persistent profile, you can delete it between sessions if you'd like to clear the offline state.\nPersistent profile is located at the following locations and you can override it with the `--user-data-dir` argument.\n\n```bash\n# Windows\n%USERPROFILE%\\AppData\\Local\\ms-playwright\\mcp-{channel}-profile\n\n# macOS\n- ~/Library/Caches/ms-playwright/mcp-{channel}-profile\n\n# Linux\n- ~/.cache/ms-playwright/mcp-{channel}-profile\n```\n\n**Isolated**\n\nIn the isolated mode, each session is started in the isolated profile. Every time you ask MCP to close the browser,\nthe session is closed and all the storage state for this session is lost. You can provide initial storage state\nto the browser via the config's `contextOptions` or via the `--storage-state` argument. Learn more about the storage\nstate [here](https://playwright.dev/docs/auth).\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--isolated\",\n        \"--storage-state={path/to/storage.json}\"\n      ]\n    }\n  }\n}\n```\n\n**Browser Extension**\n\nThe Playwright MCP Chrome Extension allows you to connect to existing browser tabs and leverage your logged-in sessions and browser state. See [extension/README.md](extension/README.md) for installation and setup instructions.\n\n### Configuration file\n\nThe Playwright MCP server can be configured using a JSON configuration file. You can specify the configuration file\nusing the `--config` command line option:\n\n```bash\nnpx @playwright/mcp@latest --config path/to/config.json\n```\n\n\u003cdetails\u003e\n\u003csummary\u003eConfiguration file schema\u003c/summary\u003e\n\n```typescript\n{\n  // Browser configuration\n  browser?: {\n    // Browser type to use (chromium, firefox, or webkit)\n    browserName?: 'chromium' | 'firefox' | 'webkit';\n\n    // Keep the browser profile in memory, do not save it to disk.\n    isolated?: boolean;\n\n    // Path to user data directory for browser profile persistence\n    userDataDir?: string;\n\n    // Browser launch options (see Playwright docs)\n    // @see https://playwright.dev/docs/api/class-browsertype#browser-type-launch\n    launchOptions?: {\n      channel?: string;        // Browser channel (e.g. 'chrome')\n      headless?: boolean;      // Run in headless mode\n      executablePath?: string; // Path to browser executable\n      // ... other Playwright launch options\n    };\n\n    // Browser context options\n    // @see https://playwright.dev/docs/api/class-browser#browser-new-context\n    contextOptions?: {\n      viewport?: { width: number, height: number };\n      // ... other Playwright context options\n    };\n\n    // CDP endpoint for connecting to existing browser\n    cdpEndpoint?: string;\n\n    // Remote Playwright server endpoint\n    remoteEndpoint?: string;\n  },\n\n  // Server configuration\n  server?: {\n    port?: number;  // Port to listen on\n    host?: string;  // Host to bind to (default: localhost)\n  },\n\n  // List of additional capabilities\n  capabilities?: Array\u003c\n    'tabs' |    // Tab management\n    'install' | // Browser installation\n    'pdf' |     // PDF generation\n    'vision' |  // Coordinate-based interactions\n  \u003e;\n\n  // Directory for output files\n  outputDir?: string;\n\n  // Network configuration\n  network?: {\n    // List of origins to allow the browser to request. Default is to allow all. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    allowedOrigins?: string[];\n\n    // List of origins to block the browser to request. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    blockedOrigins?: string[];\n  };\n \n  /**\n   * Whether to send image responses to the client. Can be \"allow\" or \"omit\". \n   * Defaults to \"allow\".\n   */\n  imageResponses?: 'allow' | 'omit';\n}\n```\n\u003c/details\u003e\n\n### Standalone MCP server\n\nWhen running headed browser on system w/o display or from worker processes of the IDEs,\nrun the MCP server from environment with the DISPLAY and pass the `--port` flag to enable HTTP transport.\n\n```bash\nnpx @playwright/mcp@latest --port 8931\n```\n\nAnd then in MCP client config, set the `url` to the HTTP endpoint:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"url\": \"http://localhost:8931/mcp\"\n    }\n  }\n}\n```\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eDocker\u003c/b\u003e\u003c/summary\u003e\n\n**NOTE:** The Docker implementation only supports headless chromium at the moment.\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"--pull=always\", \"mcr.microsoft.com/playwright/mcp\"]\n    }\n  }\n}\n```\n\nOr If you prefer to run the container as a long-lived service instead of letting the MCP client spawn it, use:\n\n```\ndocker run -d -i --rm --init --pull=always \\\n  --entrypoint node \\\n  --name playwright \\\n  -p 8931:8931 \\\n  mcr.microsoft.com/playwright/mcp \\\n  cli.js --headless --browser chromium --no-sandbox --port 8931\n```\n\nThe server will listen on host port **8931** and can be reached by any MCP client.  \n\nYou can build the Docker image yourself.\n\n```\ndocker build -t mcr.microsoft.com/playwright/mcp .\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eProgrammatic usage\u003c/b\u003e\u003c/summary\u003e\n\n```js\nimport http from 'http';\n\nimport { createConnection } from '@playwright/mcp';\nimport { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';\n\nhttp.createServer(async (req, res) =\u003e {\n  // ...\n\n  // Creates a headless Playwright MCP server with SSE transport\n  const connection = await createConnection({ browser: { launchOptions: { headless: true } } });\n  const transport = new SSEServerTransport('/messages', res);\n  await connection.connect(transport);\n\n  // ...\n});\n```\n\u003c/details\u003e\n\n### Tools\n\n\u003c!--- Tools generated by update-readme.js --\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eCore automation\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_click**\n  - Title: Click\n  - Description: Perform click on a web page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `doubleClick` (boolean, optional): Whether to perform a double click instead of a single click\n    - `button` (string, optional): Button to click, defaults to left\n    - `modifiers` (array, optional): Modifier keys to press\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_close**\n  - Title: Close browser\n  - Description: Close the page\n  - Parameters: None\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_console_messages**\n  - Title: Get console messages\n  - Description: Returns all console messages\n  - Parameters:\n    - `onlyErrors` (boolean, optional): Only return error messages\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_drag**\n  - Title: Drag mouse\n  - Description: Perform drag and drop between two elements\n  - Parameters:\n    - `startElement` (string): Human-readable source element description used to obtain the permission to interact with the element\n    - `startRef` (string): Exact source element reference from the page snapshot\n    - `endElement` (string): Human-readable target element description used to obtain the permission to interact with the element\n    - `endRef` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_evaluate**\n  - Title: Evaluate JavaScript\n  - Description: Evaluate JavaScript expression on page or element\n  - Parameters:\n    - `function` (string): () =\u003e { /* code */ } or (element) =\u003e { /* code */ } when element is provided\n    - `element` (string, optional): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string, optional): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_file_upload**\n  - Title: Upload files\n  - Description: Upload one or multiple files\n  - Parameters:\n    - `paths` (array, optional): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_fill_form**\n  - Title: Fill form\n  - Description: Fill multiple form fields\n  - Parameters:\n    - `fields` (array): Fields to fill in\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_handle_dialog**\n  - Title: Handle a dialog\n  - Description: Handle a dialog\n  - Parameters:\n    - `accept` (boolean): Whether to accept the dialog.\n    - `promptText` (string, optional): The text of the prompt in case of a prompt dialog.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_hover**\n  - Title: Hover mouse\n  - Description: Hover over element on page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_navigate**\n  - Title: Navigate to a URL\n  - Description: Navigate to a URL\n  - Parameters:\n    - `url` (string): The URL to navigate to\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_navigate_back**\n  - Title: Go back\n  - Description: Go back to the previous page\n  - Parameters: None\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_network_requests**\n  - Title: List network requests\n  - Description: Returns all network requests since loading the page\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_press_key**\n  - Title: Press a key\n  - Description: Press a key on the keyboard\n  - Parameters:\n    - `key` (string): Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_resize**\n  - Title: Resize browser window\n  - Description: Resize the browser window\n  - Parameters:\n    - `width` (number): Width of the browser window\n    - `height` (number): Height of the browser window\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_select_option**\n  - Title: Select option\n  - Description: Select an option in a dropdown\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `values` (array): Array of values to select in the dropdown. This can be a single value or multiple values.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_snapshot**\n  - Title: Page snapshot\n  - Description: Capture accessibility snapshot of the current page, this is better than screenshot\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_take_screenshot**\n  - Title: Take a screenshot\n  - Description: Take a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n  - Parameters:\n    - `type` (string, optional): Image format for the screenshot. Default is png.\n    - `filename` (string, optional): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\n    - `element` (string, optional): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n    - `ref` (string, optional): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n    - `fullPage` (boolean, optional): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_type**\n  - Title: Type text\n  - Description: Type text into editable element\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `text` (string): Text to type into the element\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n    - `slowly` (boolean, optional): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_wait_for**\n  - Title: Wait for\n  - Description: Wait for text to appear or disappear or a specified time to pass\n  - Parameters:\n    - `time` (number, optional): The time to wait in seconds\n    - `text` (string, optional): The text to wait for\n    - `textGone` (string, optional): The text to wait for to disappear\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTab management\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_tabs**\n  - Title: Manage tabs\n  - Description: List, create, close, or select a browser tab.\n  - Parameters:\n    - `action` (string): Operation to perform\n    - `index` (number, optional): Tab index, used for close/select. If omitted for close, current tab is closed.\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eBrowser installation\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_install**\n  - Title: Install the browser specified in the config\n  - Description: Install the browser specified in the config. Call this if you get an error about the browser not being installed.\n  - Parameters: None\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eCoordinate-based (opt-in via --caps=vision)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_mouse_click_xy**\n  - Title: Click\n  - Description: Click left mouse button at a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_mouse_drag_xy**\n  - Title: Drag mouse\n  - Description: Drag left mouse button to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `startX` (number): Start X coordinate\n    - `startY` (number): Start Y coordinate\n    - `endX` (number): End X coordinate\n    - `endY` (number): End Y coordinate\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_mouse_move_xy**\n  - Title: Move mouse\n  - Description: Move mouse to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003ePDF generation (opt-in via --caps=pdf)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_pdf_save**\n  - Title: Save as PDF\n  - Description: Save page as PDF\n  - Parameters:\n    - `filename` (string, optional): File name to save the pdf to. Defaults to `page-{timestamp}.pdf` if not specified. Prefer relative file names to stay within the output directory.\n  - Read-only: **true**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTest assertions (opt-in via --caps=testing)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_generate_locator**\n  - Title: Create locator for element\n  - Description: Generate locator for the given element to use in tests\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_element_visible**\n  - Title: Verify element visible\n  - Description: Verify element is visible on the page\n  - Parameters:\n    - `role` (string): ROLE of the element. Can be found in the snapshot like this: `- {ROLE} \"Accessible Name\":`\n    - `accessibleName` (string): ACCESSIBLE_NAME of the element. Can be found in the snapshot like this: `- role \"{ACCESSIBLE_NAME}\"`\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_list_visible**\n  - Title: Verify list visible\n  - Description: Verify list is visible on the page\n  - Parameters:\n    - `element` (string): Human-readable list description\n    - `ref` (string): Exact target element reference that points to the list\n    - `items` (array): Items to verify\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_text_visible**\n  - Title: Verify text visible\n  - Description: Verify text is visible on the page. Prefer browser_verify_element_visible if possible.\n  - Parameters:\n    - `text` (string): TEXT to verify. Can be found in the snapshot like this: `- role \"Accessible Name\": {TEXT}` or like this: `- text: {TEXT}`\n  - Read-only: **false**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_verify_value**\n  - Title: Verify value\n  - Description: Verify element value\n  - Parameters:\n    - `type` (string): Type of the element\n    - `element` (string): Human-readable element description\n    - `ref` (string): Exact target element reference that points to the element\n    - `value` (string): Value to verify. For checkbox, use \"true\" or \"false\".\n  - Read-only: **false**\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eTracing (opt-in via --caps=tracing)\u003c/b\u003e\u003c/summary\u003e\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_start_tracing**\n  - Title: Start tracing\n  - Description: Start trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c!-- NOTE: This has been generated via update-readme.js --\u003e\n\n- **browser_stop_tracing**\n  - Title: Stop tracing\n  - Description: Stop trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n\u003c/details\u003e\n\n\n\u003c!--- End of tools generated section --\u003e\n","stargazer_count":22253,"topics":["mcp","playwright"],"uses_custom_opengraph_image":false}}}},{"name":"oraios/serena","description":"Semantic code retrieval \u0026 editing tools for coding agents.","status":"active","repository":{"url":"https://github.com/oraios/serena","source":"github","id":"953683578","readme":"\u003cp align=\"center\" style=\"text-align:center\"\u003e\n  \u003cimg src=\"resources/serena-logo.svg#gh-light-mode-only\" style=\"width:500px\"\u003e\n  \u003cimg src=\"resources/serena-logo-dark-mode.svg#gh-dark-mode-only\" style=\"width:500px\"\u003e\n\u003c/p\u003e\n\n* :rocket: Serena is a powerful **coding agent toolkit** capable of turning an LLM into a fully-featured agent that works **directly on your codebase**.\n  Unlike most other tools, it is not tied to an LLM, framework or an interface, making it easy to use it in a variety of ways.\n* :wrench: Serena provides essential **semantic code retrieval and editing tools** that are akin to an IDE's capabilities, extracting code entities at the symbol level and exploiting relational structure. When combined with an existing coding agent, these tools greatly enhance (token) efficiency.\n* :free: Serena is **free \u0026 open-source**, enhancing the capabilities of LLMs you already have access to free of charge.\n\nYou can think of Serena as providing IDE-like tools to your LLM/coding agent. With it, the agent no longer needs to read entire\nfiles, perform grep-like searches or string replacements to find and edit the right code. Instead, it can use code centered tools like `find_symbol`, `find_referencing_symbols` and `insert_after_symbol`.\n\n\u003cp align=\"center\"\u003e\n  \u003cem\u003eSerena is under active development! See the latest updates, upcoming features, and lessons learned to stay up to date.\u003c/em\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"CHANGELOG.md\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Updates-1e293b?style=flat\u0026logo=rss\u0026logoColor=white\u0026labelColor=1e293b\" alt=\"Changelog\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"roadmap.md\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Roadmap-14532d?style=flat\u0026logo=target\u0026logoColor=white\u0026labelColor=14532d\" alt=\"Roadmap\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"lessons_learned.md\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Lessons-Learned-7c4700?style=flat\u0026logo=readthedocs\u0026logoColor=white\u0026labelColor=7c4700\" alt=\"Lessons Learned\" /\u003e\n  \u003c/a\u003e\n\u003c/p\u003e\n\n### LLM Integration\n\nSerena provides the necessary [tools](#list-of-tools) for coding workflows, but an LLM is required to do the actual work,\norchestrating tool use.\n\nFor example, **supercharge the performance of Claude Code** with a [one-line shell command](#claude-code).\n\nIn general, Serena can be integrated with an LLM in several ways:\n\n* by using the **model context protocol (MCP)**.\n  Serena provides an MCP server which integrates with\n    * Claude Code and Claude Desktop,\n    * Terminal-based clients like Codex, Gemini-CLI, Qwen3-Coder, rovodev, OpenHands CLI and others,\n    * IDEs like VSCode, Cursor or IntelliJ,\n    * Extensions like Cline or Roo Code\n    * Local clients like [OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp), [Jan](https://jan.ai/docs/mcp-examples/browser/browserbase#enable-mcp), [Agno](https://docs.agno.com/introduction/playground) and others\n* by using [mcpo to connect it to ChatGPT](docs/serena_on_chatgpt.md) or other clients that don't support MCP but do support tool calling via OpenAPI.\n* by incorporating Serena's tools into an agent framework of your choice, as illustrated [here](docs/custom_agent.md).\n  Serena's tool implementation is decoupled from the framework-specific code and can thus easily be adapted to any agent framework.\n\n### Serena in Action\n\n#### Demonstration 1: Efficient Operation in Claude Code\n\nA demonstration of Serena efficiently retrieving and editing code within Claude Code, thereby saving tokens and time. Efficient operations are not only useful for saving costs, but also for generally improving the generated code's quality. This effect may be less pronounced in very small projects, but often becomes of crucial importance in larger ones.\n\nhttps://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87\n\n#### Demonstration 2: Serena in Claude Desktop\n\nA demonstration of Serena implementing a small feature for itself (a better log GUI) with Claude Desktop.\nNote how Serena's tools enable Claude to find and edit the right symbols.\n\nhttps://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753\n\n### Programming Language Support \u0026 Semantic Analysis Capabilities\n\nSerena's semantic code analysis capabilities build on **language servers** using the widely implemented\nlanguage server protocol (LSP). The LSP provides a set of versatile code querying\nand editing functionalities based on symbolic understanding of the code.\nEquipped with these capabilities, Serena discovers and edits code just like a seasoned developer\nmaking use of an IDE's capabilities would.\nSerena can efficiently find the right context and do the right thing even in very large and\ncomplex projects! So not only is it free and open-source, it frequently achieves better results\nthan existing solutions that charge a premium.\n\nLanguage servers provide support for a wide range of programming languages.\nWith Serena, we provide direct, out-of-the-box support for:\n\n  * Python\n  * TypeScript/Javascript\n  * PHP (uses Intelephense LSP; set `INTELEPHENSE_LICENSE_KEY` environment variable for premium features)\n  * Go (requires installation of gopls)\n  * R (requires installation of the `languageserver` R package)\n  * Rust (requires [rustup](https://rustup.rs/) - uses rust-analyzer from your toolchain)\n  * C/C++ (you may experience issues with finding references, we are working on it)\n  * Zig (requires installation of ZLS - Zig Language Server)\n  * C#\n  * Ruby (by default, uses [ruby-lsp](https://github.com/Shopify/ruby-lsp), specify ruby_solargraph as your language to use the previous solargraph based implementation)\n  * Swift\n  * Kotlin (uses the pre-alpha [official kotlin LS](https://github.com/Kotlin/kotlin-lsp), some issues may appear)\n  * Java (_Note_: startup is slow, initial startup especially so. There may be issues with java on macos and linux, we are working on it.)\n  * Clojure\n  * Dart\n  * Bash\n  * Lua (automatically downloads lua-language-server if not installed)\n  * Nix (requires nixd installation)\n  * Elixir (requires installation of NextLS and Elixir; **Windows not supported**)\n  * Elm (automatically downloads elm-language-server if not installed; requires Elm compiler)\n  * Scala (requires some [manual setup](docs/scala_setup_guide_for_serena.md); uses Metals LSP)\n  * Erlang (requires installation of beam and [erlang_ls](https://github.com/erlang-ls/erlang_ls), experimental, might be slow or hang)\n  * Perl (requires installation of Perl::LanguageServer)\n  * AL\n  * Markdown (must be explicitly specified via `--language markdown` when generating project config, primarily useful for documentation-heavy projects)\n\nSupport for further languages can easily be added by providing a shallow adapter for a new language server implementation,\nsee Serena's [memory on that](.serena/memories/adding_new_language_support_guide.md).\n\n### Community Feedback\n\nMost users report that Serena has strong positive effects on the results of their coding agents, even when used within\nvery capable agents like Claude Code. Serena is often described to be a [game changer](https://www.reddit.com/r/ClaudeAI/comments/1lfsdll/try_out_serena_mcp_thank_me_later/), providing an enormous [productivity boost](https://www.reddit.com/r/ClaudeCode/comments/1mguoia/absolutely_insane_improvement_of_claude_code).\n\nSerena excels at navigating and manipulating complex codebases, providing tools that support precise code retrieval and editing in the presence of large, strongly structured codebases.\nHowever, when dealing with tasks that involve only very few/small files, you may not benefit from including Serena on top of your existing coding agent. \nIn particular, when writing code from scratch, Serena will not provide much value initially, as the more complex structures that Serena handles more gracefully than simplistic, file-based approaches are yet to be created.\n\nSeveral videos and blog posts have talked about Serena:\n\n* YouTube:\n    * [AI Labs](https://www.youtube.com/watch?v=wYWyJNs1HVk\u0026t=1s)\n    * [Yo Van Eyck](https://www.youtube.com/watch?v=UqfxuQKuMo8\u0026t=45s)\n    * [JeredBlu](https://www.youtube.com/watch?v=fzPnM3ySmjE\u0026t=32s)\n\n* Blog posts:\n    * [Serena's Design Principles](https://medium.com/@souradip1000/deconstructing-serenas-mcp-powered-semantic-code-understanding-architecture-75802515d116)\n    * [Serena with Claude Code (in Japanese)](https://blog.lai.so/serena/)\n    * [Turning Claude Code into a Development Powerhouse](https://robertmarshall.dev/blog/turning-claude-code-into-a-development-powerhouse/)\n\n## Table of Contents\n\n\u003c!-- Created with markdown-toc -i README.md --\u003e\n\u003c!-- Install it with npm install -g markdown-toc --\u003e\n\n\u003c!-- toc --\u003e\n\n- [Quick Start](#quick-start)\n  * [Running the Serena MCP Server](#running-the-serena-mcp-server)\n    + [Usage](#usage)\n    + [Using uvx](#using-uvx)\n    + [Local Installation](#local-installation)\n    + [Using Docker (Experimental)](#using-docker-experimental)\n    + [Using Nix](#using-nix)\n    + [Streamable HTTP Mode](#streamable-http-mode)\n    + [Command-Line Arguments](#command-line-arguments)\n  * [Configuration](#configuration)\n  * [Project Activation \u0026 Indexing](#project-activation--indexing)\n  * [Claude Code](#claude-code)\n  * [Codex](#codex)\n  * [Other Terminal-Based Clients](#other-terminal-based-clients)\n  * [Claude Desktop](#claude-desktop)\n  * [MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)](#mcp-coding-clients-cline-roo-code-cursor-windsurf-etc)\n  * [Local GUIs and Frameworks](#local-guis-and-frameworks)\n- [Detailed Usage and Recommendations](#detailed-usage-and-recommendations)\n  * [Tool Execution](#tool-execution)\n    + [Shell Execution and Editing Tools](#shell-execution-and-editing-tools)\n  * [Modes and Contexts](#modes-and-contexts)\n    + [Contexts](#contexts)\n    + [Modes](#modes)\n    + [Customization](#customization)\n  * [Onboarding and Memories](#onboarding-and-memories)\n  * [Prepare Your Project](#prepare-your-project)\n    + [Structure Your Codebase](#structure-your-codebase)\n    + [Start from a Clean State](#start-from-a-clean-state)\n    + [Logging, Linting, and Automated Tests](#logging-linting-and-automated-tests)\n  * [Prompting Strategies](#prompting-strategies)\n  * [Running Out of Context](#running-out-of-context)\n  * [Serena's Logs: The Dashboard and GUI Tool](#serenas-logs-the-dashboard-and-gui-tool)\n  * [Serena and GIT worktrees](#serena-and-git-worktrees)\n- [Comparison with Other Coding Agents](#comparison-with-other-coding-agents)\n  * [Subscription-Based Coding Agents](#subscription-based-coding-agents)\n  * [API-Based Coding Agents](#api-based-coding-agents)\n  * [Other MCP-Based Coding Agents](#other-mcp-based-coding-agents)\n- [Acknowledgements](#acknowledgements)\n  * [Sponsors](#sponsors)\n  * [Community Contributions](#community-contributions)\n  * [Technologies](#technologies)\n- [Customizing and Extending Serena](#customizing-and-extending-serena)\n- [List of Tools](#list-of-tools)\n\n\u003c!-- tocstop --\u003e\n\n## Quick Start\n\nSerena can be used in various ways, below you will find instructions for selected integrations.\n\n* For coding with Claude, we recommend using Serena through [Claude Code](#claude-code) or [Claude Desktop](#claude-desktop). You can also use Serena in most other [terminal-based clients](#other-terminal-based-clients).\n* If you want a GUI experience outside an IDE, you can use one of the many [local GUIs](#local-guis-and-frameworks) that support MCP servers.\n  You can also connect Serena to many web clients (including ChatGPT) using [mcpo](docs/serena_on_chatgpt.md).\n* If you want to use Serena integrated in your IDE, see the section on [other MCP clients](#other-mcp-clients---cline-roo-code-cursor-windsurf-etc).\n* You can use Serena as a library for building your own applications. We try to keep the public API stable, but you should still\n  expect breaking changes and pin Serena to a fixed version if you use it as a dependency.\n\nSerena is managed by `uv`, so you will need to [install it](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Running the Serena MCP Server\n\nYou have several options for running the MCP server, which are explained in the subsections below.\n\n#### Usage\n\nThe typical usage involves the client (Claude Code, Claude Desktop, etc.) running\nthe MCP server as a subprocess (using stdio communication),\nso the client needs to be provided with the command to run the MCP server.\n(Alternatively, you can run the MCP server in Streamable HTTP or SSE mode and tell your client\nhow to connect to it.)\n\nNote that no matter how you run the MCP server, Serena will, by default, start a small web-based dashboard on localhost that will display logs and allow shutting down the\nMCP server (since many clients fail to clean up processes correctly).\nThis and other settings can be adjusted in the [configuration](#configuration) and/or by providing [command-line arguments](#command-line-arguments).\n\n#### Using uvx\n\n`uvx` can be used to run the latest version of Serena directly from the repository, without an explicit local installation.\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena start-mcp-server\n```\n\nExplore the CLI to see some of the customization options that serena provides (more info on them below).\n\n#### Local Installation\n\n1. Clone the repository and change into it.\n\n   ```shell\n   git clone https://github.com/oraios/serena\n   cd serena\n   ```\n\n2. Optionally edit the configuration file in your home directory with\n\n   ```shell\n   uv run serena config edit\n   ```\n\n   If you just want the default config, you can skip this part, and a config file will be created when you first run Serena.\n3. Run the server with `uv`:\n\n   ```shell\n   uv run serena start-mcp-server\n   ```\n\n   When running from outside the serena installation directory, be sure to pass it, i.e., use\n\n   ```shell\n    uv run --directory /abs/path/to/serena serena start-mcp-server\n   ```\n\n#### Using Docker (Experimental)\n\n⚠️ Docker support is currently experimental with several limitations. Please read the [Docker documentation](DOCKER.md) for important caveats before using it.\n\nYou can run the Serena MCP server directly via docker as follows,\nassuming that the projects you want to work on are all located in `/path/to/your/projects`:\n\n```shell\ndocker run --rm -i --network host -v /path/to/your/projects:/workspaces/projects ghcr.io/oraios/serena:latest serena start-mcp-server --transport stdio\n```\n\nReplace `/path/to/your/projects` with the absolute path to your projects directory. The Docker approach provides:\n\n* Better security isolation for shell command execution\n* No need to install language servers and dependencies locally\n* Consistent environment across different systems\n\nAlternatively, use docker compose with the `compose.yml` file provided in the repository.\n\nSee the [Docker documentation](DOCKER.md) for detailed setup instructions, configuration options, and known limitations.\n\n#### Using Nix\n\nIf you are using Nix and [have enabled the `nix-command` and `flakes` features](https://nixos.wiki/wiki/flakes), you can run Serena using the following command:\n\n```bash\nnix run github:oraios/serena -- start-mcp-server --transport stdio\n```\n\nYou can also install Serena by referencing this repo (`github:oraios/serena`) and using it in your Nix flake. The package is exported as `serena`.\n\n#### Streamable HTTP Mode\n\nℹ️ Note that MCP servers which use stdio as a protocol are somewhat unusual as far as client/server architectures go, as the server\nnecessarily has to be started by the client in order for communication to take place via the server's standard input/output stream.\nIn other words, you do not need to start the server yourself. The client application (e.g. Claude Desktop) takes care of this and\ntherefore needs to be configured with a launch command.\n\nWhen using instead the *Streamable HTTP* mode, you control the server lifecycle yourself,\ni.e. you start the server and provide the client with the URL to connect to it.\n\nSimply provide `start-mcp-server` with the `--transport streamable-http` option and optionally provide the port.\nFor example, to run the Serena MCP server in Streamable HTTP mode on port 9121 using a local installation,\nyou would run this command from the Serena directory,\n\n```shell\nuv run serena start-mcp-server --transport streamable-http --port 9121\n```\n\nand then configure your client to connect to `http://localhost:9121/mcp`.\n\nℹ️ Note that SSE transport is supported as well, but its use is discouraged. \nUse Streamable HTTP instead.\n\n#### Command-Line Arguments\n\nThe Serena MCP server supports a wide range of additional command-line options, including the option to run in Streamable HTTP or SSE mode\nand to adapt Serena to various [contexts and modes of operation](#modes-and-contexts).\n\nRun with parameter `--help` to get a list of available options.\n\n### Configuration\n\nSerena is very flexible in terms of configuration. While for most users, the default configurations will work,\nyou can fully adjust it to your needs by editing a few yaml files. You can disable tools, change Serena's instructions\n(what we denote as the `system_prompt`), adjust the output of tools that just provide a prompt, and even adjust tool descriptions.\n\nSerena is configured in four places:\n\n1. The `serena_config.yml` for general settings that apply to all clients and projects.\n   It is located in your user directory under `.serena/serena_config.yml`.\n   If you do not explicitly create the file, it will be auto-generated when you first run Serena.\n   You can edit it directly or use\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena config edit\n   ```\n\n   (or use the `--directory` command version).\n2. In the arguments passed to the `start-mcp-server` in your client's config (see below),\n   which will apply to all sessions started by the respective client. In particular, the [context](#contexts) parameter\n   should be set appropriately for Serena to be best adjusted to existing tools and capabilities of your client.\n   See for a detailed explanation. You can override all entries from the `serena_config.yml` through command line arguments.\n3. In the `.serena/project.yml` file within your project. This will hold project-level configuration that is used whenever\n   that project is activated. This file will be autogenerated when you first use Serena on that project, but you can also\n   generate it explicitly with\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena project generate-yml\n   ```\n\n   (or use the `--directory` command version).\n4. Through the context and modes. Explore the [modes and contexts](#modes-and-contexts) section for more details.\n\nAfter the initial setup, continue with one of the sections below, depending on how you\nwant to use Serena.\n\n### Project Activation \u0026 Indexing\n\nIf you are mostly working with the same project, you can configure to always activate it at startup\nby passing `--project \u003cpath_or_name\u003e` to the `start-mcp-server` command in your client's MCP config.\nThis is especially useful for clients which configure MCP servers on a per-project basis, like Claude Code.\n\nOtherwise, the recommended way is to just ask the LLM to activate a project by providing it an absolute path to, or,\nin case the project was activated in the past, by its name. The default project name is the directory name.\n\n* \"Activate the project /path/to/my_project\"\n* \"Activate the project my_project\"\n\nAll projects that have been activated will be automatically added to your `serena_config.yml`, and for each\nproject, the file `.serena/project.yml` will be generated. You can adjust the latter, e.g., by changing the name\n(which you refer to during the activation) or other options. Make sure to not have two different projects with the\nsame name.\n\nℹ️ For larger projects, we recommend that you index your project to accelerate Serena's tools; otherwise the first\ntool application may be very slow.\nTo do so, run this from the project directory (or pass the path to the project as an argument):\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena project index\n```\n\n(or use the `--directory` command version).\n\n### Claude Code\n\nSerena is a great way to make Claude Code both cheaper and more powerful!\n\nFrom your project directory, add serena with a command like this,\n\n```shell\nclaude mcp add serena -- \u003cserena-mcp-server\u003e --context ide-assistant --project \"$(pwd)\"\n```\n\nwhere `\u003cserena-mcp-server\u003e` is your way of [running the Serena MCP server](#running-the-serena-mcp-server).\nFor example, when using `uvx`, you would run\n\n```shell\nclaude mcp add serena -- uvx --from git+https://github.com/oraios/serena serena start-mcp-server --context ide-assistant --project \"$(pwd)\"\n```\n\nℹ️ Serena comes with an instruction text, and Claude needs to read it to properly use Serena's tools.\n  As of version `v1.0.52`, claude code reads the instructions of the MCP server, so this **is handled automatically**.\n  If you are using an older version, or if Claude fails to read the instructions, you can ask it explicitly\n  to \"read Serena's initial instructions\" or run `/mcp__serena__initial_instructions` to load the instruction text.\n  If you want to make use of that, you will have to enable the corresponding tool explicitly by adding `initial_instructions` to the `included_optional_tools`\n  in your config.\n  Note that you may have to make Claude read the instructions when you start a new conversation and after any compacting operation to ensure Claude remains properly configured to use Serena's tools.\n\n### Codex\n\nSerena works with OpenAI's Codex CLI out of the box, but you have to use the `codex` context for it to work properly. (The technical reason is that Codex doesn't fully support the MCP specifications, so some massaging of tools is required.).\n\nUnlike Claude Code, in Codex you add an MCP server globally and not per project. Add the following to\n`~/.codex/config.toml` (create the file if it does not exist):\n\n```toml\n[mcp_servers.serena]\ncommand = \"uvx\"\nargs = [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\", \"--context\", \"codex\"]\n```\n\nAfter codex has started, you need to activate the project, which you can do by saying:\n\n\"Activate the current dir as project using serena\"\n\n\u003e If you don't activate the project, you will not be able to use Serena's tools!\n\nThat's it! Have a look at `~/.codex/log/codex-tui.log` to see if any errors occurred.\n\nThe Serena dashboard will run if you have not disabled it in the configuration, but due to Codex's sandboxing the webbrowser\nmay not open automatically. You can open it manually by going to `http://localhost:24282/dashboard/index.html` (or a higher port, if\nthat was already taken).\n\n\u003e Codex will often show the tools as `failed` even though they are successfully executed. This is not a problem, seems to be a bug in Codex. Despite the error message, everything works as expected.\n\n### Other Terminal-Based Clients\n\nThere are many terminal-based coding assistants that support MCP servers, such as [Codex](https://github.com/openai/codex?tab=readme-ov-file#model-context-protocol-mcp),\n[Gemini-CLI](https://github.com/google-gemini/gemini-cli), [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder),\n[rovodev](https://community.atlassian.com/forums/Rovo-for-Software-Teams-Beta/Introducing-Rovo-Dev-CLI-AI-Powered-Development-in-your-terminal/ba-p/3043623),\nthe [OpenHands CLI](https://docs.all-hands.dev/usage/how-to/cli-mode) and [opencode](https://github.com/sst/opencode).\n\nThey generally benefit from the symbolic tools provided by Serena. You might want to customize some aspects of Serena\nby writing your own context, modes or prompts to adjust it to your workflow, to other MCP servers you are using, and to\nthe client's internal capabilities.\n\n### Claude Desktop\n\nFor [Claude Desktop](https://claude.ai/download) (available for Windows and macOS), go to File / Settings / Developer / MCP Servers / Edit Config,\nwhich will let you open the JSON file `claude_desktop_config.json`.\nAdd the `serena` MCP server configuration, using a [run command](#running-the-serena-mcp-server) depending on your setup.\n\n* local installation:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uv\",\n               \"args\": [\"run\", \"--directory\", \"/abs/path/to/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n   }\n   ```\n\n* uvx:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uvx\",\n               \"args\": [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n  }\n  ```\n\n* docker:\n\n  ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"docker\",\n               \"args\": [\"run\", \"--rm\", \"-i\", \"--network\", \"host\", \"-v\", \"/path/to/your/projects:/workspaces/projects\", \"ghcr.io/oraios/serena:latest\", \"serena\", \"start-mcp-server\", \"--transport\", \"stdio\"]\n           }\n       }\n   }\n   ```\n\nIf you are using paths containing backslashes for paths on Windows\n(note that you can also just use forward slashes), be sure to escape them correctly (`\\\\`).\n\nThat's it! Save the config and then restart Claude Desktop. You are ready for activating your first project.\n\nℹ️ You can further customize the run command using additional arguments (see [above](#command-line-arguments)).\n\nNote: on Windows and macOS there are official Claude Desktop applications by Anthropic, for Linux there is an [open-source\ncommunity version](https://github.com/aaddrick/claude-desktop-debian).\n\n⚠️ Be sure to fully quit the Claude Desktop application, as closing Claude will just minimize it to the system tray – at least on Windows.\n\n⚠️ Some clients may leave behind zombie processes. You will have to find and terminate them manually then.\n    With Serena, you can activate the [dashboard](#serenas-logs-the-dashboard-and-gui-tool) to prevent unnoted processes and also use the dashboard\n    for shutting down Serena.\n\nAfter restarting, you should see Serena's tools in your chat interface (notice the small hammer icon).\n\nFor more information on MCP servers with Claude Desktop, see [the official quick start guide](https://modelcontextprotocol.io/quickstart/user).\n\n### MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)\n\nBeing an MCP Server, Serena can be included in any MCP Client. The same configuration as above,\nperhaps with small client-specific modifications, should work. Most of the popular\nexisting coding assistants (IDE extensions or VSCode-like IDEs) support connections\nto MCP Servers. It is **recommended to use the `ide-assistant` context** for these integrations by adding `\"--context\", \"ide-assistant\"` to the `args` in your MCP client's configuration. Including Serena generally boosts their performance\nby providing them tools for symbolic operations.\n\nIn this case, the billing for the usage continues to be controlled by the client of your choice\n(unlike with the Claude Desktop client). But you may still want to use Serena through such an approach,\ne.g., for one of the following reasons:\n\n1. You are already using a coding assistant (say Cline or Cursor) and just want to make it more powerful.\n2. You are on Linux and don't want to use the [community-created Claude Desktop](https://github.com/aaddrick/claude-desktop-debian).\n3. You want tighter integration of Serena into your IDE and don't mind paying for that.\n\n### Local GUIs and Frameworks\n\nOver the last months, several technologies have emerged that allow you to run a powerful local GUI\nand connect it to an MCP server. They will work with Serena out of the box.\nSome of the leading open source GUI technologies offering this are\n[Jan](https://jan.ai/docs/mcp), [OpenHands](https://github.com/All-Hands-AI/OpenHands/),\n[OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp) and [Agno](https://docs.agno.com/introduction/playground).\nThey allow combining Serena with almost any LLM (including locally running ones) and offer various other integrations.\n\n## Detailed Usage and Recommendations\n\n### Tool Execution\n\nSerena combines tools for semantic code retrieval with editing capabilities and shell execution.\nSerena's behavior can be further customized through [Modes and Contexts](#modes-and-contexts).\nFind the complete list of tools [below](#full-list-of-tools).\n\nThe use of all tools is generally recommended, as this allows Serena to provide the most value:\nOnly by executing shell commands (in particular, tests) can Serena identify and correct mistakes\nautonomously.\n\n#### Shell Execution and Editing Tools\n\nMany clients have their own shell execution tool, and by default Serena's shell tool will be disabled in them\n(e.g., when using the `ide-assistant` or `codex` context). However, when using Serena through something like\nClaude Desktop or ChatGPT, it is recommended to enable Serena's `execute_shell_command` tool to allow\nagentic behavior.\n\nIt should be noted that the `execute_shell_command` tool allows for arbitrary code execution.\nWhen using Serena as an MCP Server, clients will typically ask the user for permission\nbefore executing a tool, so as long as the user inspects execution parameters beforehand,\nthis should not be a problem.\nHowever, if you have concerns, you can choose to disable certain commands in your project's configuration file.\nIf you only want to use Serena purely for analyzing code and suggesting implementations\nwithout modifying the codebase, you can enable read-only mode by setting `read_only: true` in your project configuration file.\nThis will automatically disable all editing tools and prevent any modifications to your codebase while still\nallowing all analysis and exploration capabilities.\n\nIn general, be sure to back up your work and use a version control system in order to avoid\nlosing any work.\n\n### Modes and Contexts\n\nSerena's behavior and toolset can be adjusted using contexts and modes.\nThese allow for a high degree of customization to best suit your workflow and the environment Serena is operating in.\n\n#### Contexts\n\nA context defines the general environment in which Serena is operating.\nIt influences the initial system prompt and the set of available tools.\nA context is set at startup when launching Serena (e.g., via CLI options for an MCP server or in the agent script) and cannot be changed during an active session.\n\nSerena comes with pre-defined contexts:\n\n* `desktop-app`: Tailored for use with desktop applications like Claude Desktop. This is the default.\n* `agent`: Designed for scenarios where Serena acts as a more autonomous agent, for example, when used with Agno.\n* `ide-assistant`: Optimized for integration into IDEs like VSCode, Cursor, or Cline, focusing on in-editor coding assistance.\nChoose the context that best matches the type of integration you are using.\n\nWhen launching Serena, specify the context using `--context \u003ccontext-name\u003e`.\nNote that for cases where parameter lists are specified (e.g. Claude Desktop), you must add two parameters to the list.\n\nIf you are using a local server (such as Llama.cpp) which requires you to use OpenAI-compatible tool descriptions, use context `oaicompat-agent` instead of `agent`.\n\n#### Modes\n\nModes further refine Serena's behavior for specific types of tasks or interaction styles. Multiple modes can be active simultaneously, allowing you to combine their effects. Modes influence the system prompt and can also alter the set of available tools by excluding certain ones.\n\nExamples of built-in modes include:\n\n* `planning`: Focuses Serena on planning and analysis tasks.\n* `editing`: Optimizes Serena for direct code modification tasks.\n* `interactive`: Suitable for a conversational, back-and-forth interaction style.\n* `one-shot`: Configures Serena for tasks that should be completed in a single response, often used with `planning` for generating reports or initial plans.\n* `no-onboarding`: Skips the initial onboarding process if it's not needed for a particular session.\n* `onboarding`: (Usually triggered automatically) Focuses on the project onboarding process.\n\nModes can be set at startup (similar to contexts) but can also be _switched dynamically_ during a session. You can instruct the LLM to use the `switch_modes` tool to activate a different set of modes (e.g., \"switch to planning and one-shot modes\").\n\nWhen launching Serena, specify modes using `--mode \u003cmode-name\u003e`; multiple modes can be specified, e.g. `--mode planning --mode no-onboarding`.\n\n:warning: **Mode Compatibility**: While you can combine modes, some may be semantically incompatible (e.g., `interactive` and `one-shot`). Serena currently does not prevent incompatible combinations; it is up to the user to choose sensible mode configurations.\n\n#### Customization\n\nYou can create your own contexts and modes to precisely tailor Serena to your needs in two ways:\n\n* You can use Serena's CLI to manage modes and contexts. Check out\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena mode --help\n    ```\n\n    and\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena context --help\n    ```\n\n    _NOTE_: Custom contexts/modes are simply YAML files in `\u003chome\u003e/.serena`, they are automatically registered and available for use by their name (filename without the `.yml` extension). If you don't want to use Serena's CLI, you can create and manage them in any way you see fit.\n* **Using external YAML files**: When starting Serena, you can also provide an absolute path to a custom `.yml` file for a context or mode.\n\nThis customization allows for deep integration and adaptation of Serena to specific project requirements or personal preferences.\n\n### Onboarding and Memories\n\nBy default, Serena will perform an **onboarding process** when\nit is started for the first time for a project.\nThe goal of the onboarding is for Serena to get familiar with the project\nand to store memories, which it can then draw upon in future interactions.\nIf an LLM should fail to complete the onboarding and does not actually write the\nrespective memories to disk, you may need to ask it to do so explicitly.\n\nThe onboarding will usually read a lot of content from the project, thus filling\nup the context. It can therefore be advisable to switch to another conversation\nonce the onboarding is complete.\nAfter the onboarding, we recommend that you have a quick look at the memories and,\nif necessary, edit them or add additional ones.\n\n**Memories** are files stored in `.serena/memories/` in the project directory,\nwhich the agent can choose to read in subsequent interactions.\nFeel free to read and adjust them as needed; you can also add new ones manually.\nEvery file in the `.serena/memories/` directory is a memory file.\nWhenever Serena starts working on a project, the list of memories is\nprovided, and the agent can decide to read them.\nWe found that memories can significantly improve the user experience with Serena.\n\n### Prepare Your Project\n\n#### Structure Your Codebase\n\nSerena uses the code structure for finding, reading and editing code. This means that it will\nwork well with well-structured code but may perform poorly on fully unstructured one (like a \"God class\"\nwith enormous, non-modular functions).\nFurthermore, for languages that are not statically typed, type annotations are highly beneficial.\n\n#### Start from a Clean State\n\nIt is best to start a code generation task from a clean git state. Not only will\nthis make it easier for you to inspect the changes, but also the model itself will\nhave a chance of seeing what it has changed by calling `git diff` and thereby\ncorrect itself or continue working in a followup conversation if needed.\n\n:warning: **Important**: since Serena will write to files using the system-native line endings\nand it might want to look at the git diff, it is important to\nset `git config core.autocrlf` to `true` on Windows.\nWith `git config core.autocrlf` set to `false` on Windows, you may end up with huge diffs\nonly due to line endings. It is generally a good idea to globally enable this git setting on Windows:\n\n```shell\ngit config --global core.autocrlf true\n```\n\n#### Logging, Linting, and Automated Tests\n\nSerena can successfully complete tasks in an _agent loop_, where it iteratively\nacquires information, performs actions, and reflects on the results.\nHowever, Serena cannot use a debugger; it must rely on the results of program executions,\nlinting results, and test results to assess the correctness of its actions.\nTherefore, software that is designed to meaningful interpretable outputs (e.g. log messages)\nand that has a good test coverage is much easier to work with for Serena.\n\nWe generally recommend to start an editing task from a state where all linting checks and tests pass.\n\n### Prompting Strategies\n\nWe found that it is often a good idea to spend some time conceptualizing and planning a task\nbefore actually implementing it, especially for non-trivial task. This helps both in achieving\nbetter results and in increasing the feeling of control and staying in the loop. You can\nmake a detailed plan in one session, where Serena may read a lot of your code to build up the context,\nand then continue with the implementation in another (potentially after creating suitable memories).\n\n### Running Out of Context\n\nFor long and complicated tasks, or tasks where Serena has read a lot of content, you\nmay come close to the limits of context tokens. In that case, it is often a good idea to continue\nin a new conversation. Serena has a dedicated tool to create a summary of the current state\nof the progress and all relevant info for continuing it. You can request to create this summary and\nwrite it to a memory. Then, in a new conversation, you can just ask Serena to read the memory and\ncontinue with the task. In our experience, this worked really well. On the up-side, since in a\nsingle session there is no summarization involved, Serena does not usually get lost (unlike some\nother agents that summarize under the hood), and it is also instructed to occasionally check whether\nit's on the right track.\n\nMoreover, Serena is instructed to be frugal with context\n(e.g., to not read bodies of code symbols unnecessarily),\nbut we found that Claude is not always very good in being frugal (Gemini seemed better at it).\nYou can explicitly instruct it to not read the bodies if you know that it's not needed.\n\n### Serena's Logs: The Dashboard and GUI Tool\n\nSerena provides two convenient ways of accessing the logs of the current session:\n\n* via the **web-based dashboard** (enabled by default)\n\n    This is supported on all platforms.\n    By default, it will be accessible at `http://localhost:24282/dashboard/index.html`,\n    but a higher port may be used if the default port is unavailable/multiple instances are running.\n\n* via the **GUI tool** (disabled by default)\n\n    This is mainly supported on Windows, but it may also work on Linux; macOS is unsupported.\n\nBoth can be enabled, configured or disabled in Serena's configuration file (`serena_config.yml`, see above).\nIf enabled, they will automatically be opened as soon as the Serena agent/MCP server is started.\nThe web dashboard will display usage statistics of Serena's tools if you set  `record_tool_usage_stats: True` in your config.\n\nIn addition to viewing logs, both tools allow to shut down the Serena agent.\nThis function is provided, because clients like Claude Desktop may fail to terminate the MCP server subprocess\nwhen they themselves are closed.\n\n### Serena and GIT worktrees\n[git-worktree](https://git-scm.com/docs/git-worktree) can be an excellent way to parallelize your work. More on this in [Anthropic: Run parallel Claude Code sessions with Git worktrees](https://docs.claude.com/en/docs/claude-code/common-workflows#run-parallel-claude-code-sessions-with-git-worktrees).\n\nWhen it comes to serena AND git-worktree AND larger projects (that take longer to index), the recommended way is to COPY your `$ORIG_PROJECT/.serena/cache` to `$GIT_WORKTREE/.serena/cache`. After you have performed pre-indexing of your project described in [Project Activation \u0026 Indexing](#project-activation--indexing) section. To avoid having to re-index per each git work tree that you create. \n\n\n## Comparison with Other Coding Agents\n\nTo our knowledge, Serena is the first fully-featured coding agent where the\nentire functionality\nis available through an MCP server, thus not requiring API keys or\nsubscriptions.\n\n### Subscription-Based Coding Agents\n\nMany prominent subscription-based coding agents are parts of IDEs like\nWindsurf, Cursor and VSCode.\nSerena's functionality is similar to Cursor's Agent, Windsurf's Cascade or\nVSCode's agent mode.\n\nSerena has the advantage of not requiring a subscription.\nA potential disadvantage is that it\nis not directly integrated into an IDE, so the inspection of newly written code\nis not as seamless.\n\nMore technical differences are:\n\n* Serena is not bound to a specific IDE or CLI.\n  Serena's MCP server can be used with any MCP client (including some IDEs),\n  and the Agno-based agent provides additional ways of applying its functionality.\n* Serena is not bound to a specific large language model or API.\n* Serena navigates and edits code using a language server, so it has a symbolic\n  understanding of the code.\n  IDE-based tools often use a RAG-based or purely text-based approach, which is often\n  less powerful, especially for large codebases.\n* Serena is open-source and has a small codebase, so it can be easily extended\n  and modified.\n\n### API-Based Coding Agents\n\nAn alternative to subscription-based agents are API-based agents like Claude\nCode, Cline, Aider, Roo Code and others, where the usage costs map directly\nto the API costs of the underlying LLM.\nSome of them (like Cline) can even be included in IDEs as an extension.\nThey are often very powerful and their main downside are the (potentially very\nhigh) API costs.\n\nSerena itself can be used as an API-based agent (see the section on Agno above).\nWe have not yet written a CLI tool or a\ndedicated IDE extension for Serena (and there is probably no need for the latter, as\nSerena can already be used with any IDE that supports MCP servers).\nIf there is demand for a Serena as a CLI tool like Claude Code, we will\nconsider writing one.\n\nThe main difference between Serena and other API-based agents is that Serena can\nalso be used as an MCP server, thus not requiring\nan API key and bypassing the API costs. This is a unique feature of Serena.\n\n### Other MCP-Based Coding Agents\n\nThere are other MCP servers designed for coding, like [DesktopCommander](https://github.com/wonderwhy-er/DesktopCommanderMCP) and\n[codemcp](https://github.com/ezyang/codemcp).\nHowever, to the best of our knowledge, none of them provide semantic code\nretrieval and editing tools; they rely purely on text-based analysis.\nIt is the integration of language servers and the MCP that makes Serena unique\nand so powerful for challenging coding tasks, especially in the context of\nlarger codebases.\n\n## Acknowledgements\n\n### Sponsors\n\nWe are very grateful to our [sponsors](https://github.com/sponsors/oraios) who help us drive Serena's development. The core team\n(the founders of [Oraios AI](https://oraios-ai.de/)) put in a lot of work in order to turn Serena into a useful open source project. \nSo far, there is no business model behind this project, and sponsors are our only source of income from it.\n\nSponsors help us dedicating more time to the project, managing contributions, and working on larger features (like better tooling based on more advanced\nLSP features, VSCode integration, debugging via the DAP, and several others).\nIf you find this project useful to your work, or would like to accelerate the development of Serena, consider becoming a sponsor.\n\nWe are proud to announce that the Visual Studio Code team, together with Microsoft’s Open Source Programs Office and GitHub Open Source\nhave decided to sponsor Serena with a one-time contribution!\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"resources/vscode_sponsor_logo.png\" alt=\"Visual Studio Code sponsor logo\" width=\"220\"\u003e\n\u003c/p\u003e\n\n### Community Contributions\n\nA significant part of Serena, especially support for various languages, was contributed by the open source community.\nWe are very grateful for the many contributors who made this possible and who played an important role in making Serena\nwhat it is today.\n\n### Technologies\nWe built Serena on top of multiple existing open-source technologies, the most important ones being:\n\n1. [multilspy](https://github.com/microsoft/multilspy).\n   A library which wraps language server implementations and adapts them for interaction via Python\n   and which provided the basis for our library Solid-LSP (src/solidlsp).\n   Solid-LSP provides pure synchronous LSP calls and extends the original library with the symbolic logic\n   that Serena required.\n2. [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n3. [Agno](https://github.com/agno-agi/agno) and\n   the associated [agent-ui](https://github.com/agno-agi/agent-ui),\n   which we use to allow Serena to work with any model, beyond the ones\n   supporting the MCP.\n4. All the language servers that we use through Solid-LSP.\n\nWithout these projects, Serena would not have been possible (or would have been significantly more difficult to build).\n\n## Customizing and Extending Serena\n\nIt is straightforward to extend Serena's AI functionality with your own ideas.\nSimply implement a new tool by subclassing\n`serena.agent.Tool` and implement the `apply` method with a signature\nthat matches the tool's requirements.\nOnce implemented, `SerenaAgent` will automatically have access to the new tool.\n\nIt is also relatively straightforward to add [support for a new programming language](/.serena/memories/adding_new_language_support_guide.md).\n\nWe look forward to seeing what the community will come up with!\nFor details on contributing, see [contributing guidelines](/CONTRIBUTING.md).\n\n## List of Tools\n\nHere is the list of Serena's default tools with a short description (output of `uv run serena tools list`):\n\n* `activate_project`: Activates a project based on the project name or path.\n* `check_onboarding_performed`: Checks whether project onboarding was already performed.\n* `create_text_file`: Creates/overwrites a file in the project directory.\n* `delete_memory`: Deletes a memory from Serena's project-specific memory store.\n* `execute_shell_command`: Executes a shell command.\n* `find_file`: Finds files in the given relative paths\n* `find_referencing_symbols`: Finds symbols that reference the symbol at the given location (optionally filtered by type).\n* `find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `get_current_config`: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.\n* `get_symbols_overview`: Gets an overview of the top-level symbols defined in a given file.\n* `insert_after_symbol`: Inserts content after the end of the definition of a given symbol.\n* `insert_before_symbol`: Inserts content before the beginning of the definition of a given symbol.\n* `list_dir`: Lists files and directories in the given directory (optionally with recursion).\n* `list_memories`: Lists memories in Serena's project-specific memory store.\n* `onboarding`: Performs onboarding (identifying the project structure and essential tasks, e.g. for testing or building).\n* `prepare_for_new_conversation`: Provides instructions for preparing for a new conversation (in order to continue with the necessary context).\n* `read_file`: Reads a file within the project directory.\n* `read_memory`: Reads the memory with the given name from Serena's project-specific memory store.\n* `rename_symbol`: Renames a symbol throughout the codebase using language server refactoring capabilities.\n* `replace_regex`: Replaces content in a file by using regular expressions.\n* `replace_symbol_body`: Replaces the full definition of a symbol.\n* `search_for_pattern`: Performs a search for a pattern in the project.\n* `think_about_collected_information`: Thinking tool for pondering the completeness of collected information.\n* `think_about_task_adherence`: Thinking tool for determining whether the agent is still on track with the current task.\n* `think_about_whether_you_are_done`: Thinking tool for determining whether the task is truly completed.\n* `write_memory`: Writes a named memory (for future reference) to Serena's project-specific memory store.\n\nThere are several tools that are disabled by default, and have to be enabled explicitly, e.g., through the context or modes.\nNote that several of our default contexts do enable some of these tools. For example, the `desktop-app` context enables the `execute_shell_command` tool.\n\nThe full list of optional tools is (output of `uv run serena tools list --only-optional`):\n\n* `delete_lines`: Deletes a range of lines within a file.\n* `get_current_config`: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.\n* `initial_instructions`: Gets the initial instructions for the current project.\n    Should only be used in settings where the system prompt cannot be set,\n    e.g. in clients you have no control over, like Claude Desktop.\n* `insert_at_line`: Inserts content at a given line in a file.\n* `jet_brains_find_referencing_symbols`: Finds symbols that reference the given symbol\n* `jet_brains_find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `jet_brains_get_symbols_overview`: Retrieves an overview of the top-level symbols within a specified file\n* `remove_project`: Removes a project from the Serena configuration.\n* `replace_lines`: Replaces a range of lines within a file with new content.\n* `restart_language_server`: Restarts the language server, may be necessary when edits not through Serena happen.\n* `summarize_changes`: Provides instructions for summarizing the changes made to the codebase.\n* `switch_modes`: Activates modes by providing a list of their names\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:52Z","updated_at":"2025-10-22T11:21:07Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"pypi","identifier":"serena","version":"latest","runtime_hint":"uvx","runtime_arguments":[{"is_required":true,"format":"string","type":"named","name":"--from"},{"is_required":true,"format":"string","value":"git+https://github.com/oraios/serena","type":"positional","name":"git+https://github.com/oraios/serena"},{"is_required":true,"format":"string","value":"serena","type":"positional","name":"serena","value_hint":"cli"},{"is_required":true,"format":"string","value":"start-mcp-server","type":"positional","name":"start-mcp-server","value_hint":"subcommand"}],"package_arguments":[{"is_required":true,"format":"string","type":"named","name":"--context"},{"is_required":true,"format":"string","value":"ide-assistant","type":"positional","name":"ide-assistant"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"0658e342-7ae7-493a-b378-a324363ee322","is_latest":true,"published_at":"2025-09-09T10:55:22.910285Z","updated_at":"2025-09-09T10:55:22.910285Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Serena","is_in_organization":true,"license":"MIT License","name":"serena","name_with_owner":"oraios/serena","opengraph_image_url":"https://opengraph.githubassets.com/be8717cc1a6a3fb88cea14a861ba2b812892cf5ecc45da9df59c42919ff321a0/oraios/serena","owner_avatar_url":"https://avatars.githubusercontent.com/u/181485370?v=4","preferred_image":"https://avatars.githubusercontent.com/u/181485370?v=4","primary_language":"Python","primary_language_color":"#3572A5","pushed_at":"2025-10-21T22:37:12Z","readme":"\u003cp align=\"center\" style=\"text-align:center\"\u003e\n  \u003cimg src=\"resources/serena-logo.svg#gh-light-mode-only\" style=\"width:500px\"\u003e\n  \u003cimg src=\"resources/serena-logo-dark-mode.svg#gh-dark-mode-only\" style=\"width:500px\"\u003e\n\u003c/p\u003e\n\n* :rocket: Serena is a powerful **coding agent toolkit** capable of turning an LLM into a fully-featured agent that works **directly on your codebase**.\n  Unlike most other tools, it is not tied to an LLM, framework or an interface, making it easy to use it in a variety of ways.\n* :wrench: Serena provides essential **semantic code retrieval and editing tools** that are akin to an IDE's capabilities, extracting code entities at the symbol level and exploiting relational structure. When combined with an existing coding agent, these tools greatly enhance (token) efficiency.\n* :free: Serena is **free \u0026 open-source**, enhancing the capabilities of LLMs you already have access to free of charge.\n\nYou can think of Serena as providing IDE-like tools to your LLM/coding agent. With it, the agent no longer needs to read entire\nfiles, perform grep-like searches or string replacements to find and edit the right code. Instead, it can use code centered tools like `find_symbol`, `find_referencing_symbols` and `insert_after_symbol`.\n\n\u003cp align=\"center\"\u003e\n  \u003cem\u003eSerena is under active development! See the latest updates, upcoming features, and lessons learned to stay up to date.\u003c/em\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"CHANGELOG.md\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Updates-1e293b?style=flat\u0026logo=rss\u0026logoColor=white\u0026labelColor=1e293b\" alt=\"Changelog\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"roadmap.md\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Roadmap-14532d?style=flat\u0026logo=target\u0026logoColor=white\u0026labelColor=14532d\" alt=\"Roadmap\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"lessons_learned.md\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/Lessons-Learned-7c4700?style=flat\u0026logo=readthedocs\u0026logoColor=white\u0026labelColor=7c4700\" alt=\"Lessons Learned\" /\u003e\n  \u003c/a\u003e\n\u003c/p\u003e\n\n### LLM Integration\n\nSerena provides the necessary [tools](#list-of-tools) for coding workflows, but an LLM is required to do the actual work,\norchestrating tool use.\n\nFor example, **supercharge the performance of Claude Code** with a [one-line shell command](#claude-code).\n\nIn general, Serena can be integrated with an LLM in several ways:\n\n* by using the **model context protocol (MCP)**.\n  Serena provides an MCP server which integrates with\n    * Claude Code and Claude Desktop,\n    * Terminal-based clients like Codex, Gemini-CLI, Qwen3-Coder, rovodev, OpenHands CLI and others,\n    * IDEs like VSCode, Cursor or IntelliJ,\n    * Extensions like Cline or Roo Code\n    * Local clients like [OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp), [Jan](https://jan.ai/docs/mcp-examples/browser/browserbase#enable-mcp), [Agno](https://docs.agno.com/introduction/playground) and others\n* by using [mcpo to connect it to ChatGPT](docs/serena_on_chatgpt.md) or other clients that don't support MCP but do support tool calling via OpenAPI.\n* by incorporating Serena's tools into an agent framework of your choice, as illustrated [here](docs/custom_agent.md).\n  Serena's tool implementation is decoupled from the framework-specific code and can thus easily be adapted to any agent framework.\n\n### Serena in Action\n\n#### Demonstration 1: Efficient Operation in Claude Code\n\nA demonstration of Serena efficiently retrieving and editing code within Claude Code, thereby saving tokens and time. Efficient operations are not only useful for saving costs, but also for generally improving the generated code's quality. This effect may be less pronounced in very small projects, but often becomes of crucial importance in larger ones.\n\nhttps://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87\n\n#### Demonstration 2: Serena in Claude Desktop\n\nA demonstration of Serena implementing a small feature for itself (a better log GUI) with Claude Desktop.\nNote how Serena's tools enable Claude to find and edit the right symbols.\n\nhttps://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753\n\n### Programming Language Support \u0026 Semantic Analysis Capabilities\n\nSerena's semantic code analysis capabilities build on **language servers** using the widely implemented\nlanguage server protocol (LSP). The LSP provides a set of versatile code querying\nand editing functionalities based on symbolic understanding of the code.\nEquipped with these capabilities, Serena discovers and edits code just like a seasoned developer\nmaking use of an IDE's capabilities would.\nSerena can efficiently find the right context and do the right thing even in very large and\ncomplex projects! So not only is it free and open-source, it frequently achieves better results\nthan existing solutions that charge a premium.\n\nLanguage servers provide support for a wide range of programming languages.\nWith Serena, we provide direct, out-of-the-box support for:\n\n  * Python\n  * TypeScript/Javascript\n  * PHP (uses Intelephense LSP; set `INTELEPHENSE_LICENSE_KEY` environment variable for premium features)\n  * Go (requires installation of gopls)\n  * R (requires installation of the `languageserver` R package)\n  * Rust (requires [rustup](https://rustup.rs/) - uses rust-analyzer from your toolchain)\n  * C/C++ (you may experience issues with finding references, we are working on it)\n  * Zig (requires installation of ZLS - Zig Language Server)\n  * C#\n  * Ruby (by default, uses [ruby-lsp](https://github.com/Shopify/ruby-lsp), specify ruby_solargraph as your language to use the previous solargraph based implementation)\n  * Swift\n  * Kotlin (uses the pre-alpha [official kotlin LS](https://github.com/Kotlin/kotlin-lsp), some issues may appear)\n  * Java (_Note_: startup is slow, initial startup especially so. There may be issues with java on macos and linux, we are working on it.)\n  * Clojure\n  * Dart\n  * Bash\n  * Lua (automatically downloads lua-language-server if not installed)\n  * Nix (requires nixd installation)\n  * Elixir (requires installation of NextLS and Elixir; **Windows not supported**)\n  * Elm (automatically downloads elm-language-server if not installed; requires Elm compiler)\n  * Scala (requires some [manual setup](docs/scala_setup_guide_for_serena.md); uses Metals LSP)\n  * Erlang (requires installation of beam and [erlang_ls](https://github.com/erlang-ls/erlang_ls), experimental, might be slow or hang)\n  * Perl (requires installation of Perl::LanguageServer)\n  * AL\n  * Markdown (must be explicitly specified via `--language markdown` when generating project config, primarily useful for documentation-heavy projects)\n\nSupport for further languages can easily be added by providing a shallow adapter for a new language server implementation,\nsee Serena's [memory on that](.serena/memories/adding_new_language_support_guide.md).\n\n### Community Feedback\n\nMost users report that Serena has strong positive effects on the results of their coding agents, even when used within\nvery capable agents like Claude Code. Serena is often described to be a [game changer](https://www.reddit.com/r/ClaudeAI/comments/1lfsdll/try_out_serena_mcp_thank_me_later/), providing an enormous [productivity boost](https://www.reddit.com/r/ClaudeCode/comments/1mguoia/absolutely_insane_improvement_of_claude_code).\n\nSerena excels at navigating and manipulating complex codebases, providing tools that support precise code retrieval and editing in the presence of large, strongly structured codebases.\nHowever, when dealing with tasks that involve only very few/small files, you may not benefit from including Serena on top of your existing coding agent. \nIn particular, when writing code from scratch, Serena will not provide much value initially, as the more complex structures that Serena handles more gracefully than simplistic, file-based approaches are yet to be created.\n\nSeveral videos and blog posts have talked about Serena:\n\n* YouTube:\n    * [AI Labs](https://www.youtube.com/watch?v=wYWyJNs1HVk\u0026t=1s)\n    * [Yo Van Eyck](https://www.youtube.com/watch?v=UqfxuQKuMo8\u0026t=45s)\n    * [JeredBlu](https://www.youtube.com/watch?v=fzPnM3ySmjE\u0026t=32s)\n\n* Blog posts:\n    * [Serena's Design Principles](https://medium.com/@souradip1000/deconstructing-serenas-mcp-powered-semantic-code-understanding-architecture-75802515d116)\n    * [Serena with Claude Code (in Japanese)](https://blog.lai.so/serena/)\n    * [Turning Claude Code into a Development Powerhouse](https://robertmarshall.dev/blog/turning-claude-code-into-a-development-powerhouse/)\n\n## Table of Contents\n\n\u003c!-- Created with markdown-toc -i README.md --\u003e\n\u003c!-- Install it with npm install -g markdown-toc --\u003e\n\n\u003c!-- toc --\u003e\n\n- [Quick Start](#quick-start)\n  * [Running the Serena MCP Server](#running-the-serena-mcp-server)\n    + [Usage](#usage)\n    + [Using uvx](#using-uvx)\n    + [Local Installation](#local-installation)\n    + [Using Docker (Experimental)](#using-docker-experimental)\n    + [Using Nix](#using-nix)\n    + [Streamable HTTP Mode](#streamable-http-mode)\n    + [Command-Line Arguments](#command-line-arguments)\n  * [Configuration](#configuration)\n  * [Project Activation \u0026 Indexing](#project-activation--indexing)\n  * [Claude Code](#claude-code)\n  * [Codex](#codex)\n  * [Other Terminal-Based Clients](#other-terminal-based-clients)\n  * [Claude Desktop](#claude-desktop)\n  * [MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)](#mcp-coding-clients-cline-roo-code-cursor-windsurf-etc)\n  * [Local GUIs and Frameworks](#local-guis-and-frameworks)\n- [Detailed Usage and Recommendations](#detailed-usage-and-recommendations)\n  * [Tool Execution](#tool-execution)\n    + [Shell Execution and Editing Tools](#shell-execution-and-editing-tools)\n  * [Modes and Contexts](#modes-and-contexts)\n    + [Contexts](#contexts)\n    + [Modes](#modes)\n    + [Customization](#customization)\n  * [Onboarding and Memories](#onboarding-and-memories)\n  * [Prepare Your Project](#prepare-your-project)\n    + [Structure Your Codebase](#structure-your-codebase)\n    + [Start from a Clean State](#start-from-a-clean-state)\n    + [Logging, Linting, and Automated Tests](#logging-linting-and-automated-tests)\n  * [Prompting Strategies](#prompting-strategies)\n  * [Running Out of Context](#running-out-of-context)\n  * [Serena's Logs: The Dashboard and GUI Tool](#serenas-logs-the-dashboard-and-gui-tool)\n  * [Serena and GIT worktrees](#serena-and-git-worktrees)\n- [Comparison with Other Coding Agents](#comparison-with-other-coding-agents)\n  * [Subscription-Based Coding Agents](#subscription-based-coding-agents)\n  * [API-Based Coding Agents](#api-based-coding-agents)\n  * [Other MCP-Based Coding Agents](#other-mcp-based-coding-agents)\n- [Acknowledgements](#acknowledgements)\n  * [Sponsors](#sponsors)\n  * [Community Contributions](#community-contributions)\n  * [Technologies](#technologies)\n- [Customizing and Extending Serena](#customizing-and-extending-serena)\n- [List of Tools](#list-of-tools)\n\n\u003c!-- tocstop --\u003e\n\n## Quick Start\n\nSerena can be used in various ways, below you will find instructions for selected integrations.\n\n* For coding with Claude, we recommend using Serena through [Claude Code](#claude-code) or [Claude Desktop](#claude-desktop). You can also use Serena in most other [terminal-based clients](#other-terminal-based-clients).\n* If you want a GUI experience outside an IDE, you can use one of the many [local GUIs](#local-guis-and-frameworks) that support MCP servers.\n  You can also connect Serena to many web clients (including ChatGPT) using [mcpo](docs/serena_on_chatgpt.md).\n* If you want to use Serena integrated in your IDE, see the section on [other MCP clients](#other-mcp-clients---cline-roo-code-cursor-windsurf-etc).\n* You can use Serena as a library for building your own applications. We try to keep the public API stable, but you should still\n  expect breaking changes and pin Serena to a fixed version if you use it as a dependency.\n\nSerena is managed by `uv`, so you will need to [install it](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Running the Serena MCP Server\n\nYou have several options for running the MCP server, which are explained in the subsections below.\n\n#### Usage\n\nThe typical usage involves the client (Claude Code, Claude Desktop, etc.) running\nthe MCP server as a subprocess (using stdio communication),\nso the client needs to be provided with the command to run the MCP server.\n(Alternatively, you can run the MCP server in Streamable HTTP or SSE mode and tell your client\nhow to connect to it.)\n\nNote that no matter how you run the MCP server, Serena will, by default, start a small web-based dashboard on localhost that will display logs and allow shutting down the\nMCP server (since many clients fail to clean up processes correctly).\nThis and other settings can be adjusted in the [configuration](#configuration) and/or by providing [command-line arguments](#command-line-arguments).\n\n#### Using uvx\n\n`uvx` can be used to run the latest version of Serena directly from the repository, without an explicit local installation.\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena start-mcp-server\n```\n\nExplore the CLI to see some of the customization options that serena provides (more info on them below).\n\n#### Local Installation\n\n1. Clone the repository and change into it.\n\n   ```shell\n   git clone https://github.com/oraios/serena\n   cd serena\n   ```\n\n2. Optionally edit the configuration file in your home directory with\n\n   ```shell\n   uv run serena config edit\n   ```\n\n   If you just want the default config, you can skip this part, and a config file will be created when you first run Serena.\n3. Run the server with `uv`:\n\n   ```shell\n   uv run serena start-mcp-server\n   ```\n\n   When running from outside the serena installation directory, be sure to pass it, i.e., use\n\n   ```shell\n    uv run --directory /abs/path/to/serena serena start-mcp-server\n   ```\n\n#### Using Docker (Experimental)\n\n⚠️ Docker support is currently experimental with several limitations. Please read the [Docker documentation](DOCKER.md) for important caveats before using it.\n\nYou can run the Serena MCP server directly via docker as follows,\nassuming that the projects you want to work on are all located in `/path/to/your/projects`:\n\n```shell\ndocker run --rm -i --network host -v /path/to/your/projects:/workspaces/projects ghcr.io/oraios/serena:latest serena start-mcp-server --transport stdio\n```\n\nReplace `/path/to/your/projects` with the absolute path to your projects directory. The Docker approach provides:\n\n* Better security isolation for shell command execution\n* No need to install language servers and dependencies locally\n* Consistent environment across different systems\n\nAlternatively, use docker compose with the `compose.yml` file provided in the repository.\n\nSee the [Docker documentation](DOCKER.md) for detailed setup instructions, configuration options, and known limitations.\n\n#### Using Nix\n\nIf you are using Nix and [have enabled the `nix-command` and `flakes` features](https://nixos.wiki/wiki/flakes), you can run Serena using the following command:\n\n```bash\nnix run github:oraios/serena -- start-mcp-server --transport stdio\n```\n\nYou can also install Serena by referencing this repo (`github:oraios/serena`) and using it in your Nix flake. The package is exported as `serena`.\n\n#### Streamable HTTP Mode\n\nℹ️ Note that MCP servers which use stdio as a protocol are somewhat unusual as far as client/server architectures go, as the server\nnecessarily has to be started by the client in order for communication to take place via the server's standard input/output stream.\nIn other words, you do not need to start the server yourself. The client application (e.g. Claude Desktop) takes care of this and\ntherefore needs to be configured with a launch command.\n\nWhen using instead the *Streamable HTTP* mode, you control the server lifecycle yourself,\ni.e. you start the server and provide the client with the URL to connect to it.\n\nSimply provide `start-mcp-server` with the `--transport streamable-http` option and optionally provide the port.\nFor example, to run the Serena MCP server in Streamable HTTP mode on port 9121 using a local installation,\nyou would run this command from the Serena directory,\n\n```shell\nuv run serena start-mcp-server --transport streamable-http --port 9121\n```\n\nand then configure your client to connect to `http://localhost:9121/mcp`.\n\nℹ️ Note that SSE transport is supported as well, but its use is discouraged. \nUse Streamable HTTP instead.\n\n#### Command-Line Arguments\n\nThe Serena MCP server supports a wide range of additional command-line options, including the option to run in Streamable HTTP or SSE mode\nand to adapt Serena to various [contexts and modes of operation](#modes-and-contexts).\n\nRun with parameter `--help` to get a list of available options.\n\n### Configuration\n\nSerena is very flexible in terms of configuration. While for most users, the default configurations will work,\nyou can fully adjust it to your needs by editing a few yaml files. You can disable tools, change Serena's instructions\n(what we denote as the `system_prompt`), adjust the output of tools that just provide a prompt, and even adjust tool descriptions.\n\nSerena is configured in four places:\n\n1. The `serena_config.yml` for general settings that apply to all clients and projects.\n   It is located in your user directory under `.serena/serena_config.yml`.\n   If you do not explicitly create the file, it will be auto-generated when you first run Serena.\n   You can edit it directly or use\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena config edit\n   ```\n\n   (or use the `--directory` command version).\n2. In the arguments passed to the `start-mcp-server` in your client's config (see below),\n   which will apply to all sessions started by the respective client. In particular, the [context](#contexts) parameter\n   should be set appropriately for Serena to be best adjusted to existing tools and capabilities of your client.\n   See for a detailed explanation. You can override all entries from the `serena_config.yml` through command line arguments.\n3. In the `.serena/project.yml` file within your project. This will hold project-level configuration that is used whenever\n   that project is activated. This file will be autogenerated when you first use Serena on that project, but you can also\n   generate it explicitly with\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena project generate-yml\n   ```\n\n   (or use the `--directory` command version).\n4. Through the context and modes. Explore the [modes and contexts](#modes-and-contexts) section for more details.\n\nAfter the initial setup, continue with one of the sections below, depending on how you\nwant to use Serena.\n\n### Project Activation \u0026 Indexing\n\nIf you are mostly working with the same project, you can configure to always activate it at startup\nby passing `--project \u003cpath_or_name\u003e` to the `start-mcp-server` command in your client's MCP config.\nThis is especially useful for clients which configure MCP servers on a per-project basis, like Claude Code.\n\nOtherwise, the recommended way is to just ask the LLM to activate a project by providing it an absolute path to, or,\nin case the project was activated in the past, by its name. The default project name is the directory name.\n\n* \"Activate the project /path/to/my_project\"\n* \"Activate the project my_project\"\n\nAll projects that have been activated will be automatically added to your `serena_config.yml`, and for each\nproject, the file `.serena/project.yml` will be generated. You can adjust the latter, e.g., by changing the name\n(which you refer to during the activation) or other options. Make sure to not have two different projects with the\nsame name.\n\nℹ️ For larger projects, we recommend that you index your project to accelerate Serena's tools; otherwise the first\ntool application may be very slow.\nTo do so, run this from the project directory (or pass the path to the project as an argument):\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena project index\n```\n\n(or use the `--directory` command version).\n\n### Claude Code\n\nSerena is a great way to make Claude Code both cheaper and more powerful!\n\nFrom your project directory, add serena with a command like this,\n\n```shell\nclaude mcp add serena -- \u003cserena-mcp-server\u003e --context ide-assistant --project \"$(pwd)\"\n```\n\nwhere `\u003cserena-mcp-server\u003e` is your way of [running the Serena MCP server](#running-the-serena-mcp-server).\nFor example, when using `uvx`, you would run\n\n```shell\nclaude mcp add serena -- uvx --from git+https://github.com/oraios/serena serena start-mcp-server --context ide-assistant --project \"$(pwd)\"\n```\n\nℹ️ Serena comes with an instruction text, and Claude needs to read it to properly use Serena's tools.\n  As of version `v1.0.52`, claude code reads the instructions of the MCP server, so this **is handled automatically**.\n  If you are using an older version, or if Claude fails to read the instructions, you can ask it explicitly\n  to \"read Serena's initial instructions\" or run `/mcp__serena__initial_instructions` to load the instruction text.\n  If you want to make use of that, you will have to enable the corresponding tool explicitly by adding `initial_instructions` to the `included_optional_tools`\n  in your config.\n  Note that you may have to make Claude read the instructions when you start a new conversation and after any compacting operation to ensure Claude remains properly configured to use Serena's tools.\n\n### Codex\n\nSerena works with OpenAI's Codex CLI out of the box, but you have to use the `codex` context for it to work properly. (The technical reason is that Codex doesn't fully support the MCP specifications, so some massaging of tools is required.).\n\nUnlike Claude Code, in Codex you add an MCP server globally and not per project. Add the following to\n`~/.codex/config.toml` (create the file if it does not exist):\n\n```toml\n[mcp_servers.serena]\ncommand = \"uvx\"\nargs = [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\", \"--context\", \"codex\"]\n```\n\nAfter codex has started, you need to activate the project, which you can do by saying:\n\n\"Activate the current dir as project using serena\"\n\n\u003e If you don't activate the project, you will not be able to use Serena's tools!\n\nThat's it! Have a look at `~/.codex/log/codex-tui.log` to see if any errors occurred.\n\nThe Serena dashboard will run if you have not disabled it in the configuration, but due to Codex's sandboxing the webbrowser\nmay not open automatically. You can open it manually by going to `http://localhost:24282/dashboard/index.html` (or a higher port, if\nthat was already taken).\n\n\u003e Codex will often show the tools as `failed` even though they are successfully executed. This is not a problem, seems to be a bug in Codex. Despite the error message, everything works as expected.\n\n### Other Terminal-Based Clients\n\nThere are many terminal-based coding assistants that support MCP servers, such as [Codex](https://github.com/openai/codex?tab=readme-ov-file#model-context-protocol-mcp),\n[Gemini-CLI](https://github.com/google-gemini/gemini-cli), [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder),\n[rovodev](https://community.atlassian.com/forums/Rovo-for-Software-Teams-Beta/Introducing-Rovo-Dev-CLI-AI-Powered-Development-in-your-terminal/ba-p/3043623),\nthe [OpenHands CLI](https://docs.all-hands.dev/usage/how-to/cli-mode) and [opencode](https://github.com/sst/opencode).\n\nThey generally benefit from the symbolic tools provided by Serena. You might want to customize some aspects of Serena\nby writing your own context, modes or prompts to adjust it to your workflow, to other MCP servers you are using, and to\nthe client's internal capabilities.\n\n### Claude Desktop\n\nFor [Claude Desktop](https://claude.ai/download) (available for Windows and macOS), go to File / Settings / Developer / MCP Servers / Edit Config,\nwhich will let you open the JSON file `claude_desktop_config.json`.\nAdd the `serena` MCP server configuration, using a [run command](#running-the-serena-mcp-server) depending on your setup.\n\n* local installation:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uv\",\n               \"args\": [\"run\", \"--directory\", \"/abs/path/to/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n   }\n   ```\n\n* uvx:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uvx\",\n               \"args\": [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n  }\n  ```\n\n* docker:\n\n  ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"docker\",\n               \"args\": [\"run\", \"--rm\", \"-i\", \"--network\", \"host\", \"-v\", \"/path/to/your/projects:/workspaces/projects\", \"ghcr.io/oraios/serena:latest\", \"serena\", \"start-mcp-server\", \"--transport\", \"stdio\"]\n           }\n       }\n   }\n   ```\n\nIf you are using paths containing backslashes for paths on Windows\n(note that you can also just use forward slashes), be sure to escape them correctly (`\\\\`).\n\nThat's it! Save the config and then restart Claude Desktop. You are ready for activating your first project.\n\nℹ️ You can further customize the run command using additional arguments (see [above](#command-line-arguments)).\n\nNote: on Windows and macOS there are official Claude Desktop applications by Anthropic, for Linux there is an [open-source\ncommunity version](https://github.com/aaddrick/claude-desktop-debian).\n\n⚠️ Be sure to fully quit the Claude Desktop application, as closing Claude will just minimize it to the system tray – at least on Windows.\n\n⚠️ Some clients may leave behind zombie processes. You will have to find and terminate them manually then.\n    With Serena, you can activate the [dashboard](#serenas-logs-the-dashboard-and-gui-tool) to prevent unnoted processes and also use the dashboard\n    for shutting down Serena.\n\nAfter restarting, you should see Serena's tools in your chat interface (notice the small hammer icon).\n\nFor more information on MCP servers with Claude Desktop, see [the official quick start guide](https://modelcontextprotocol.io/quickstart/user).\n\n### MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)\n\nBeing an MCP Server, Serena can be included in any MCP Client. The same configuration as above,\nperhaps with small client-specific modifications, should work. Most of the popular\nexisting coding assistants (IDE extensions or VSCode-like IDEs) support connections\nto MCP Servers. It is **recommended to use the `ide-assistant` context** for these integrations by adding `\"--context\", \"ide-assistant\"` to the `args` in your MCP client's configuration. Including Serena generally boosts their performance\nby providing them tools for symbolic operations.\n\nIn this case, the billing for the usage continues to be controlled by the client of your choice\n(unlike with the Claude Desktop client). But you may still want to use Serena through such an approach,\ne.g., for one of the following reasons:\n\n1. You are already using a coding assistant (say Cline or Cursor) and just want to make it more powerful.\n2. You are on Linux and don't want to use the [community-created Claude Desktop](https://github.com/aaddrick/claude-desktop-debian).\n3. You want tighter integration of Serena into your IDE and don't mind paying for that.\n\n### Local GUIs and Frameworks\n\nOver the last months, several technologies have emerged that allow you to run a powerful local GUI\nand connect it to an MCP server. They will work with Serena out of the box.\nSome of the leading open source GUI technologies offering this are\n[Jan](https://jan.ai/docs/mcp), [OpenHands](https://github.com/All-Hands-AI/OpenHands/),\n[OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp) and [Agno](https://docs.agno.com/introduction/playground).\nThey allow combining Serena with almost any LLM (including locally running ones) and offer various other integrations.\n\n## Detailed Usage and Recommendations\n\n### Tool Execution\n\nSerena combines tools for semantic code retrieval with editing capabilities and shell execution.\nSerena's behavior can be further customized through [Modes and Contexts](#modes-and-contexts).\nFind the complete list of tools [below](#full-list-of-tools).\n\nThe use of all tools is generally recommended, as this allows Serena to provide the most value:\nOnly by executing shell commands (in particular, tests) can Serena identify and correct mistakes\nautonomously.\n\n#### Shell Execution and Editing Tools\n\nMany clients have their own shell execution tool, and by default Serena's shell tool will be disabled in them\n(e.g., when using the `ide-assistant` or `codex` context). However, when using Serena through something like\nClaude Desktop or ChatGPT, it is recommended to enable Serena's `execute_shell_command` tool to allow\nagentic behavior.\n\nIt should be noted that the `execute_shell_command` tool allows for arbitrary code execution.\nWhen using Serena as an MCP Server, clients will typically ask the user for permission\nbefore executing a tool, so as long as the user inspects execution parameters beforehand,\nthis should not be a problem.\nHowever, if you have concerns, you can choose to disable certain commands in your project's configuration file.\nIf you only want to use Serena purely for analyzing code and suggesting implementations\nwithout modifying the codebase, you can enable read-only mode by setting `read_only: true` in your project configuration file.\nThis will automatically disable all editing tools and prevent any modifications to your codebase while still\nallowing all analysis and exploration capabilities.\n\nIn general, be sure to back up your work and use a version control system in order to avoid\nlosing any work.\n\n### Modes and Contexts\n\nSerena's behavior and toolset can be adjusted using contexts and modes.\nThese allow for a high degree of customization to best suit your workflow and the environment Serena is operating in.\n\n#### Contexts\n\nA context defines the general environment in which Serena is operating.\nIt influences the initial system prompt and the set of available tools.\nA context is set at startup when launching Serena (e.g., via CLI options for an MCP server or in the agent script) and cannot be changed during an active session.\n\nSerena comes with pre-defined contexts:\n\n* `desktop-app`: Tailored for use with desktop applications like Claude Desktop. This is the default.\n* `agent`: Designed for scenarios where Serena acts as a more autonomous agent, for example, when used with Agno.\n* `ide-assistant`: Optimized for integration into IDEs like VSCode, Cursor, or Cline, focusing on in-editor coding assistance.\nChoose the context that best matches the type of integration you are using.\n\nWhen launching Serena, specify the context using `--context \u003ccontext-name\u003e`.\nNote that for cases where parameter lists are specified (e.g. Claude Desktop), you must add two parameters to the list.\n\nIf you are using a local server (such as Llama.cpp) which requires you to use OpenAI-compatible tool descriptions, use context `oaicompat-agent` instead of `agent`.\n\n#### Modes\n\nModes further refine Serena's behavior for specific types of tasks or interaction styles. Multiple modes can be active simultaneously, allowing you to combine their effects. Modes influence the system prompt and can also alter the set of available tools by excluding certain ones.\n\nExamples of built-in modes include:\n\n* `planning`: Focuses Serena on planning and analysis tasks.\n* `editing`: Optimizes Serena for direct code modification tasks.\n* `interactive`: Suitable for a conversational, back-and-forth interaction style.\n* `one-shot`: Configures Serena for tasks that should be completed in a single response, often used with `planning` for generating reports or initial plans.\n* `no-onboarding`: Skips the initial onboarding process if it's not needed for a particular session.\n* `onboarding`: (Usually triggered automatically) Focuses on the project onboarding process.\n\nModes can be set at startup (similar to contexts) but can also be _switched dynamically_ during a session. You can instruct the LLM to use the `switch_modes` tool to activate a different set of modes (e.g., \"switch to planning and one-shot modes\").\n\nWhen launching Serena, specify modes using `--mode \u003cmode-name\u003e`; multiple modes can be specified, e.g. `--mode planning --mode no-onboarding`.\n\n:warning: **Mode Compatibility**: While you can combine modes, some may be semantically incompatible (e.g., `interactive` and `one-shot`). Serena currently does not prevent incompatible combinations; it is up to the user to choose sensible mode configurations.\n\n#### Customization\n\nYou can create your own contexts and modes to precisely tailor Serena to your needs in two ways:\n\n* You can use Serena's CLI to manage modes and contexts. Check out\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena mode --help\n    ```\n\n    and\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena context --help\n    ```\n\n    _NOTE_: Custom contexts/modes are simply YAML files in `\u003chome\u003e/.serena`, they are automatically registered and available for use by their name (filename without the `.yml` extension). If you don't want to use Serena's CLI, you can create and manage them in any way you see fit.\n* **Using external YAML files**: When starting Serena, you can also provide an absolute path to a custom `.yml` file for a context or mode.\n\nThis customization allows for deep integration and adaptation of Serena to specific project requirements or personal preferences.\n\n### Onboarding and Memories\n\nBy default, Serena will perform an **onboarding process** when\nit is started for the first time for a project.\nThe goal of the onboarding is for Serena to get familiar with the project\nand to store memories, which it can then draw upon in future interactions.\nIf an LLM should fail to complete the onboarding and does not actually write the\nrespective memories to disk, you may need to ask it to do so explicitly.\n\nThe onboarding will usually read a lot of content from the project, thus filling\nup the context. It can therefore be advisable to switch to another conversation\nonce the onboarding is complete.\nAfter the onboarding, we recommend that you have a quick look at the memories and,\nif necessary, edit them or add additional ones.\n\n**Memories** are files stored in `.serena/memories/` in the project directory,\nwhich the agent can choose to read in subsequent interactions.\nFeel free to read and adjust them as needed; you can also add new ones manually.\nEvery file in the `.serena/memories/` directory is a memory file.\nWhenever Serena starts working on a project, the list of memories is\nprovided, and the agent can decide to read them.\nWe found that memories can significantly improve the user experience with Serena.\n\n### Prepare Your Project\n\n#### Structure Your Codebase\n\nSerena uses the code structure for finding, reading and editing code. This means that it will\nwork well with well-structured code but may perform poorly on fully unstructured one (like a \"God class\"\nwith enormous, non-modular functions).\nFurthermore, for languages that are not statically typed, type annotations are highly beneficial.\n\n#### Start from a Clean State\n\nIt is best to start a code generation task from a clean git state. Not only will\nthis make it easier for you to inspect the changes, but also the model itself will\nhave a chance of seeing what it has changed by calling `git diff` and thereby\ncorrect itself or continue working in a followup conversation if needed.\n\n:warning: **Important**: since Serena will write to files using the system-native line endings\nand it might want to look at the git diff, it is important to\nset `git config core.autocrlf` to `true` on Windows.\nWith `git config core.autocrlf` set to `false` on Windows, you may end up with huge diffs\nonly due to line endings. It is generally a good idea to globally enable this git setting on Windows:\n\n```shell\ngit config --global core.autocrlf true\n```\n\n#### Logging, Linting, and Automated Tests\n\nSerena can successfully complete tasks in an _agent loop_, where it iteratively\nacquires information, performs actions, and reflects on the results.\nHowever, Serena cannot use a debugger; it must rely on the results of program executions,\nlinting results, and test results to assess the correctness of its actions.\nTherefore, software that is designed to meaningful interpretable outputs (e.g. log messages)\nand that has a good test coverage is much easier to work with for Serena.\n\nWe generally recommend to start an editing task from a state where all linting checks and tests pass.\n\n### Prompting Strategies\n\nWe found that it is often a good idea to spend some time conceptualizing and planning a task\nbefore actually implementing it, especially for non-trivial task. This helps both in achieving\nbetter results and in increasing the feeling of control and staying in the loop. You can\nmake a detailed plan in one session, where Serena may read a lot of your code to build up the context,\nand then continue with the implementation in another (potentially after creating suitable memories).\n\n### Running Out of Context\n\nFor long and complicated tasks, or tasks where Serena has read a lot of content, you\nmay come close to the limits of context tokens. In that case, it is often a good idea to continue\nin a new conversation. Serena has a dedicated tool to create a summary of the current state\nof the progress and all relevant info for continuing it. You can request to create this summary and\nwrite it to a memory. Then, in a new conversation, you can just ask Serena to read the memory and\ncontinue with the task. In our experience, this worked really well. On the up-side, since in a\nsingle session there is no summarization involved, Serena does not usually get lost (unlike some\nother agents that summarize under the hood), and it is also instructed to occasionally check whether\nit's on the right track.\n\nMoreover, Serena is instructed to be frugal with context\n(e.g., to not read bodies of code symbols unnecessarily),\nbut we found that Claude is not always very good in being frugal (Gemini seemed better at it).\nYou can explicitly instruct it to not read the bodies if you know that it's not needed.\n\n### Serena's Logs: The Dashboard and GUI Tool\n\nSerena provides two convenient ways of accessing the logs of the current session:\n\n* via the **web-based dashboard** (enabled by default)\n\n    This is supported on all platforms.\n    By default, it will be accessible at `http://localhost:24282/dashboard/index.html`,\n    but a higher port may be used if the default port is unavailable/multiple instances are running.\n\n* via the **GUI tool** (disabled by default)\n\n    This is mainly supported on Windows, but it may also work on Linux; macOS is unsupported.\n\nBoth can be enabled, configured or disabled in Serena's configuration file (`serena_config.yml`, see above).\nIf enabled, they will automatically be opened as soon as the Serena agent/MCP server is started.\nThe web dashboard will display usage statistics of Serena's tools if you set  `record_tool_usage_stats: True` in your config.\n\nIn addition to viewing logs, both tools allow to shut down the Serena agent.\nThis function is provided, because clients like Claude Desktop may fail to terminate the MCP server subprocess\nwhen they themselves are closed.\n\n### Serena and GIT worktrees\n[git-worktree](https://git-scm.com/docs/git-worktree) can be an excellent way to parallelize your work. More on this in [Anthropic: Run parallel Claude Code sessions with Git worktrees](https://docs.claude.com/en/docs/claude-code/common-workflows#run-parallel-claude-code-sessions-with-git-worktrees).\n\nWhen it comes to serena AND git-worktree AND larger projects (that take longer to index), the recommended way is to COPY your `$ORIG_PROJECT/.serena/cache` to `$GIT_WORKTREE/.serena/cache`. After you have performed pre-indexing of your project described in [Project Activation \u0026 Indexing](#project-activation--indexing) section. To avoid having to re-index per each git work tree that you create. \n\n\n## Comparison with Other Coding Agents\n\nTo our knowledge, Serena is the first fully-featured coding agent where the\nentire functionality\nis available through an MCP server, thus not requiring API keys or\nsubscriptions.\n\n### Subscription-Based Coding Agents\n\nMany prominent subscription-based coding agents are parts of IDEs like\nWindsurf, Cursor and VSCode.\nSerena's functionality is similar to Cursor's Agent, Windsurf's Cascade or\nVSCode's agent mode.\n\nSerena has the advantage of not requiring a subscription.\nA potential disadvantage is that it\nis not directly integrated into an IDE, so the inspection of newly written code\nis not as seamless.\n\nMore technical differences are:\n\n* Serena is not bound to a specific IDE or CLI.\n  Serena's MCP server can be used with any MCP client (including some IDEs),\n  and the Agno-based agent provides additional ways of applying its functionality.\n* Serena is not bound to a specific large language model or API.\n* Serena navigates and edits code using a language server, so it has a symbolic\n  understanding of the code.\n  IDE-based tools often use a RAG-based or purely text-based approach, which is often\n  less powerful, especially for large codebases.\n* Serena is open-source and has a small codebase, so it can be easily extended\n  and modified.\n\n### API-Based Coding Agents\n\nAn alternative to subscription-based agents are API-based agents like Claude\nCode, Cline, Aider, Roo Code and others, where the usage costs map directly\nto the API costs of the underlying LLM.\nSome of them (like Cline) can even be included in IDEs as an extension.\nThey are often very powerful and their main downside are the (potentially very\nhigh) API costs.\n\nSerena itself can be used as an API-based agent (see the section on Agno above).\nWe have not yet written a CLI tool or a\ndedicated IDE extension for Serena (and there is probably no need for the latter, as\nSerena can already be used with any IDE that supports MCP servers).\nIf there is demand for a Serena as a CLI tool like Claude Code, we will\nconsider writing one.\n\nThe main difference between Serena and other API-based agents is that Serena can\nalso be used as an MCP server, thus not requiring\nan API key and bypassing the API costs. This is a unique feature of Serena.\n\n### Other MCP-Based Coding Agents\n\nThere are other MCP servers designed for coding, like [DesktopCommander](https://github.com/wonderwhy-er/DesktopCommanderMCP) and\n[codemcp](https://github.com/ezyang/codemcp).\nHowever, to the best of our knowledge, none of them provide semantic code\nretrieval and editing tools; they rely purely on text-based analysis.\nIt is the integration of language servers and the MCP that makes Serena unique\nand so powerful for challenging coding tasks, especially in the context of\nlarger codebases.\n\n## Acknowledgements\n\n### Sponsors\n\nWe are very grateful to our [sponsors](https://github.com/sponsors/oraios) who help us drive Serena's development. The core team\n(the founders of [Oraios AI](https://oraios-ai.de/)) put in a lot of work in order to turn Serena into a useful open source project. \nSo far, there is no business model behind this project, and sponsors are our only source of income from it.\n\nSponsors help us dedicating more time to the project, managing contributions, and working on larger features (like better tooling based on more advanced\nLSP features, VSCode integration, debugging via the DAP, and several others).\nIf you find this project useful to your work, or would like to accelerate the development of Serena, consider becoming a sponsor.\n\nWe are proud to announce that the Visual Studio Code team, together with Microsoft’s Open Source Programs Office and GitHub Open Source\nhave decided to sponsor Serena with a one-time contribution!\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"resources/vscode_sponsor_logo.png\" alt=\"Visual Studio Code sponsor logo\" width=\"220\"\u003e\n\u003c/p\u003e\n\n### Community Contributions\n\nA significant part of Serena, especially support for various languages, was contributed by the open source community.\nWe are very grateful for the many contributors who made this possible and who played an important role in making Serena\nwhat it is today.\n\n### Technologies\nWe built Serena on top of multiple existing open-source technologies, the most important ones being:\n\n1. [multilspy](https://github.com/microsoft/multilspy).\n   A library which wraps language server implementations and adapts them for interaction via Python\n   and which provided the basis for our library Solid-LSP (src/solidlsp).\n   Solid-LSP provides pure synchronous LSP calls and extends the original library with the symbolic logic\n   that Serena required.\n2. [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n3. [Agno](https://github.com/agno-agi/agno) and\n   the associated [agent-ui](https://github.com/agno-agi/agent-ui),\n   which we use to allow Serena to work with any model, beyond the ones\n   supporting the MCP.\n4. All the language servers that we use through Solid-LSP.\n\nWithout these projects, Serena would not have been possible (or would have been significantly more difficult to build).\n\n## Customizing and Extending Serena\n\nIt is straightforward to extend Serena's AI functionality with your own ideas.\nSimply implement a new tool by subclassing\n`serena.agent.Tool` and implement the `apply` method with a signature\nthat matches the tool's requirements.\nOnce implemented, `SerenaAgent` will automatically have access to the new tool.\n\nIt is also relatively straightforward to add [support for a new programming language](/.serena/memories/adding_new_language_support_guide.md).\n\nWe look forward to seeing what the community will come up with!\nFor details on contributing, see [contributing guidelines](/CONTRIBUTING.md).\n\n## List of Tools\n\nHere is the list of Serena's default tools with a short description (output of `uv run serena tools list`):\n\n* `activate_project`: Activates a project based on the project name or path.\n* `check_onboarding_performed`: Checks whether project onboarding was already performed.\n* `create_text_file`: Creates/overwrites a file in the project directory.\n* `delete_memory`: Deletes a memory from Serena's project-specific memory store.\n* `execute_shell_command`: Executes a shell command.\n* `find_file`: Finds files in the given relative paths\n* `find_referencing_symbols`: Finds symbols that reference the symbol at the given location (optionally filtered by type).\n* `find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `get_current_config`: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.\n* `get_symbols_overview`: Gets an overview of the top-level symbols defined in a given file.\n* `insert_after_symbol`: Inserts content after the end of the definition of a given symbol.\n* `insert_before_symbol`: Inserts content before the beginning of the definition of a given symbol.\n* `list_dir`: Lists files and directories in the given directory (optionally with recursion).\n* `list_memories`: Lists memories in Serena's project-specific memory store.\n* `onboarding`: Performs onboarding (identifying the project structure and essential tasks, e.g. for testing or building).\n* `prepare_for_new_conversation`: Provides instructions for preparing for a new conversation (in order to continue with the necessary context).\n* `read_file`: Reads a file within the project directory.\n* `read_memory`: Reads the memory with the given name from Serena's project-specific memory store.\n* `rename_symbol`: Renames a symbol throughout the codebase using language server refactoring capabilities.\n* `replace_regex`: Replaces content in a file by using regular expressions.\n* `replace_symbol_body`: Replaces the full definition of a symbol.\n* `search_for_pattern`: Performs a search for a pattern in the project.\n* `think_about_collected_information`: Thinking tool for pondering the completeness of collected information.\n* `think_about_task_adherence`: Thinking tool for determining whether the agent is still on track with the current task.\n* `think_about_whether_you_are_done`: Thinking tool for determining whether the task is truly completed.\n* `write_memory`: Writes a named memory (for future reference) to Serena's project-specific memory store.\n\nThere are several tools that are disabled by default, and have to be enabled explicitly, e.g., through the context or modes.\nNote that several of our default contexts do enable some of these tools. For example, the `desktop-app` context enables the `execute_shell_command` tool.\n\nThe full list of optional tools is (output of `uv run serena tools list --only-optional`):\n\n* `delete_lines`: Deletes a range of lines within a file.\n* `get_current_config`: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.\n* `initial_instructions`: Gets the initial instructions for the current project.\n    Should only be used in settings where the system prompt cannot be set,\n    e.g. in clients you have no control over, like Claude Desktop.\n* `insert_at_line`: Inserts content at a given line in a file.\n* `jet_brains_find_referencing_symbols`: Finds symbols that reference the given symbol\n* `jet_brains_find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `jet_brains_get_symbols_overview`: Retrieves an overview of the top-level symbols within a specified file\n* `remove_project`: Removes a project from the Serena configuration.\n* `replace_lines`: Replaces a range of lines within a file with new content.\n* `restart_language_server`: Restarts the language server, may be necessary when edits not through Serena happen.\n* `summarize_changes`: Provides instructions for summarizing the changes made to the codebase.\n* `switch_modes`: Activates modes by providing a list of their names\n","stargazer_count":14664,"topics":["agent","ai","llms","vibe-coding","mcp-server","ai-coding","language-server","programming","claude","claude-code"],"uses_custom_opengraph_image":false}}}},{"name":"chromedevtools/chrome-devtools-mcp","description":"Lets your coding agent (such as Gemini, Claude, Cursor or Copilot) control and inspect a live Chrome browser.","status":"active","repository":{"url":"https://github.com/ChromeDevTools/chrome-devtools-mcp","source":"github","id":"1054793726","readme":"# Chrome DevTools MCP\n\n[![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devtools-mcp.svg)](https://npmjs.org/package/chrome-devtools-mcp)\n\n`chrome-devtools-mcp` lets your coding agent (such as Gemini, Claude, Cursor or Copilot)\ncontrol and inspect a live Chrome browser. It acts as a Model-Context-Protocol\n(MCP) server, giving your AI coding assistant access to the full power of\nChrome DevTools for reliable automation, in-depth debugging, and performance analysis.\n\n## [Tool reference](./docs/tool-reference.md) | [Changelog](./CHANGELOG.md) | [Contributing](./CONTRIBUTING.md) | [Troubleshooting](./docs/troubleshooting.md)\n\n## Key features\n\n- **Get performance insights**: Uses [Chrome\n  DevTools](https://github.com/ChromeDevTools/devtools-frontend) to record\n  traces and extract actionable performance insights.\n- **Advanced browser debugging**: Analyze network requests, take screenshots and\n  check the browser console.\n- **Reliable automation**. Uses\n  [puppeteer](https://github.com/puppeteer/puppeteer) to automate actions in\n  Chrome and automatically wait for action results.\n\n## Disclaimers\n\n`chrome-devtools-mcp` exposes content of the browser instance to the MCP clients\nallowing them to inspect, debug, and modify any data in the browser or DevTools.\nAvoid sharing sensitive or personal information that you don't want to share with\nMCP clients.\n\n## Requirements\n\n- [Node.js](https://nodejs.org/) v20.19 or a newer [latest maintenance LTS](https://github.com/nodejs/Release#release-schedule) version.\n- [Chrome](https://www.google.com/chrome/) current stable version or newer.\n- [npm](https://www.npmjs.com/).\n\n## Getting started\n\nAdd the following config to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\n\u003e [!NOTE]  \n\u003e Using `chrome-devtools-mcp@latest` ensures that your MCP client will always use the latest version of the Chrome DevTools MCP server.\n\n### MCP Client configuration\n\n\u003cdetails\u003e\n  \u003csummary\u003eAmp\u003c/summary\u003e\n  Follow https://ampcode.com/manual#mcp and use the config provided above. You can also install the Chrome DevTools MCP server using the CLI:\n\n```bash\namp mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eClaude Code\u003c/summary\u003e\n    Use the Claude Code CLI to add the Chrome DevTools MCP server (\u003ca href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\"\u003eguide\u003c/a\u003e):\n\n```bash\nclaude mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCline\u003c/summary\u003e\n  Follow https://docs.cline.bot/mcp/configuring-mcp-servers and use the config provided above.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCodex\u003c/summary\u003e\n  Follow the \u003ca href=\"https://github.com/openai/codex/blob/main/docs/advanced.md#model-context-protocol-mcp\"\u003econfigure MCP guide\u003c/a\u003e\n  using the standard config from above. You can also install the Chrome DevTools MCP server using the Codex CLI:\n\n```bash\ncodex mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n**On Windows 11**\n\nConfigure the Chrome install location and increase the startup timeout by updating `.codex/config.toml` and adding the following `env` and `startup_timeout_ms` parameters:\n\n```\n[mcp_servers.chrome-devtools]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"chrome-devtools-mcp@latest\",\n]\nenv = { SystemRoot=\"C:\\\\Windows\", PROGRAMFILES=\"C:\\\\Program Files\" }\nstartup_timeout_ms = 20_000\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCopilot CLI\u003c/summary\u003e\n\nStart Copilot CLI:\n\n```\ncopilot\n```\n\nStart the dialog to add a new MCP server by running:\n\n```\n/mcp add\n```\n\nConfigure the following fields and press `CTRL+S` to save the configuration:\n\n- **Server name:** `chrome-devtools`\n- **Server Type:** `[1] Local`\n- **Command:** `npx -y chrome-devtools-mcp@latest`\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCopilot / VS Code\u003c/summary\u003e\n  Follow the MCP install \u003ca href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server\"\u003eguide\u003c/a\u003e,\n  with the standard config from above. You can also install the Chrome DevTools MCP server using the VS Code CLI:\n  \n  ```bash\n  code --add-mcp '{\"name\":\"chrome-devtools\",\"command\":\"npx\",\"args\":[\"chrome-devtools-mcp@latest\"]}'\n  ```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCursor\u003c/summary\u003e\n\n**Click the button to install:**\n\n[\u003cimg src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\"\u003e](https://cursor.com/en/install-mcp?name=chrome-devtools\u0026config=eyJjb21tYW5kIjoibnB4IC15IGNocm9tZS1kZXZ0b29scy1tY3BAbGF0ZXN0In0%3D)\n\n**Or install manually:**\n\nGo to `Cursor Settings` -\u003e `MCP` -\u003e `New MCP Server`. Use the config provided above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eGemini CLI\u003c/summary\u003e\nInstall the Chrome DevTools MCP server using the Gemini CLI.\n\n**Project wide:**\n\n```bash\ngemini mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n**Globally:**\n\n```bash\ngemini mcp add -s user chrome-devtools npx chrome-devtools-mcp@latest\n```\n\nAlternatively, follow the \u003ca href=\"https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server\"\u003eMCP guide\u003c/a\u003e and use the standard config from above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eGemini Code Assist\u003c/summary\u003e\n  Follow the \u003ca href=\"https://cloud.google.com/gemini/docs/codeassist/use-agentic-chat-pair-programmer#configure-mcp-servers\"\u003econfigure MCP guide\u003c/a\u003e\n  using the standard config from above.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eJetBrains AI Assistant \u0026 Junie\u003c/summary\u003e\n\nGo to `Settings | Tools | AI Assistant | Model Context Protocol (MCP)` -\u003e `Add`. Use the config provided above.\nThe same way chrome-devtools-mcp can be configured for JetBrains Junie in `Settings | Tools | Junie | MCP Settings` -\u003e `Add`. Use the config provided above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eKiro\u003c/summary\u003e\n\nIn **Kiro Settings**, go to `Configure MCP` \u003e `Open Workspace or User MCP Config` \u003e Use the configuration snippet provided above.\n\nOr, from the IDE **Activity Bar** \u003e `Kiro` \u003e `MCP Servers` \u003e `Click Open MCP Config`. Use the configuration snippet provided above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eQoder\u003c/summary\u003e\n\nIn **Qoder Settings**, go to `MCP Server` \u003e `+ Add` \u003e Use the configuration snippet provided above.\n\nAlternatively, follow the \u003ca href=\"https://docs.qoder.com/user-guide/chat/model-context-protocol\"\u003eMCP guide\u003c/a\u003e and use the standard config from above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eVisual Studio\u003c/summary\u003e\n  \n  **Click the button to install:**\n  \n  [\u003cimg src=\"https://img.shields.io/badge/Visual_Studio-Install-C16FDE?logo=visualstudio\u0026logoColor=white\" alt=\"Install in Visual Studio\"\u003e](https://vs-open.link/mcp-install?%7B%22name%22%3A%22chrome-devtools%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22chrome-devtools-mcp%40latest%22%5D%7D)\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eWarp\u003c/summary\u003e\n\nGo to `Settings | AI | Manage MCP Servers` -\u003e `+ Add` to [add an MCP Server](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server). Use the config provided above.\n\n\u003c/details\u003e\n\n### Your first prompt\n\nEnter the following prompt in your MCP Client to check if everything is working:\n\n```\nCheck the performance of https://developers.chrome.com\n```\n\nYour MCP client should open the browser and record a performance trace.\n\n\u003e [!NOTE]  \n\u003e The MCP server will start the browser automatically once the MCP client uses a tool that requires a running browser instance. Connecting to the Chrome DevTools MCP server on its own will not automatically start the browser.\n\n## Tools\n\nIf you run into any issues, checkout our [troubleshooting guide](./docs/troubleshooting.md).\n\n\u003c!-- BEGIN AUTO GENERATED TOOLS --\u003e\n\n- **Input automation** (7 tools)\n  - [`click`](docs/tool-reference.md#click)\n  - [`drag`](docs/tool-reference.md#drag)\n  - [`fill`](docs/tool-reference.md#fill)\n  - [`fill_form`](docs/tool-reference.md#fill_form)\n  - [`handle_dialog`](docs/tool-reference.md#handle_dialog)\n  - [`hover`](docs/tool-reference.md#hover)\n  - [`upload_file`](docs/tool-reference.md#upload_file)\n- **Navigation automation** (7 tools)\n  - [`close_page`](docs/tool-reference.md#close_page)\n  - [`list_pages`](docs/tool-reference.md#list_pages)\n  - [`navigate_page`](docs/tool-reference.md#navigate_page)\n  - [`navigate_page_history`](docs/tool-reference.md#navigate_page_history)\n  - [`new_page`](docs/tool-reference.md#new_page)\n  - [`select_page`](docs/tool-reference.md#select_page)\n  - [`wait_for`](docs/tool-reference.md#wait_for)\n- **Emulation** (3 tools)\n  - [`emulate_cpu`](docs/tool-reference.md#emulate_cpu)\n  - [`emulate_network`](docs/tool-reference.md#emulate_network)\n  - [`resize_page`](docs/tool-reference.md#resize_page)\n- **Performance** (3 tools)\n  - [`performance_analyze_insight`](docs/tool-reference.md#performance_analyze_insight)\n  - [`performance_start_trace`](docs/tool-reference.md#performance_start_trace)\n  - [`performance_stop_trace`](docs/tool-reference.md#performance_stop_trace)\n- **Network** (2 tools)\n  - [`get_network_request`](docs/tool-reference.md#get_network_request)\n  - [`list_network_requests`](docs/tool-reference.md#list_network_requests)\n- **Debugging** (5 tools)\n  - [`evaluate_script`](docs/tool-reference.md#evaluate_script)\n  - [`get_console_message`](docs/tool-reference.md#get_console_message)\n  - [`list_console_messages`](docs/tool-reference.md#list_console_messages)\n  - [`take_screenshot`](docs/tool-reference.md#take_screenshot)\n  - [`take_snapshot`](docs/tool-reference.md#take_snapshot)\n\n\u003c!-- END AUTO GENERATED TOOLS --\u003e\n\n## Configuration\n\nThe Chrome DevTools MCP server supports the following configuration option:\n\n\u003c!-- BEGIN AUTO GENERATED OPTIONS --\u003e\n\n- **`--browserUrl`, `-u`**\n  Connect to a running Chrome instance using port forwarding. For more details see: https://developer.chrome.com/docs/devtools/remote-debugging/local-server.\n  - **Type:** string\n\n- **`--wsEndpoint`, `-w`**\n  WebSocket endpoint to connect to a running Chrome instance (e.g., ws://127.0.0.1:9222/devtools/browser/\u003cid\u003e). Alternative to --browserUrl.\n  - **Type:** string\n\n- **`--wsHeaders`**\n  Custom headers for WebSocket connection in JSON format (e.g., '{\"Authorization\":\"Bearer token\"}'). Only works with --wsEndpoint.\n  - **Type:** string\n\n- **`--headless`**\n  Whether to run in headless (no UI) mode.\n  - **Type:** boolean\n  - **Default:** `false`\n\n- **`--executablePath`, `-e`**\n  Path to custom Chrome executable.\n  - **Type:** string\n\n- **`--isolated`**\n  If specified, creates a temporary user-data-dir that is automatically cleaned up after the browser is closed.\n  - **Type:** boolean\n  - **Default:** `false`\n\n- **`--channel`**\n  Specify a different Chrome channel that should be used. The default is the stable channel version.\n  - **Type:** string\n  - **Choices:** `stable`, `canary`, `beta`, `dev`\n\n- **`--logFile`**\n  Path to a file to write debug logs to. Set the env variable `DEBUG` to `*` to enable verbose logs. Useful for submitting bug reports.\n  - **Type:** string\n\n- **`--viewport`**\n  Initial viewport size for the Chrome instances started by the server. For example, `1280x720`. In headless mode, max size is 3840x2160px.\n  - **Type:** string\n\n- **`--proxyServer`**\n  Proxy server configuration for Chrome passed as --proxy-server when launching the browser. See https://www.chromium.org/developers/design-documents/network-settings/ for details.\n  - **Type:** string\n\n- **`--acceptInsecureCerts`**\n  If enabled, ignores errors relative to self-signed and expired certificates. Use with caution.\n  - **Type:** boolean\n\n- **`--chromeArg`**\n  Additional arguments for Chrome. Only applies when Chrome is launched by chrome-devtools-mcp.\n  - **Type:** array\n\n- **`--categoryEmulation`**\n  Set to false to exlcude tools related to emulation.\n  - **Type:** boolean\n  - **Default:** `true`\n\n- **`--categoryPerformance`**\n  Set to false to exlcude tools related to performance.\n  - **Type:** boolean\n  - **Default:** `true`\n\n- **`--categoryNetwork`**\n  Set to false to exlcude tools related to network.\n  - **Type:** boolean\n  - **Default:** `true`\n\n\u003c!-- END AUTO GENERATED OPTIONS --\u003e\n\nPass them via the `args` property in the JSON configuration. For example:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"chrome-devtools-mcp@latest\",\n        \"--channel=canary\",\n        \"--headless=true\",\n        \"--isolated=true\"\n      ]\n    }\n  }\n}\n```\n\n### Connecting via WebSocket with custom headers\n\nYou can connect directly to a Chrome WebSocket endpoint and include custom headers (e.g., for authentication):\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"chrome-devtools-mcp@latest\",\n        \"--wsEndpoint=ws://127.0.0.1:9222/devtools/browser/\u003cid\u003e\",\n        \"--wsHeaders={\\\"Authorization\\\":\\\"Bearer YOUR_TOKEN\\\"}\"\n      ]\n    }\n  }\n}\n```\n\nTo get the WebSocket endpoint from a running Chrome instance, visit `http://127.0.0.1:9222/json/version` and look for the `webSocketDebuggerUrl` field.\n\nYou can also run `npx chrome-devtools-mcp@latest --help` to see all available configuration options.\n\n## Concepts\n\n### User data directory\n\n`chrome-devtools-mcp` starts a Chrome's stable channel instance using the following user\ndata directory:\n\n- Linux / macOS: `$HOME/.cache/chrome-devtools-mcp/chrome-profile-$CHANNEL`\n- Windows: `%HOMEPATH%/.cache/chrome-devtools-mcp/chrome-profile-$CHANNEL`\n\nThe user data directory is not cleared between runs and shared across\nall instances of `chrome-devtools-mcp`. Set the `isolated` option to `true`\nto use a temporary user data dir instead which will be cleared automatically after\nthe browser is closed.\n\n### Connecting to a running Chrome instance\n\nYou can connect to a running Chrome instance by using the `--browser-url` option. This is useful if you want to use your existing Chrome profile or if you are running the MCP server in a sandboxed environment that does not allow starting a new Chrome instance.\n\nHere is a step-by-step guide on how to connect to a running Chrome Stable instance:\n\n**Step 1: Configure the MCP client**\n\nAdd the `--browser-url` option to your MCP client configuration. The value of this option should be the URL of the running Chrome instance. `http://127.0.0.1:9222` is a common default.\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"chrome-devtools-mcp@latest\",\n        \"--browser-url=http://127.0.0.1:9222\"\n      ]\n    }\n  }\n}\n```\n\n**Step 2: Start the Chrome browser**\n\n\u003e [!WARNING]  \n\u003e Enabling the remote debugging port opens up a debugging port on the running browser instance. Any application on your machine can connect to this port and control the browser. Make sure that you are not browsing any sensitive websites while the debugging port is open.\n\nStart the Chrome browser with the remote debugging port enabled. Make sure to close any running Chrome instances before starting a new one with the debugging port enabled. The port number you choose must be the same as the one you specified in the `--browser-url` option in your MCP client configuration.\n\nFor security reasons, [Chrome requires you to use a non-default user data directory](https://developer.chrome.com/blog/remote-debugging-port) when enabling the remote debugging port. You can specify a custom directory using the `--user-data-dir` flag. This ensures that your regular browsing profile and data are not exposed to the debugging session.\n\n**macOS**\n\n```bash\n/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-profile-stable\n```\n\n**Linux**\n\n```bash\n/usr/bin/google-chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-profile-stable\n```\n\n**Windows**\n\n```bash\n\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222 --user-data-dir=\"%TEMP%\\chrome-profile-stable\"\n```\n\n**Step 3: Test your setup**\n\nAfter configuring the MCP client and starting the Chrome browser, you can test your setup by running a simple prompt in your MCP client:\n\n```\nCheck the performance of https://developers.chrome.com\n```\n\nYour MCP client should connect to the running Chrome instance and receive a performance report.\n\nIf you hit VM-to-host port forwarding issues, see the “Remote debugging between virtual machine (VM) and host fails” section in [`docs/troubleshooting.md`](./docs/troubleshooting.md#remote-debugging-between-virtual-machine-vm-and-host-fails).\n\nFor more details on remote debugging, see the [Chrome DevTools documentation](https://developer.chrome.com/docs/devtools/remote-debugging/).\n\n## Known limitations\n\n### Operating system sandboxes\n\nSome MCP clients allow sandboxing the MCP server using macOS Seatbelt or Linux\ncontainers. If sandboxes are enabled, `chrome-devtools-mcp` is not able to start\nChrome that requires permissions to create its own sandboxes. As a workaround,\neither disable sandboxing for `chrome-devtools-mcp` in your MCP client or use\n`--browser-url` to connect to a Chrome instance that you start manually outside\nof the MCP client sandbox.\n"},"version":"0.0.1-seed","created_at":"2025-09-25T13:26:42Z","updated_at":"2025-10-22T11:21:50Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"chrome-devtools-mcp","version":"latest","runtime_hint":"npx","package_arguments":[{"description":"Optional: connect to an already-running Chrome (remote debugging / port-forward). Example: http://127.0.0.1:9222","is_required":true,"format":"string","value":"{browser_url}","variables":{"browser_url":{"description":"Optional: connect to an already-running Chrome (remote debugging / port-forward). Example: http://127.0.0.1:9222","is_required":true}},"type":"named","name":"--browserUrl"},{"description":"Run Chrome headless (true/false). Default: false","format":"string","value":"{headless}","variables":{"headless":{"description":"Run Chrome headless (true/false). Default: false","default":"false"}},"type":"named","name":"--headless"},{"description":"Use a temporary user-data-dir (true/false). Default: false","format":"string","value":"{isolated}","variables":{"isolated":{"description":"Use a temporary user-data-dir (true/false). Default: false","default":"false"}},"type":"named","name":"--isolated"},{"description":"Chrome channel: stable | canary | beta | dev (default: stable).","format":"string","value":"{chrome_channel}","variables":{"chrome_channel":{"description":"Chrome channel: stable | canary | beta | dev (default: stable).","default":"stable"}},"type":"named","name":"--channel"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"13749964-2447-4c31-bcab-32731cced504","is_latest":true,"published_at":"2025-09-09T10:55:22.907061Z","updated_at":"2025-09-09T10:55:22.907061Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Chrome DevTools","homepage_url":"https://npmjs.org/package/chrome-devtools-mcp","is_in_organization":true,"license":"Apache License 2.0","name":"chrome-devtools-mcp","name_with_owner":"ChromeDevTools/chrome-devtools-mcp","opengraph_image_url":"https://opengraph.githubassets.com/2a4653d4afd0cf29900b8d223d6fa08f20d27a6ed18a96fe3cab11f8f2f64530/ChromeDevTools/chrome-devtools-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/11260967?v=4","preferred_image":"https://avatars.githubusercontent.com/u/11260967?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-22T11:02:31Z","readme":"# Chrome DevTools MCP\n\n[![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devtools-mcp.svg)](https://npmjs.org/package/chrome-devtools-mcp)\n\n`chrome-devtools-mcp` lets your coding agent (such as Gemini, Claude, Cursor or Copilot)\ncontrol and inspect a live Chrome browser. It acts as a Model-Context-Protocol\n(MCP) server, giving your AI coding assistant access to the full power of\nChrome DevTools for reliable automation, in-depth debugging, and performance analysis.\n\n## [Tool reference](./docs/tool-reference.md) | [Changelog](./CHANGELOG.md) | [Contributing](./CONTRIBUTING.md) | [Troubleshooting](./docs/troubleshooting.md)\n\n## Key features\n\n- **Get performance insights**: Uses [Chrome\n  DevTools](https://github.com/ChromeDevTools/devtools-frontend) to record\n  traces and extract actionable performance insights.\n- **Advanced browser debugging**: Analyze network requests, take screenshots and\n  check the browser console.\n- **Reliable automation**. Uses\n  [puppeteer](https://github.com/puppeteer/puppeteer) to automate actions in\n  Chrome and automatically wait for action results.\n\n## Disclaimers\n\n`chrome-devtools-mcp` exposes content of the browser instance to the MCP clients\nallowing them to inspect, debug, and modify any data in the browser or DevTools.\nAvoid sharing sensitive or personal information that you don't want to share with\nMCP clients.\n\n## Requirements\n\n- [Node.js](https://nodejs.org/) v20.19 or a newer [latest maintenance LTS](https://github.com/nodejs/Release#release-schedule) version.\n- [Chrome](https://www.google.com/chrome/) current stable version or newer.\n- [npm](https://www.npmjs.com/).\n\n## Getting started\n\nAdd the following config to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n```\n\n\u003e [!NOTE]  \n\u003e Using `chrome-devtools-mcp@latest` ensures that your MCP client will always use the latest version of the Chrome DevTools MCP server.\n\n### MCP Client configuration\n\n\u003cdetails\u003e\n  \u003csummary\u003eAmp\u003c/summary\u003e\n  Follow https://ampcode.com/manual#mcp and use the config provided above. You can also install the Chrome DevTools MCP server using the CLI:\n\n```bash\namp mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eClaude Code\u003c/summary\u003e\n    Use the Claude Code CLI to add the Chrome DevTools MCP server (\u003ca href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\"\u003eguide\u003c/a\u003e):\n\n```bash\nclaude mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCline\u003c/summary\u003e\n  Follow https://docs.cline.bot/mcp/configuring-mcp-servers and use the config provided above.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCodex\u003c/summary\u003e\n  Follow the \u003ca href=\"https://github.com/openai/codex/blob/main/docs/advanced.md#model-context-protocol-mcp\"\u003econfigure MCP guide\u003c/a\u003e\n  using the standard config from above. You can also install the Chrome DevTools MCP server using the Codex CLI:\n\n```bash\ncodex mcp add chrome-devtools -- npx chrome-devtools-mcp@latest\n```\n\n**On Windows 11**\n\nConfigure the Chrome install location and increase the startup timeout by updating `.codex/config.toml` and adding the following `env` and `startup_timeout_ms` parameters:\n\n```\n[mcp_servers.chrome-devtools]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"chrome-devtools-mcp@latest\",\n]\nenv = { SystemRoot=\"C:\\\\Windows\", PROGRAMFILES=\"C:\\\\Program Files\" }\nstartup_timeout_ms = 20_000\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCopilot CLI\u003c/summary\u003e\n\nStart Copilot CLI:\n\n```\ncopilot\n```\n\nStart the dialog to add a new MCP server by running:\n\n```\n/mcp add\n```\n\nConfigure the following fields and press `CTRL+S` to save the configuration:\n\n- **Server name:** `chrome-devtools`\n- **Server Type:** `[1] Local`\n- **Command:** `npx -y chrome-devtools-mcp@latest`\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCopilot / VS Code\u003c/summary\u003e\n  Follow the MCP install \u003ca href=\"https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server\"\u003eguide\u003c/a\u003e,\n  with the standard config from above. You can also install the Chrome DevTools MCP server using the VS Code CLI:\n  \n  ```bash\n  code --add-mcp '{\"name\":\"chrome-devtools\",\"command\":\"npx\",\"args\":[\"chrome-devtools-mcp@latest\"]}'\n  ```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eCursor\u003c/summary\u003e\n\n**Click the button to install:**\n\n[\u003cimg src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\"\u003e](https://cursor.com/en/install-mcp?name=chrome-devtools\u0026config=eyJjb21tYW5kIjoibnB4IC15IGNocm9tZS1kZXZ0b29scy1tY3BAbGF0ZXN0In0%3D)\n\n**Or install manually:**\n\nGo to `Cursor Settings` -\u003e `MCP` -\u003e `New MCP Server`. Use the config provided above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eGemini CLI\u003c/summary\u003e\nInstall the Chrome DevTools MCP server using the Gemini CLI.\n\n**Project wide:**\n\n```bash\ngemini mcp add chrome-devtools npx chrome-devtools-mcp@latest\n```\n\n**Globally:**\n\n```bash\ngemini mcp add -s user chrome-devtools npx chrome-devtools-mcp@latest\n```\n\nAlternatively, follow the \u003ca href=\"https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server\"\u003eMCP guide\u003c/a\u003e and use the standard config from above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eGemini Code Assist\u003c/summary\u003e\n  Follow the \u003ca href=\"https://cloud.google.com/gemini/docs/codeassist/use-agentic-chat-pair-programmer#configure-mcp-servers\"\u003econfigure MCP guide\u003c/a\u003e\n  using the standard config from above.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eJetBrains AI Assistant \u0026 Junie\u003c/summary\u003e\n\nGo to `Settings | Tools | AI Assistant | Model Context Protocol (MCP)` -\u003e `Add`. Use the config provided above.\nThe same way chrome-devtools-mcp can be configured for JetBrains Junie in `Settings | Tools | Junie | MCP Settings` -\u003e `Add`. Use the config provided above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eKiro\u003c/summary\u003e\n\nIn **Kiro Settings**, go to `Configure MCP` \u003e `Open Workspace or User MCP Config` \u003e Use the configuration snippet provided above.\n\nOr, from the IDE **Activity Bar** \u003e `Kiro` \u003e `MCP Servers` \u003e `Click Open MCP Config`. Use the configuration snippet provided above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eQoder\u003c/summary\u003e\n\nIn **Qoder Settings**, go to `MCP Server` \u003e `+ Add` \u003e Use the configuration snippet provided above.\n\nAlternatively, follow the \u003ca href=\"https://docs.qoder.com/user-guide/chat/model-context-protocol\"\u003eMCP guide\u003c/a\u003e and use the standard config from above.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eVisual Studio\u003c/summary\u003e\n  \n  **Click the button to install:**\n  \n  [\u003cimg src=\"https://img.shields.io/badge/Visual_Studio-Install-C16FDE?logo=visualstudio\u0026logoColor=white\" alt=\"Install in Visual Studio\"\u003e](https://vs-open.link/mcp-install?%7B%22name%22%3A%22chrome-devtools%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22chrome-devtools-mcp%40latest%22%5D%7D)\n\u003c/details\u003e\n\n\u003cdetails\u003e\n  \u003csummary\u003eWarp\u003c/summary\u003e\n\nGo to `Settings | AI | Manage MCP Servers` -\u003e `+ Add` to [add an MCP Server](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server). Use the config provided above.\n\n\u003c/details\u003e\n\n### Your first prompt\n\nEnter the following prompt in your MCP Client to check if everything is working:\n\n```\nCheck the performance of https://developers.chrome.com\n```\n\nYour MCP client should open the browser and record a performance trace.\n\n\u003e [!NOTE]  \n\u003e The MCP server will start the browser automatically once the MCP client uses a tool that requires a running browser instance. Connecting to the Chrome DevTools MCP server on its own will not automatically start the browser.\n\n## Tools\n\nIf you run into any issues, checkout our [troubleshooting guide](./docs/troubleshooting.md).\n\n\u003c!-- BEGIN AUTO GENERATED TOOLS --\u003e\n\n- **Input automation** (7 tools)\n  - [`click`](docs/tool-reference.md#click)\n  - [`drag`](docs/tool-reference.md#drag)\n  - [`fill`](docs/tool-reference.md#fill)\n  - [`fill_form`](docs/tool-reference.md#fill_form)\n  - [`handle_dialog`](docs/tool-reference.md#handle_dialog)\n  - [`hover`](docs/tool-reference.md#hover)\n  - [`upload_file`](docs/tool-reference.md#upload_file)\n- **Navigation automation** (7 tools)\n  - [`close_page`](docs/tool-reference.md#close_page)\n  - [`list_pages`](docs/tool-reference.md#list_pages)\n  - [`navigate_page`](docs/tool-reference.md#navigate_page)\n  - [`navigate_page_history`](docs/tool-reference.md#navigate_page_history)\n  - [`new_page`](docs/tool-reference.md#new_page)\n  - [`select_page`](docs/tool-reference.md#select_page)\n  - [`wait_for`](docs/tool-reference.md#wait_for)\n- **Emulation** (3 tools)\n  - [`emulate_cpu`](docs/tool-reference.md#emulate_cpu)\n  - [`emulate_network`](docs/tool-reference.md#emulate_network)\n  - [`resize_page`](docs/tool-reference.md#resize_page)\n- **Performance** (3 tools)\n  - [`performance_analyze_insight`](docs/tool-reference.md#performance_analyze_insight)\n  - [`performance_start_trace`](docs/tool-reference.md#performance_start_trace)\n  - [`performance_stop_trace`](docs/tool-reference.md#performance_stop_trace)\n- **Network** (2 tools)\n  - [`get_network_request`](docs/tool-reference.md#get_network_request)\n  - [`list_network_requests`](docs/tool-reference.md#list_network_requests)\n- **Debugging** (5 tools)\n  - [`evaluate_script`](docs/tool-reference.md#evaluate_script)\n  - [`get_console_message`](docs/tool-reference.md#get_console_message)\n  - [`list_console_messages`](docs/tool-reference.md#list_console_messages)\n  - [`take_screenshot`](docs/tool-reference.md#take_screenshot)\n  - [`take_snapshot`](docs/tool-reference.md#take_snapshot)\n\n\u003c!-- END AUTO GENERATED TOOLS --\u003e\n\n## Configuration\n\nThe Chrome DevTools MCP server supports the following configuration option:\n\n\u003c!-- BEGIN AUTO GENERATED OPTIONS --\u003e\n\n- **`--browserUrl`, `-u`**\n  Connect to a running Chrome instance using port forwarding. For more details see: https://developer.chrome.com/docs/devtools/remote-debugging/local-server.\n  - **Type:** string\n\n- **`--wsEndpoint`, `-w`**\n  WebSocket endpoint to connect to a running Chrome instance (e.g., ws://127.0.0.1:9222/devtools/browser/\u003cid\u003e). Alternative to --browserUrl.\n  - **Type:** string\n\n- **`--wsHeaders`**\n  Custom headers for WebSocket connection in JSON format (e.g., '{\"Authorization\":\"Bearer token\"}'). Only works with --wsEndpoint.\n  - **Type:** string\n\n- **`--headless`**\n  Whether to run in headless (no UI) mode.\n  - **Type:** boolean\n  - **Default:** `false`\n\n- **`--executablePath`, `-e`**\n  Path to custom Chrome executable.\n  - **Type:** string\n\n- **`--isolated`**\n  If specified, creates a temporary user-data-dir that is automatically cleaned up after the browser is closed.\n  - **Type:** boolean\n  - **Default:** `false`\n\n- **`--channel`**\n  Specify a different Chrome channel that should be used. The default is the stable channel version.\n  - **Type:** string\n  - **Choices:** `stable`, `canary`, `beta`, `dev`\n\n- **`--logFile`**\n  Path to a file to write debug logs to. Set the env variable `DEBUG` to `*` to enable verbose logs. Useful for submitting bug reports.\n  - **Type:** string\n\n- **`--viewport`**\n  Initial viewport size for the Chrome instances started by the server. For example, `1280x720`. In headless mode, max size is 3840x2160px.\n  - **Type:** string\n\n- **`--proxyServer`**\n  Proxy server configuration for Chrome passed as --proxy-server when launching the browser. See https://www.chromium.org/developers/design-documents/network-settings/ for details.\n  - **Type:** string\n\n- **`--acceptInsecureCerts`**\n  If enabled, ignores errors relative to self-signed and expired certificates. Use with caution.\n  - **Type:** boolean\n\n- **`--chromeArg`**\n  Additional arguments for Chrome. Only applies when Chrome is launched by chrome-devtools-mcp.\n  - **Type:** array\n\n- **`--categoryEmulation`**\n  Set to false to exlcude tools related to emulation.\n  - **Type:** boolean\n  - **Default:** `true`\n\n- **`--categoryPerformance`**\n  Set to false to exlcude tools related to performance.\n  - **Type:** boolean\n  - **Default:** `true`\n\n- **`--categoryNetwork`**\n  Set to false to exlcude tools related to network.\n  - **Type:** boolean\n  - **Default:** `true`\n\n\u003c!-- END AUTO GENERATED OPTIONS --\u003e\n\nPass them via the `args` property in the JSON configuration. For example:\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"chrome-devtools-mcp@latest\",\n        \"--channel=canary\",\n        \"--headless=true\",\n        \"--isolated=true\"\n      ]\n    }\n  }\n}\n```\n\n### Connecting via WebSocket with custom headers\n\nYou can connect directly to a Chrome WebSocket endpoint and include custom headers (e.g., for authentication):\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"chrome-devtools-mcp@latest\",\n        \"--wsEndpoint=ws://127.0.0.1:9222/devtools/browser/\u003cid\u003e\",\n        \"--wsHeaders={\\\"Authorization\\\":\\\"Bearer YOUR_TOKEN\\\"}\"\n      ]\n    }\n  }\n}\n```\n\nTo get the WebSocket endpoint from a running Chrome instance, visit `http://127.0.0.1:9222/json/version` and look for the `webSocketDebuggerUrl` field.\n\nYou can also run `npx chrome-devtools-mcp@latest --help` to see all available configuration options.\n\n## Concepts\n\n### User data directory\n\n`chrome-devtools-mcp` starts a Chrome's stable channel instance using the following user\ndata directory:\n\n- Linux / macOS: `$HOME/.cache/chrome-devtools-mcp/chrome-profile-$CHANNEL`\n- Windows: `%HOMEPATH%/.cache/chrome-devtools-mcp/chrome-profile-$CHANNEL`\n\nThe user data directory is not cleared between runs and shared across\nall instances of `chrome-devtools-mcp`. Set the `isolated` option to `true`\nto use a temporary user data dir instead which will be cleared automatically after\nthe browser is closed.\n\n### Connecting to a running Chrome instance\n\nYou can connect to a running Chrome instance by using the `--browser-url` option. This is useful if you want to use your existing Chrome profile or if you are running the MCP server in a sandboxed environment that does not allow starting a new Chrome instance.\n\nHere is a step-by-step guide on how to connect to a running Chrome Stable instance:\n\n**Step 1: Configure the MCP client**\n\nAdd the `--browser-url` option to your MCP client configuration. The value of this option should be the URL of the running Chrome instance. `http://127.0.0.1:9222` is a common default.\n\n```json\n{\n  \"mcpServers\": {\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"chrome-devtools-mcp@latest\",\n        \"--browser-url=http://127.0.0.1:9222\"\n      ]\n    }\n  }\n}\n```\n\n**Step 2: Start the Chrome browser**\n\n\u003e [!WARNING]  \n\u003e Enabling the remote debugging port opens up a debugging port on the running browser instance. Any application on your machine can connect to this port and control the browser. Make sure that you are not browsing any sensitive websites while the debugging port is open.\n\nStart the Chrome browser with the remote debugging port enabled. Make sure to close any running Chrome instances before starting a new one with the debugging port enabled. The port number you choose must be the same as the one you specified in the `--browser-url` option in your MCP client configuration.\n\nFor security reasons, [Chrome requires you to use a non-default user data directory](https://developer.chrome.com/blog/remote-debugging-port) when enabling the remote debugging port. You can specify a custom directory using the `--user-data-dir` flag. This ensures that your regular browsing profile and data are not exposed to the debugging session.\n\n**macOS**\n\n```bash\n/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-profile-stable\n```\n\n**Linux**\n\n```bash\n/usr/bin/google-chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-profile-stable\n```\n\n**Windows**\n\n```bash\n\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9222 --user-data-dir=\"%TEMP%\\chrome-profile-stable\"\n```\n\n**Step 3: Test your setup**\n\nAfter configuring the MCP client and starting the Chrome browser, you can test your setup by running a simple prompt in your MCP client:\n\n```\nCheck the performance of https://developers.chrome.com\n```\n\nYour MCP client should connect to the running Chrome instance and receive a performance report.\n\nIf you hit VM-to-host port forwarding issues, see the “Remote debugging between virtual machine (VM) and host fails” section in [`docs/troubleshooting.md`](./docs/troubleshooting.md#remote-debugging-between-virtual-machine-vm-and-host-fails).\n\nFor more details on remote debugging, see the [Chrome DevTools documentation](https://developer.chrome.com/docs/devtools/remote-debugging/).\n\n## Known limitations\n\n### Operating system sandboxes\n\nSome MCP clients allow sandboxing the MCP server using macOS Seatbelt or Linux\ncontainers. If sandboxes are enabled, `chrome-devtools-mcp` is not able to start\nChrome that requires permissions to create its own sandboxes. As a workaround,\neither disable sandboxing for `chrome-devtools-mcp` in your MCP client or use\n`--browser-url` to connect to a Chrome instance that you start manually outside\nof the MCP client sandbox.\n","stargazer_count":11938,"topics":["mcp-server","puppeteer","chrome-devtools","browser","chrome","debugging","devtools","mcp"],"uses_custom_opengraph_image":false}}}},{"name":"firecrawl/firecrawl-mcp-server","description":"Extract web data with Firecrawl","status":"active","repository":{"url":"https://github.com/firecrawl/firecrawl-mcp-server","source":"github","id":"899407931","readme":"\u003cdiv align=\"center\"\u003e\n  \u003ca name=\"readme-top\"\u003e\u003c/a\u003e\n  \u003cimg\n    src=\"https://raw.githubusercontent.com/firecrawl/firecrawl-mcp-server/main/img/fire.png\"\n    height=\"140\"\n  \u003e\n\u003c/div\u003e\n\n# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/firecrawl/firecrawl) for web scraping capabilities.\n\n\u003e Big thanks to [@vrknetha](https://github.com/vrknetha), [@knacklabs](https://www.knacklabs.ai) for the initial implementation!\n\n## Features\n\n- Web scraping, crawling, and discovery\n- Search and content extraction\n- Deep research and batch scraping\n- Automatic retries and rate limiting\n- Cloud and self-hosted support\n- SSE support\n\n\u003e Play around with [our MCP Server on MCP.so's playground](https://mcp.so/playground?server=firecrawl-mcp-server) or on [Klavis AI](https://www.klavis.ai/mcp-servers).\n\n## Installation\n\n### Running with npx\n\n```bash\nenv FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g firecrawl-mcp\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️\nNote: Requires Cursor version 0.45.6+\nFor the most up-to-date configuration instructions, please refer to the official Cursor documentation on configuring MCP servers:\n[Cursor MCP Server Configuration Guide](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\nTo configure Firecrawl MCP in Cursor **v0.48.6**\n\n1. Open Cursor Settings\n2. Go to Features \u003e MCP Servers\n3. Click \"+ Add new global MCP server\"\n4. Enter the following code:\n   ```json\n   {\n     \"mcpServers\": {\n       \"firecrawl-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"firecrawl-mcp\"],\n         \"env\": {\n           \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\"\n         }\n       }\n     }\n   }\n   ```\n\nTo configure Firecrawl MCP in Cursor **v0.45.6**\n\n1. Open Cursor Settings\n2. Go to Features \u003e MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"firecrawl-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`\n\n\u003e If you are using Windows and are running into issues, try `cmd /c \"set FIRECRAWL_API_KEY=your-api-key \u0026\u0026 npx -y firecrawl-mcp\"`\n\nReplace `your-api-key` with your Firecrawl API key. If you don't have one yet, you can create an account and get it from https://www.firecrawl.dev/app/api-keys\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Firecrawl MCP when appropriate, but you can explicitly request it by describing your web scraping needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Running with Streamable HTTP Local Mode\n\nTo run the server using Streamable HTTP locally instead of the default stdio transport:\n\n```bash\nenv HTTP_STREAMABLE_SERVER=true FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\nUse the url: http://localhost:3000/mcp\n\n### Installing via Smithery (Legacy)\n\nTo install Firecrawl for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl):\n\n```bash\nnpx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude\n```\n\n### Running on VS Code\n\nFor one-click installation, click one of the install buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl\u0026inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl\u0026inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D\u0026quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"Firecrawl API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"firecrawl\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"firecrawl-mcp\"],\n        \"env\": {\n          \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"Firecrawl API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required for Cloud API\n\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key\n  - Required when using cloud API (default)\n  - Optional when using self-hosted instance with `FIRECRAWL_API_URL`\n- `FIRECRAWL_API_URL` (Optional): Custom API endpoint for self-hosted instances\n  - Example: `https://firecrawl.your-domain.com`\n  - If not provided, the cloud API will be used (requires API key)\n\n#### Optional Configuration\n\n##### Retry Configuration\n\n- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Maximum number of retry attempts (default: 3)\n- `FIRECRAWL_RETRY_INITIAL_DELAY`: Initial delay in milliseconds before first retry (default: 1000)\n- `FIRECRAWL_RETRY_MAX_DELAY`: Maximum delay in milliseconds between retries (default: 10000)\n- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Exponential backoff multiplier (default: 2)\n\n##### Credit Usage Monitoring\n\n- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Credit usage warning threshold (default: 1000)\n- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Credit usage critical threshold (default: 100)\n\n### Configuration Examples\n\nFor cloud API usage with custom retry and credit monitoring:\n\n```bash\n# Required for cloud API\nexport FIRECRAWL_API_KEY=your-api-key\n\n# Optional retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Increase max retry attempts\nexport FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Start with 2s delay\nexport FIRECRAWL_RETRY_MAX_DELAY=30000       # Maximum 30s delay\nexport FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # More aggressive backoff\n\n# Optional credit monitoring\nexport FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Warning at 2000 credits\nexport FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Critical at 500 credits\n```\n\nFor self-hosted instance:\n\n```bash\n# Required for self-hosted\nexport FIRECRAWL_API_URL=https://firecrawl.your-domain.com\n\n# Optional authentication for self-hosted\nexport FIRECRAWL_API_KEY=your-api-key  # If your instance requires auth\n\n# Custom retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=10\nexport FIRECRAWL_RETRY_INITIAL_DELAY=500     # Start with faster retries\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY_HERE\",\n\n        \"FIRECRAWL_RETRY_MAX_ATTEMPTS\": \"5\",\n        \"FIRECRAWL_RETRY_INITIAL_DELAY\": \"2000\",\n        \"FIRECRAWL_RETRY_MAX_DELAY\": \"30000\",\n        \"FIRECRAWL_RETRY_BACKOFF_FACTOR\": \"3\",\n\n        \"FIRECRAWL_CREDIT_WARNING_THRESHOLD\": \"2000\",\n        \"FIRECRAWL_CREDIT_CRITICAL_THRESHOLD\": \"500\"\n      }\n    }\n  }\n}\n```\n\n### System Configuration\n\nThe server includes several configurable parameters that can be set via environment variables. Here are the default values if not configured:\n\n```typescript\nconst CONFIG = {\n  retry: {\n    maxAttempts: 3, // Number of retry attempts for rate-limited requests\n    initialDelay: 1000, // Initial delay before first retry (in milliseconds)\n    maxDelay: 10000, // Maximum delay between retries (in milliseconds)\n    backoffFactor: 2, // Multiplier for exponential backoff\n  },\n  credit: {\n    warningThreshold: 1000, // Warn when credit usage reaches this level\n    criticalThreshold: 100, // Critical alert when credit usage reaches this level\n  },\n};\n```\n\nThese configurations control:\n\n1. **Retry Behavior**\n\n   - Automatically retries failed requests due to rate limits\n   - Uses exponential backoff to avoid overwhelming the API\n   - Example: With default settings, retries will be attempted at:\n     - 1st retry: 1 second delay\n     - 2nd retry: 2 seconds delay\n     - 3rd retry: 4 seconds delay (capped at maxDelay)\n\n2. **Credit Usage Monitoring**\n   - Tracks API credit consumption for cloud API usage\n   - Provides warnings at specified thresholds\n   - Helps prevent unexpected service interruption\n   - Example: With default settings:\n     - Warning at 1000 credits remaining\n     - Critical alert at 100 credits remaining\n\n### Rate Limiting and Batch Processing\n\nThe server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:\n\n- Automatic rate limit handling with exponential backoff\n- Efficient parallel processing for batch operations\n- Smart request queuing and throttling\n- Automatic retries for transient errors\n\n## How to Choose a Tool\n\nUse this guide to select the right tool for your task:\n\n- **If you know the exact URL(s) you want:**\n  - For one: use **scrape**\n  - For many: use **batch_scrape**\n- **If you need to discover URLs on a site:** use **map**\n- **If you want to search the web for info:** use **search**\n- **If you want to extract structured data:** use **extract**\n- **If you want to analyze a whole site or section:** use **crawl** (with limits!)\n\n### Quick Reference Table\n\n| Tool         | Best for                            | Returns         |\n| ------------ | ----------------------------------- | --------------- |\n| scrape       | Single page content                 | markdown/html   |\n| batch_scrape | Multiple known URLs                 | markdown/html[] |\n| map          | Discovering URLs on a site          | URL[]           |\n| crawl        | Multi-page extraction (with limits) | markdown/html[] |\n| search       | Web search for info                 | results[]       |\n| extract      | Structured data from pages          | JSON            |\n\n## Available Tools\n\n### 1. Scrape Tool (`firecrawl_scrape`)\n\nScrape content from a single URL with advanced options.\n\n**Best for:**\n\n- Single page content extraction, when you know exactly which page contains the information.\n\n**Not recommended for:**\n\n- Extracting content from multiple pages (use batch_scrape for known URLs, or map + batch_scrape to discover URLs first, or crawl for full page content)\n- When you're unsure which page contains the information (use search)\n- When you need structured data (use extract)\n\n**Common mistakes:**\n\n- Using scrape for a list of URLs (use batch_scrape instead).\n\n**Prompt Example:**\n\n\u003e \"Get the content of the page at https://example.com.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"formats\": [\"markdown\"],\n    \"onlyMainContent\": true,\n    \"waitFor\": 1000,\n    \"timeout\": 30000,\n    \"mobile\": false,\n    \"includeTags\": [\"article\", \"main\"],\n    \"excludeTags\": [\"nav\", \"footer\"],\n    \"skipTlsVerification\": false\n  }\n}\n```\n\n**Returns:**\n\n- Markdown, HTML, or other formats as specified.\n\n### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)\n\nScrape multiple URLs efficiently with built-in rate limiting and parallel processing.\n\n**Best for:**\n\n- Retrieving content from multiple pages, when you know exactly which pages to scrape.\n\n**Not recommended for:**\n\n- Discovering URLs (use map first if you don't know the URLs)\n- Scraping a single page (use scrape)\n\n**Common mistakes:**\n\n- Using batch_scrape with too many URLs at once (may hit rate limits or token overflow)\n\n**Prompt Example:**\n\n\u003e \"Get the content of these three blog posts: [url1, url2, url3].\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_batch_scrape\",\n  \"arguments\": {\n    \"urls\": [\"https://example1.com\", \"https://example2.com\"],\n    \"options\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. Check Batch Status (`firecrawl_check_batch_status`)\n\nCheck the status of a batch operation.\n\n```json\n{\n  \"name\": \"firecrawl_check_batch_status\",\n  \"arguments\": {\n    \"id\": \"batch_1\"\n  }\n}\n```\n\n### 4. Map Tool (`firecrawl_map`)\n\nMap a website to discover all indexed URLs on the site.\n\n**Best for:**\n\n- Discovering URLs on a website before deciding what to scrape\n- Finding specific sections of a website\n\n**Not recommended for:**\n\n- When you already know which specific URL you need (use scrape or batch_scrape)\n- When you need the content of the pages (use scrape after mapping)\n\n**Common mistakes:**\n\n- Using crawl to discover URLs instead of map\n\n**Prompt Example:**\n\n\u003e \"List all URLs on example.com.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_map\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n**Returns:**\n\n- Array of URLs found on the site\n\n### 5. Search Tool (`firecrawl_search`)\n\nSearch the web and optionally extract content from search results.\n\n**Best for:**\n\n- Finding specific information across multiple websites, when you don't know which website has the information.\n- When you need the most relevant content for a query\n\n**Not recommended for:**\n\n- When you already know which website to scrape (use scrape)\n- When you need comprehensive coverage of a single website (use map or crawl)\n\n**Common mistakes:**\n\n- Using crawl or map for open-ended questions (use search instead)\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_search\",\n  \"arguments\": {\n    \"query\": \"latest AI research papers 2023\",\n    \"limit\": 5,\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n\n- Array of search results (with optional scraped content)\n\n**Prompt Example:**\n\n\u003e \"Find the latest research papers on AI published in 2023.\"\n\n### 6. Crawl Tool (`firecrawl_crawl`)\n\nStarts an asynchronous crawl job on a website and extract content from all pages.\n\n**Best for:**\n\n- Extracting content from multiple related pages, when you need comprehensive coverage.\n\n**Not recommended for:**\n\n- Extracting content from a single page (use scrape)\n- When token limits are a concern (use map + batch_scrape)\n- When you need fast results (crawling can be slow)\n\n**Warning:** Crawl responses can be very large and may exceed token limits. Limit the crawl depth and number of pages, or use map + batch_scrape for better control.\n\n**Common mistakes:**\n\n- Setting limit or maxDepth too high (causes token overflow)\n- Using crawl for a single page (use scrape instead)\n\n**Prompt Example:**\n\n\u003e \"Get all blog posts from the first two levels of example.com/blog.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_crawl\",\n  \"arguments\": {\n    \"url\": \"https://example.com/blog/*\",\n    \"maxDepth\": 2,\n    \"limit\": 100,\n    \"allowExternalLinks\": false,\n    \"deduplicateSimilarURLs\": true\n  }\n}\n```\n\n**Returns:**\n\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Started crawl for: https://example.com/* with job ID: 550e8400-e29b-41d4-a716-446655440000. Use firecrawl_check_crawl_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Check Crawl Status (`firecrawl_check_crawl_status`)\n\nCheck the status of a crawl job.\n\n```json\n{\n  \"name\": \"firecrawl_check_crawl_status\",\n  \"arguments\": {\n    \"id\": \"550e8400-e29b-41d4-a716-446655440000\"\n  }\n}\n```\n\n**Returns:**\n\n- Response includes the status of the crawl job:\n\n### 8. Extract Tool (`firecrawl_extract`)\n\nExtract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.\n\n**Best for:**\n\n- Extracting specific structured data like prices, names, details.\n\n**Not recommended for:**\n\n- When you need the full content of a page (use scrape)\n- When you're not looking for specific structured data\n\n**Arguments:**\n\n- `urls`: Array of URLs to extract information from\n- `prompt`: Custom prompt for the LLM extraction\n- `systemPrompt`: System prompt to guide the LLM\n- `schema`: JSON schema for structured data extraction\n- `allowExternalLinks`: Allow extraction from external links\n- `enableWebSearch`: Enable web search for additional context\n- `includeSubdomains`: Include subdomains in extraction\n\nWhen using a self-hosted instance, the extraction will use your configured LLM. For cloud API, it uses Firecrawl's managed LLM service.\n**Prompt Example:**\n\n\u003e \"Extract the product name, price, and description from these product pages.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_extract\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n    \"prompt\": \"Extract product information including name, price, and description\",\n    \"systemPrompt\": \"You are a helpful assistant that extracts product information\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"price\": { \"type\": \"number\" },\n        \"description\": { \"type\": \"string\" }\n      },\n      \"required\": [\"name\", \"price\"]\n    },\n    \"allowExternalLinks\": false,\n    \"enableWebSearch\": false,\n    \"includeSubdomains\": false\n  }\n}\n```\n\n**Returns:**\n\n- Extracted structured data as defined by your schema\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"name\": \"Example Product\",\n        \"price\": 99.99,\n        \"description\": \"This is an example product description\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n- Operation status and progress\n- Performance metrics\n- Credit usage monitoring\n- Rate limit tracking\n- Error conditions\n\nExample log messages:\n\n```\n[INFO] Firecrawl MCP Server initialized successfully\n[INFO] Starting scrape for URL: https://example.com\n[INFO] Batch operation queued with ID: batch_1\n[WARNING] Credit usage has reached warning threshold\n[ERROR] Rate limit exceeded, retrying in 2s...\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Credit usage warnings\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Rate limit exceeded. Retrying in 2 seconds...\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n### Thanks to contributors\n\nThanks to [@vrknetha](https://github.com/vrknetha), [@cawstudios](https://caw.tech) for the initial implementation!\n\nThanks to MCP.so and Klavis AI for hosting and [@gstarwd](https://github.com/gstarwd), [@xiangkaiz](https://github.com/xiangkaiz) and [@zihaolin96](https://github.com/zihaolin96) for integrating our server.\n\n## License\n\nMIT License - see LICENSE file for details\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:45Z","updated_at":"2025-10-22T11:21:01Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"firecrawl-mcp","version":"latest","runtime_hint":"npx","runtime_arguments":[{"is_required":true,"format":"string","value":"-y","type":"positional","value_hint":"noninteractive_mode"}],"environment_variables":[{"value":"{api_key}","variables":{"api_key":{"description":"your API key","is_required":true,"is_secret":true}},"name":"FIRECRAWL_API_KEY"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"d586c74c-4aa1-4aa2-9aa3-046e58fcb84f","is_latest":true,"published_at":"2025-09-09T10:55:22.908539Z","updated_at":"2025-09-09T10:55:22.908539Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Firecrawl","homepage_url":"https://firecrawl.dev","is_in_organization":true,"license":"MIT License","name":"firecrawl-mcp-server","name_with_owner":"firecrawl/firecrawl-mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/7537200d3b36f0e3135d866d5e3092c1abd11b8681587f5c7d14132269909be8/firecrawl/firecrawl-mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/135057108?v=4","preferred_image":"https://avatars.githubusercontent.com/u/135057108?v=4","primary_language":"JavaScript","primary_language_color":"#f1e05a","pushed_at":"2025-10-19T15:08:42Z","readme":"\u003cdiv align=\"center\"\u003e\n  \u003ca name=\"readme-top\"\u003e\u003c/a\u003e\n  \u003cimg\n    src=\"https://raw.githubusercontent.com/firecrawl/firecrawl-mcp-server/main/img/fire.png\"\n    height=\"140\"\n  \u003e\n\u003c/div\u003e\n\n# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/firecrawl/firecrawl) for web scraping capabilities.\n\n\u003e Big thanks to [@vrknetha](https://github.com/vrknetha), [@knacklabs](https://www.knacklabs.ai) for the initial implementation!\n\n## Features\n\n- Web scraping, crawling, and discovery\n- Search and content extraction\n- Deep research and batch scraping\n- Automatic retries and rate limiting\n- Cloud and self-hosted support\n- SSE support\n\n\u003e Play around with [our MCP Server on MCP.so's playground](https://mcp.so/playground?server=firecrawl-mcp-server) or on [Klavis AI](https://www.klavis.ai/mcp-servers).\n\n## Installation\n\n### Running with npx\n\n```bash\nenv FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g firecrawl-mcp\n```\n\n### Running on Cursor\n\nConfiguring Cursor 🖥️\nNote: Requires Cursor version 0.45.6+\nFor the most up-to-date configuration instructions, please refer to the official Cursor documentation on configuring MCP servers:\n[Cursor MCP Server Configuration Guide](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\nTo configure Firecrawl MCP in Cursor **v0.48.6**\n\n1. Open Cursor Settings\n2. Go to Features \u003e MCP Servers\n3. Click \"+ Add new global MCP server\"\n4. Enter the following code:\n   ```json\n   {\n     \"mcpServers\": {\n       \"firecrawl-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"firecrawl-mcp\"],\n         \"env\": {\n           \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\"\n         }\n       }\n     }\n   }\n   ```\n\nTo configure Firecrawl MCP in Cursor **v0.45.6**\n\n1. Open Cursor Settings\n2. Go to Features \u003e MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"firecrawl-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`\n\n\u003e If you are using Windows and are running into issues, try `cmd /c \"set FIRECRAWL_API_KEY=your-api-key \u0026\u0026 npx -y firecrawl-mcp\"`\n\nReplace `your-api-key` with your Firecrawl API key. If you don't have one yet, you can create an account and get it from https://www.firecrawl.dev/app/api-keys\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Firecrawl MCP when appropriate, but you can explicitly request it by describing your web scraping needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Running with Streamable HTTP Local Mode\n\nTo run the server using Streamable HTTP locally instead of the default stdio transport:\n\n```bash\nenv HTTP_STREAMABLE_SERVER=true FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\nUse the url: http://localhost:3000/mcp\n\n### Installing via Smithery (Legacy)\n\nTo install Firecrawl for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl):\n\n```bash\nnpx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude\n```\n\n### Running on VS Code\n\nFor one-click installation, click one of the install buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl\u0026inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl\u0026inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D\u0026config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D\u0026quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"Firecrawl API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"firecrawl\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"firecrawl-mcp\"],\n        \"env\": {\n          \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"Firecrawl API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required for Cloud API\n\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key\n  - Required when using cloud API (default)\n  - Optional when using self-hosted instance with `FIRECRAWL_API_URL`\n- `FIRECRAWL_API_URL` (Optional): Custom API endpoint for self-hosted instances\n  - Example: `https://firecrawl.your-domain.com`\n  - If not provided, the cloud API will be used (requires API key)\n\n#### Optional Configuration\n\n##### Retry Configuration\n\n- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Maximum number of retry attempts (default: 3)\n- `FIRECRAWL_RETRY_INITIAL_DELAY`: Initial delay in milliseconds before first retry (default: 1000)\n- `FIRECRAWL_RETRY_MAX_DELAY`: Maximum delay in milliseconds between retries (default: 10000)\n- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Exponential backoff multiplier (default: 2)\n\n##### Credit Usage Monitoring\n\n- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Credit usage warning threshold (default: 1000)\n- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Credit usage critical threshold (default: 100)\n\n### Configuration Examples\n\nFor cloud API usage with custom retry and credit monitoring:\n\n```bash\n# Required for cloud API\nexport FIRECRAWL_API_KEY=your-api-key\n\n# Optional retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Increase max retry attempts\nexport FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Start with 2s delay\nexport FIRECRAWL_RETRY_MAX_DELAY=30000       # Maximum 30s delay\nexport FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # More aggressive backoff\n\n# Optional credit monitoring\nexport FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Warning at 2000 credits\nexport FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Critical at 500 credits\n```\n\nFor self-hosted instance:\n\n```bash\n# Required for self-hosted\nexport FIRECRAWL_API_URL=https://firecrawl.your-domain.com\n\n# Optional authentication for self-hosted\nexport FIRECRAWL_API_KEY=your-api-key  # If your instance requires auth\n\n# Custom retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=10\nexport FIRECRAWL_RETRY_INITIAL_DELAY=500     # Start with faster retries\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY_HERE\",\n\n        \"FIRECRAWL_RETRY_MAX_ATTEMPTS\": \"5\",\n        \"FIRECRAWL_RETRY_INITIAL_DELAY\": \"2000\",\n        \"FIRECRAWL_RETRY_MAX_DELAY\": \"30000\",\n        \"FIRECRAWL_RETRY_BACKOFF_FACTOR\": \"3\",\n\n        \"FIRECRAWL_CREDIT_WARNING_THRESHOLD\": \"2000\",\n        \"FIRECRAWL_CREDIT_CRITICAL_THRESHOLD\": \"500\"\n      }\n    }\n  }\n}\n```\n\n### System Configuration\n\nThe server includes several configurable parameters that can be set via environment variables. Here are the default values if not configured:\n\n```typescript\nconst CONFIG = {\n  retry: {\n    maxAttempts: 3, // Number of retry attempts for rate-limited requests\n    initialDelay: 1000, // Initial delay before first retry (in milliseconds)\n    maxDelay: 10000, // Maximum delay between retries (in milliseconds)\n    backoffFactor: 2, // Multiplier for exponential backoff\n  },\n  credit: {\n    warningThreshold: 1000, // Warn when credit usage reaches this level\n    criticalThreshold: 100, // Critical alert when credit usage reaches this level\n  },\n};\n```\n\nThese configurations control:\n\n1. **Retry Behavior**\n\n   - Automatically retries failed requests due to rate limits\n   - Uses exponential backoff to avoid overwhelming the API\n   - Example: With default settings, retries will be attempted at:\n     - 1st retry: 1 second delay\n     - 2nd retry: 2 seconds delay\n     - 3rd retry: 4 seconds delay (capped at maxDelay)\n\n2. **Credit Usage Monitoring**\n   - Tracks API credit consumption for cloud API usage\n   - Provides warnings at specified thresholds\n   - Helps prevent unexpected service interruption\n   - Example: With default settings:\n     - Warning at 1000 credits remaining\n     - Critical alert at 100 credits remaining\n\n### Rate Limiting and Batch Processing\n\nThe server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:\n\n- Automatic rate limit handling with exponential backoff\n- Efficient parallel processing for batch operations\n- Smart request queuing and throttling\n- Automatic retries for transient errors\n\n## How to Choose a Tool\n\nUse this guide to select the right tool for your task:\n\n- **If you know the exact URL(s) you want:**\n  - For one: use **scrape**\n  - For many: use **batch_scrape**\n- **If you need to discover URLs on a site:** use **map**\n- **If you want to search the web for info:** use **search**\n- **If you want to extract structured data:** use **extract**\n- **If you want to analyze a whole site or section:** use **crawl** (with limits!)\n\n### Quick Reference Table\n\n| Tool         | Best for                            | Returns         |\n| ------------ | ----------------------------------- | --------------- |\n| scrape       | Single page content                 | markdown/html   |\n| batch_scrape | Multiple known URLs                 | markdown/html[] |\n| map          | Discovering URLs on a site          | URL[]           |\n| crawl        | Multi-page extraction (with limits) | markdown/html[] |\n| search       | Web search for info                 | results[]       |\n| extract      | Structured data from pages          | JSON            |\n\n## Available Tools\n\n### 1. Scrape Tool (`firecrawl_scrape`)\n\nScrape content from a single URL with advanced options.\n\n**Best for:**\n\n- Single page content extraction, when you know exactly which page contains the information.\n\n**Not recommended for:**\n\n- Extracting content from multiple pages (use batch_scrape for known URLs, or map + batch_scrape to discover URLs first, or crawl for full page content)\n- When you're unsure which page contains the information (use search)\n- When you need structured data (use extract)\n\n**Common mistakes:**\n\n- Using scrape for a list of URLs (use batch_scrape instead).\n\n**Prompt Example:**\n\n\u003e \"Get the content of the page at https://example.com.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"formats\": [\"markdown\"],\n    \"onlyMainContent\": true,\n    \"waitFor\": 1000,\n    \"timeout\": 30000,\n    \"mobile\": false,\n    \"includeTags\": [\"article\", \"main\"],\n    \"excludeTags\": [\"nav\", \"footer\"],\n    \"skipTlsVerification\": false\n  }\n}\n```\n\n**Returns:**\n\n- Markdown, HTML, or other formats as specified.\n\n### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)\n\nScrape multiple URLs efficiently with built-in rate limiting and parallel processing.\n\n**Best for:**\n\n- Retrieving content from multiple pages, when you know exactly which pages to scrape.\n\n**Not recommended for:**\n\n- Discovering URLs (use map first if you don't know the URLs)\n- Scraping a single page (use scrape)\n\n**Common mistakes:**\n\n- Using batch_scrape with too many URLs at once (may hit rate limits or token overflow)\n\n**Prompt Example:**\n\n\u003e \"Get the content of these three blog posts: [url1, url2, url3].\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_batch_scrape\",\n  \"arguments\": {\n    \"urls\": [\"https://example1.com\", \"https://example2.com\"],\n    \"options\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. Check Batch Status (`firecrawl_check_batch_status`)\n\nCheck the status of a batch operation.\n\n```json\n{\n  \"name\": \"firecrawl_check_batch_status\",\n  \"arguments\": {\n    \"id\": \"batch_1\"\n  }\n}\n```\n\n### 4. Map Tool (`firecrawl_map`)\n\nMap a website to discover all indexed URLs on the site.\n\n**Best for:**\n\n- Discovering URLs on a website before deciding what to scrape\n- Finding specific sections of a website\n\n**Not recommended for:**\n\n- When you already know which specific URL you need (use scrape or batch_scrape)\n- When you need the content of the pages (use scrape after mapping)\n\n**Common mistakes:**\n\n- Using crawl to discover URLs instead of map\n\n**Prompt Example:**\n\n\u003e \"List all URLs on example.com.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_map\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n**Returns:**\n\n- Array of URLs found on the site\n\n### 5. Search Tool (`firecrawl_search`)\n\nSearch the web and optionally extract content from search results.\n\n**Best for:**\n\n- Finding specific information across multiple websites, when you don't know which website has the information.\n- When you need the most relevant content for a query\n\n**Not recommended for:**\n\n- When you already know which website to scrape (use scrape)\n- When you need comprehensive coverage of a single website (use map or crawl)\n\n**Common mistakes:**\n\n- Using crawl or map for open-ended questions (use search instead)\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_search\",\n  \"arguments\": {\n    \"query\": \"latest AI research papers 2023\",\n    \"limit\": 5,\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n\n- Array of search results (with optional scraped content)\n\n**Prompt Example:**\n\n\u003e \"Find the latest research papers on AI published in 2023.\"\n\n### 6. Crawl Tool (`firecrawl_crawl`)\n\nStarts an asynchronous crawl job on a website and extract content from all pages.\n\n**Best for:**\n\n- Extracting content from multiple related pages, when you need comprehensive coverage.\n\n**Not recommended for:**\n\n- Extracting content from a single page (use scrape)\n- When token limits are a concern (use map + batch_scrape)\n- When you need fast results (crawling can be slow)\n\n**Warning:** Crawl responses can be very large and may exceed token limits. Limit the crawl depth and number of pages, or use map + batch_scrape for better control.\n\n**Common mistakes:**\n\n- Setting limit or maxDepth too high (causes token overflow)\n- Using crawl for a single page (use scrape instead)\n\n**Prompt Example:**\n\n\u003e \"Get all blog posts from the first two levels of example.com/blog.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_crawl\",\n  \"arguments\": {\n    \"url\": \"https://example.com/blog/*\",\n    \"maxDepth\": 2,\n    \"limit\": 100,\n    \"allowExternalLinks\": false,\n    \"deduplicateSimilarURLs\": true\n  }\n}\n```\n\n**Returns:**\n\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Started crawl for: https://example.com/* with job ID: 550e8400-e29b-41d4-a716-446655440000. Use firecrawl_check_crawl_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Check Crawl Status (`firecrawl_check_crawl_status`)\n\nCheck the status of a crawl job.\n\n```json\n{\n  \"name\": \"firecrawl_check_crawl_status\",\n  \"arguments\": {\n    \"id\": \"550e8400-e29b-41d4-a716-446655440000\"\n  }\n}\n```\n\n**Returns:**\n\n- Response includes the status of the crawl job:\n\n### 8. Extract Tool (`firecrawl_extract`)\n\nExtract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.\n\n**Best for:**\n\n- Extracting specific structured data like prices, names, details.\n\n**Not recommended for:**\n\n- When you need the full content of a page (use scrape)\n- When you're not looking for specific structured data\n\n**Arguments:**\n\n- `urls`: Array of URLs to extract information from\n- `prompt`: Custom prompt for the LLM extraction\n- `systemPrompt`: System prompt to guide the LLM\n- `schema`: JSON schema for structured data extraction\n- `allowExternalLinks`: Allow extraction from external links\n- `enableWebSearch`: Enable web search for additional context\n- `includeSubdomains`: Include subdomains in extraction\n\nWhen using a self-hosted instance, the extraction will use your configured LLM. For cloud API, it uses Firecrawl's managed LLM service.\n**Prompt Example:**\n\n\u003e \"Extract the product name, price, and description from these product pages.\"\n\n**Usage Example:**\n\n```json\n{\n  \"name\": \"firecrawl_extract\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n    \"prompt\": \"Extract product information including name, price, and description\",\n    \"systemPrompt\": \"You are a helpful assistant that extracts product information\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"price\": { \"type\": \"number\" },\n        \"description\": { \"type\": \"string\" }\n      },\n      \"required\": [\"name\", \"price\"]\n    },\n    \"allowExternalLinks\": false,\n    \"enableWebSearch\": false,\n    \"includeSubdomains\": false\n  }\n}\n```\n\n**Returns:**\n\n- Extracted structured data as defined by your schema\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"name\": \"Example Product\",\n        \"price\": 99.99,\n        \"description\": \"This is an example product description\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n- Operation status and progress\n- Performance metrics\n- Credit usage monitoring\n- Rate limit tracking\n- Error conditions\n\nExample log messages:\n\n```\n[INFO] Firecrawl MCP Server initialized successfully\n[INFO] Starting scrape for URL: https://example.com\n[INFO] Batch operation queued with ID: batch_1\n[WARNING] Credit usage has reached warning threshold\n[ERROR] Rate limit exceeded, retrying in 2s...\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Credit usage warnings\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Rate limit exceeded. Retrying in 2 seconds...\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n### Thanks to contributors\n\nThanks to [@vrknetha](https://github.com/vrknetha), [@cawstudios](https://caw.tech) for the initial implementation!\n\nThanks to MCP.so and Klavis AI for hosting and [@gstarwd](https://github.com/gstarwd), [@xiangkaiz](https://github.com/xiangkaiz) and [@zihaolin96](https://github.com/zihaolin96) for integrating our server.\n\n## License\n\nMIT License - see LICENSE file for details\n","stargazer_count":4765,"topics":["batch-processing","claude","content-extraction","data-collection","firecrawl","firecrawl-ai","llm-tools","mcp-server","model-context-protocol","search-api"],"uses_custom_opengraph_image":false}}}},{"name":"coplaydev/unity-mcp","description":"Control the Unity Editor from MCP clients via a Unity bridge + local Python server.","status":"active","repository":{"url":"https://github.com/CoplayDev/unity-mcp","source":"github","id":"950564038","readme":"\u003cimg width=\"676\" height=\"380\" alt=\"MCP for Unity\" src=\"https://github.com/user-attachments/assets/b712e41d-273c-48b2-9041-82bd17ace267\" /\u003e\n\n| [English](README.md) | [简体中文](README-zh.md) |\n|----------------------|---------------------------------|\n\n#### 由 [Coplay](https://www.coplay.dev/?ref=unity-mcp) 荣誉赞助和维护 -- Unity 最好的 AI 助手。[在此阅读背景故事。](https://www.coplay.dev/blog/coplay-and-open-source-unity-mcp-join-forces)\n\n[![Discord](https://img.shields.io/badge/discord-join-red.svg?logo=discord\u0026logoColor=white)](https://discord.gg/y4p8KfzrN4)\n[![](https://img.shields.io/badge/Unity-000000?style=flat\u0026logo=unity\u0026logoColor=blue 'Unity')](https://unity.com/releases/editor/archive)\n[![python](https://img.shields.io/badge/Python-3.12-3776AB.svg?style=flat\u0026logo=python\u0026logoColor=white)](https://www.python.org)\n[![](https://badge.mcpx.dev?status=on 'MCP Enabled')](https://modelcontextprotocol.io/introduction)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/w/CoplayDev/unity-mcp)\n![GitHub Issues or Pull Requests](https://img.shields.io/github/issues/CoplayDev/unity-mcp)\n[![](https://img.shields.io/badge/License-MIT-red.svg 'MIT License')](https://opensource.org/licenses/MIT)\n[![](https://img.shields.io/badge/Sponsor-Coplay-red.svg 'Coplay')](https://www.coplay.dev/?ref=unity-mcp)\n\n**使用大语言模型创建您的 Unity 应用！**\n\nMCP for Unity 作为桥梁，允许 AI 助手（如 Claude、Cursor）通过本地 **MCP（模型上下文协议）客户端** 直接与您的 Unity 编辑器交互。为您的大语言模型提供管理资源、控制场景、编辑脚本和自动化 Unity 任务的工具。\n\n---\n\n### 💬 加入我们的 [Discord](https://discord.gg/y4p8KfzrN4)\n\n**获得帮助、分享想法，与其他 MCP for Unity 开发者协作！**\n\n---\n\n## 主要功能 🚀\n\n* **🗣️ 自然语言操控：** 指示您的大语言模型执行 Unity 任务。\n* **🛠️ 强大工具：** 管理资源、场景、材质、脚本和编辑器功能。\n* **🤖 自动化：** 自动化重复的 Unity 工作流程。\n* **🧩 可扩展：** 设计为与各种 MCP 客户端协作。\n\n\u003cdetails open\u003e\n  \u003csummary\u003e\u003cstrong\u003e 可用工具 \u003c/strong\u003e\u003c/summary\u003e\n\n  您的大语言模型可以使用以下功能：\n\n  * `read_console`: 获取控制台消息或清除控制台。\n  * `manage_script`: 管理 C# 脚本（创建、读取、更新、删除）。\n  * `manage_editor`: 控制和查询编辑器的状态和设置。\n  * `manage_scene`: 管理场景（加载、保存、创建、获取层次结构等）。\n  * `manage_asset`: 执行资源操作（导入、创建、修改、删除等）。\n  * `manage_shader`: 执行着色器 CRUD 操作（创建、读取、修改、删除）。\n  * `manage_gameobject`: 管理游戏对象：创建、修改、删除、查找和组件操作。\n  * `execute_menu_item`: 执行 Unity 编辑器菜单项（例如，执行\"File/Save Project\"）。\n  * `apply_text_edits`: 具有前置条件哈希和原子多编辑批次的精确文本编辑。\n  * `script_apply_edits`: 结构化 C# 方法/类编辑（插入/替换/删除），具有更安全的边界。\n  * `validate_script`: 快速验证（基本/标准）以在写入前后捕获语法/结构问题。\n\u003c/details\u003e\n\n---\n\n## 工作原理\n\nMCP for Unity 使用两个组件连接您的工具：\n\n1. **MCP for Unity Bridge：** 在编辑器内运行的 Unity 包。（通过包管理器安装）。\n2. **MCP for Unity Server：** 本地运行的 Python 服务器，在 Unity Bridge 和您的 MCP 客户端之间进行通信。（首次运行时由包自动安装或通过自动设置；手动设置作为备选方案）。\n\n\u003cimg width=\"562\" height=\"121\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9abf9c66-70d1-4b82-9587-658e0d45dc3e\" /\u003e\n\n---\n\n## 安装 ⚙️\n\n### 前置要求\n\n* **Python：** 版本 3.12 或更新。[下载 Python](https://www.python.org/downloads/)\n* **Unity Hub 和编辑器：** 版本 2021.3 LTS 或更新。[下载 Unity](https://unity.com/download)\n* **uv（Python 工具链管理器）：**\n    ```bash\n    # macOS / Linux\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n\n    # Windows (PowerShell)\n    winget install --id=astral-sh.uv  -e\n\n    # 文档: https://docs.astral.sh/uv/getting-started/installation/\n    ```\n\n* **MCP 客户端：** [Claude Desktop](https://claude.ai/download) | [Claude Code](https://github.com/anthropics/claude-code) | [Cursor](https://www.cursor.com/en/downloads) | [Visual Studio Code Copilot](https://code.visualstudio.com/docs/copilot/overview) | [Windsurf](https://windsurf.com) | 其他客户端可通过手动配置使用\n\n* \u003cdetails\u003e \u003csummary\u003e\u003cstrong\u003e[可选] Roslyn 用于高级脚本验证\u003c/strong\u003e\u003c/summary\u003e\n\n    对于捕获未定义命名空间、类型和方法的**严格**验证级别：\n\n    **方法 1：Unity 的 NuGet（推荐）**\n    1. 安装 [NuGetForUnity](https://github.com/GlitchEnzo/NuGetForUnity)\n    2. 前往 `Window \u003e NuGet Package Manager`\n    3. 搜索 `Microsoft.CodeAnalysis.CSharp`，选择版本 3.11.0 并安装包\n    5. 前往 `Player Settings \u003e Scripting Define Symbols`\n    6. 添加 `USE_ROSLYN`\n    7. 重启 Unity\n\n    **方法 2：手动 DLL 安装**\n    1. 从 [NuGet](https://www.nuget.org/packages/Microsoft.CodeAnalysis.CSharp/) 下载 Microsoft.CodeAnalysis.CSharp.dll 和依赖项\n    2. 将 DLL 放置在 `Assets/Plugins/` 文件夹中\n    3. 确保 .NET 兼容性设置正确\n    4. 将 `USE_ROSLYN` 添加到脚本定义符号\n    5. 重启 Unity\n\n    **注意：** 没有 Roslyn 时，脚本验证会回退到基本结构检查。Roslyn 启用完整的 C# 编译器诊断和精确错误报告。\u003c/details\u003e\n\n---\n### 🌟 步骤 1：安装 Unity 包\n\n#### 通过 Git URL 安装\n\n1. 打开您的 Unity 项目。\n2. 前往 `Window \u003e Package Manager`。\n3. 点击 `+` -\u003e `Add package from git URL...`。\n4. 输入：\n    ```\n    https://github.com/CoplayDev/unity-mcp.git?path=/MCPForUnity\n    ```\n5. 点击 `Add`。\n6. MCP 服务器在首次运行时或通过自动设置由包自动安装。如果失败，请使用手动配置（如下）。\n\n#### 通过 OpenUPM 安装\n\n1. 安装 [OpenUPM CLI](https://openupm.com/docs/getting-started-cli.html)\n2. 打开终端（PowerShell、Terminal 等）并导航到您的 Unity 项目目录\n3. 运行 `openupm add com.coplaydev.unity-mcp`\n\n**注意：** 如果您在 Coplay 维护之前安装了 MCP 服务器，您需要在重新安装新版本之前卸载旧包。\n\n### 🛠️ 步骤 2：配置您的 MCP 客户端\n将您的 MCP 客户端（Claude、Cursor 等）连接到步骤 1（自动）设置的 Python 服务器或通过手动配置（如下）。\n\n\u003cimg width=\"648\" height=\"599\" alt=\"MCPForUnity-Readme-Image\" src=\"https://github.com/user-attachments/assets/b4a725da-5c43-4bd6-80d6-ee2e3cca9596\" /\u003e\n\n**选项 A：自动设置（推荐用于 Claude/Cursor/VSC Copilot）**\n\n1. 在 Unity 中，前往 `Window \u003e MCP for Unity`。\n2. 点击 `Auto-Setup`。\n3. 寻找绿色状态指示器 🟢 和\"Connected ✓\"。*（这会尝试自动修改 MCP 客户端的配置文件）。*\n\n\u003cdetails\u003e\u003csummary\u003e\u003cstrong\u003e客户端特定故障排除\u003c/strong\u003e\u003c/summary\u003e\n\n- **VSCode**：使用 `Code/User/mcp.json` 和顶级 `servers.unityMCP` 以及 `\"type\": \"stdio\"`。在 Windows 上，MCP for Unity 写入绝对路径 `uv.exe`（优先选择 WinGet Links shim）以避免 PATH 问题。\n- **Cursor / Windsurf** [(**帮助链接**)](https://github.com/CoplayDev/unity-mcp/wiki/1.-Fix-Unity-MCP-and-Cursor,-VSCode-\u0026-Windsurf)：如果缺少 `uv`，MCP for Unity 窗口会显示\"uv Not Found\"和快速 [HELP] 链接以及\"Choose `uv` Install Location\"按钮。\n- **Claude Code** [(**帮助链接**)](https://github.com/CoplayDev/unity-mcp/wiki/2.-Fix-Unity-MCP-and-Claude-Code)：如果找不到 `claude`，窗口会显示\"Claude Not Found\"和 [HELP] 以及\"Choose Claude Location\"按钮。注销现在会立即更新 UI。\u003c/details\u003e\n\n**选项 B：手动配置**\n\n如果自动设置失败或您使用不同的客户端：\n\n1. **找到您的 MCP 客户端配置文件。**（查看客户端文档）。\n    * *Claude 示例（macOS）：* `~/Library/Application Support/Claude/claude_desktop_config.json`\n    * *Claude 示例（Windows）：* `%APPDATA%\\Claude\\claude_desktop_config.json`\n2. **编辑文件** 以添加/更新 `mcpServers` 部分，使用步骤 1 中的*精确*路径。\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003e点击查看客户端特定的 JSON 配置片段...\u003c/strong\u003e\u003c/summary\u003e\n\n---\n**Claude Code**\n\n如果您正在使用 Claude Code，您可以使用以下命令注册 MCP 服务器：\n🚨**确保从您的 Unity 项目主目录运行这些命令**🚨\n\n**macOS：**\n\n```bash\nclaude mcp add UnityMCP -- uv --directory /Users/USERNAME/Library/AppSupport/UnityMCP/UnityMcpServer/src run server.py\n```\n\n**Windows：**\n\n```bash\nclaude mcp add UnityMCP -- \"C:/Users/USERNAME/AppData/Local/Microsoft/WinGet/Links/uv.exe\" --directory \"C:/Users/USERNAME/AppData/Local/UnityMCP/UnityMcpServer/src\" run server.py\n```\n**VSCode（所有操作系统）**\n\n```json\n{\n  \"servers\": {\n    \"unityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\",\"\u003cABSOLUTE_PATH_TO\u003e/UnityMcpServer/src\",\"run\",\"server.py\"],\n      \"type\": \"stdio\"\n    }\n  }\n}\n```\n\n在 Windows 上，将 `command` 设置为绝对 shim，例如 `C:\\\\Users\\\\YOU\\\\AppData\\\\Local\\\\Microsoft\\\\WinGet\\\\Links\\\\uv.exe`。\n\n**Windows：**\n\n```json\n{\n  \"mcpServers\": {\n    \"UnityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\UnityMCP\\\\UnityMcpServer\\\\src\",\n        \"server.py\"\n      ]\n    }\n    // ... 其他服务器可能在这里 ...\n  }\n}\n```\n\n（记得替换 YOUR_USERNAME 并使用双反斜杠 \\\\）\n\n**macOS：**\n\n```json\n{\n  \"mcpServers\": {\n    \"UnityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"/Users/YOUR_USERNAME/Library/AppSupport/UnityMCP/UnityMcpServer/src\",\n        \"server.py\"\n      ]\n    }\n    // ... 其他服务器可能在这里 ...\n  }\n}\n```\n\n（替换 YOUR_USERNAME。注意：AppSupport 是\"Application Support\"的符号链接，以避免引号问题）\n\n**Linux：**\n\n```json\n{\n  \"mcpServers\": {\n    \"UnityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"/home/YOUR_USERNAME/.local/share/UnityMCP/UnityMcpServer/src\",\n        \"server.py\"\n      ]\n    }\n    // ... 其他服务器可能在这里 ...\n  }\n}\n```\n\n（替换 YOUR_USERNAME）\n\n\u003c/details\u003e\n\n---\n\n## 使用方法 ▶️\n\n1. **打开您的 Unity 项目。** MCP for Unity 包应该自动连接。通过 Window \u003e MCP for Unity 检查状态。\n\n2. **启动您的 MCP 客户端**（Claude、Cursor 等）。它应该使用安装步骤 2 中的配置自动启动 MCP for Unity 服务器（Python）。\n\n3. **交互！** Unity 工具现在应该在您的 MCP 客户端中可用。\n\n    示例提示：`创建一个 3D 玩家控制器`，`创建一个 3D 井字游戏`，`创建一个酷炫的着色器并应用到立方体上`。\n\n---\n\n## 开发和贡献 🛠️\n\n### 添加自定义工具\n\nMCP for Unity 使用与 Unity 的 C# 脚本绑定的 Python MCP 服务器来实现工具功能。如果您想使用自己的工具扩展功能，请参阅 **[CUSTOM_TOOLS.md](docs/CUSTOM_TOOLS.md)** 了解如何操作。\n\n### 贡献项目\n\n如果您正在为 MCP for Unity 做贡献或想要测试核心更改，我们有开发工具来简化您的工作流程：\n\n- **开发部署脚本**：快速部署和测试您对 MCP for Unity Bridge 和 Python 服务器的更改\n- **自动备份系统**：具有简单回滚功能的安全测试\n- **热重载工作流程**：核心开发的快速迭代周期\n\n📖 **查看 [README-DEV.md](docs/README-DEV.md)** 获取完整的开发设置和工作流程文档。\n\n### 贡献 🤝\n\n帮助改进 MCP for Unity！\n\n1. **Fork** 主仓库。\n2. **创建分支**（`feature/your-idea` 或 `bugfix/your-fix`）。\n3. **进行更改。**\n4. **提交**（feat: Add cool new feature）。\n5. **推送** 您的分支。\n6. **对主分支开启拉取请求**。\n\n---\n\n## 📊 遥测和隐私\n\nMCP for Unity 包含**注重隐私的匿名遥测**来帮助我们改进产品。我们收集使用分析和性能数据，但**绝不**收集您的代码、项目名称或个人信息。\n\n- **🔒 匿名**：仅随机 UUID，无个人数据\n- **🚫 轻松退出**：设置 `DISABLE_TELEMETRY=true` 环境变量\n- **📖 透明**：查看 [TELEMETRY.md](docs/TELEMETRY.md) 获取完整详情\n\n您的隐私对我们很重要。所有遥测都是可选的，旨在尊重您的工作流程。\n\n---\n\n## 故障排除 ❓\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003e点击查看常见问题和修复方法...\u003c/strong\u003e\u003c/summary\u003e\n\n- **Unity Bridge 未运行/连接：**\n    - 确保 Unity 编辑器已打开。\n    - 检查状态窗口：Window \u003e MCP for Unity。\n    - 重启 Unity。\n- **MCP 客户端未连接/服务器未启动：**\n    - **验证服务器路径：** 双重检查您的 MCP 客户端 JSON 配置中的 --directory 路径。它必须完全匹配安装位置：\n      - **Windows：** `%USERPROFILE%\\AppData\\Local\\UnityMCP\\UnityMcpServer\\src`\n      - **macOS：** `~/Library/AppSupport/UnityMCP/UnityMcpServer\\src`\n      - **Linux：** `~/.local/share/UnityMCP/UnityMcpServer\\src`\n    - **验证 uv：** 确保 `uv` 已安装并正常工作（`uv --version`）。\n    - **手动运行：** 尝试直接从终端运行服务器以查看错误：\n      ```bash\n      cd /path/to/your/UnityMCP/UnityMcpServer/src\n      uv run server.py\n      ```\n- **自动配置失败：**\n    - 使用手动配置步骤。自动配置可能缺乏写入 MCP 客户端配置文件的权限。\n\n\u003c/details\u003e\n\n仍然卡住？[开启问题](https://github.com/CoplayDev/unity-mcp/issues) 或 [加入 Discord](https://discord.gg/y4p8KfzrN4)！\n\n---\n\n## 许可证 📜\n\nMIT 许可证。查看 [LICENSE](LICENSE) 文件。\n\n---\n\n## Star历史\n\n[![Star History Chart](https://api.star-history.com/svg?repos=CoplayDev/unity-mcp\u0026type=Date)](https://www.star-history.com/#CoplayDev/unity-mcp\u0026Date)\n\n## 赞助\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://www.coplay.dev/?ref=unity-mcp\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\n    \u003cimg src=\"logo.png\" alt=\"Coplay Logo\" width=\"100%\"\u003e\n  \u003c/a\u003e\n\u003c/p\u003e"},"version":"1.0.0","created_at":"2025-09-23T17:40:53Z","updated_at":"2025-10-22T11:21:09Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"uv","identifier":"uv","version":"v4.0.0","runtime_hint":"uv","runtime_arguments":[{"is_required":true,"format":"string","value":"{unity_mcp_server_src}","variables":{"unity_mcp_server_src":{"description":"Absolute path to the Unity MCP Server 'src' folder installed by the Unity package (e.g., macOS: ~/Library/Application Support/UnityMCP/UnityMcpServer/src; Windows: %USERPROFILE%/AppData/Local/UnityMCP/UnityMcpServer/src; Linux: ~/.local/share/UnityMCP/UnityMcpServer/src).","is_required":true}},"type":"named","name":"--directory","value_hint":"directory_flag"},{"is_required":true,"format":"string","value":"run","type":"positional","name":"run","value_hint":"run"},{"is_required":true,"format":"string","value":"server.py","type":"positional","name":"server.py","value_hint":"entrypoint"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"776e74ed-3a5a-4642-8710-13ee91ed1c10","is_latest":true,"published_at":"2025-09-09T10:55:22.907116Z","updated_at":"2025-09-09T10:55:22.907116Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Unity","homepage_url":"https://www.coplay.dev","is_in_organization":true,"license":"MIT License","name":"unity-mcp","name_with_owner":"CoplayDev/unity-mcp","opengraph_image_url":"https://repository-images.githubusercontent.com/950564038/29569e93-0949-4b10-87c5-c530ba068a26","owner_avatar_url":"https://avatars.githubusercontent.com/u/188132522?v=4","preferred_image":"https://avatars.githubusercontent.com/u/188132522?v=4","primary_language":"C#","primary_language_color":"#178600","pushed_at":"2025-10-21T23:08:29Z","readme":"\u003cimg width=\"676\" height=\"380\" alt=\"MCP for Unity\" src=\"https://github.com/user-attachments/assets/b712e41d-273c-48b2-9041-82bd17ace267\" /\u003e\n\n| [English](README.md) | [简体中文](README-zh.md) |\n|----------------------|---------------------------------|\n\n#### 由 [Coplay](https://www.coplay.dev/?ref=unity-mcp) 荣誉赞助和维护 -- Unity 最好的 AI 助手。[在此阅读背景故事。](https://www.coplay.dev/blog/coplay-and-open-source-unity-mcp-join-forces)\n\n[![Discord](https://img.shields.io/badge/discord-join-red.svg?logo=discord\u0026logoColor=white)](https://discord.gg/y4p8KfzrN4)\n[![](https://img.shields.io/badge/Unity-000000?style=flat\u0026logo=unity\u0026logoColor=blue 'Unity')](https://unity.com/releases/editor/archive)\n[![python](https://img.shields.io/badge/Python-3.12-3776AB.svg?style=flat\u0026logo=python\u0026logoColor=white)](https://www.python.org)\n[![](https://badge.mcpx.dev?status=on 'MCP Enabled')](https://modelcontextprotocol.io/introduction)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/w/CoplayDev/unity-mcp)\n![GitHub Issues or Pull Requests](https://img.shields.io/github/issues/CoplayDev/unity-mcp)\n[![](https://img.shields.io/badge/License-MIT-red.svg 'MIT License')](https://opensource.org/licenses/MIT)\n[![](https://img.shields.io/badge/Sponsor-Coplay-red.svg 'Coplay')](https://www.coplay.dev/?ref=unity-mcp)\n\n**使用大语言模型创建您的 Unity 应用！**\n\nMCP for Unity 作为桥梁，允许 AI 助手（如 Claude、Cursor）通过本地 **MCP（模型上下文协议）客户端** 直接与您的 Unity 编辑器交互。为您的大语言模型提供管理资源、控制场景、编辑脚本和自动化 Unity 任务的工具。\n\n---\n\n### 💬 加入我们的 [Discord](https://discord.gg/y4p8KfzrN4)\n\n**获得帮助、分享想法，与其他 MCP for Unity 开发者协作！**\n\n---\n\n## 主要功能 🚀\n\n* **🗣️ 自然语言操控：** 指示您的大语言模型执行 Unity 任务。\n* **🛠️ 强大工具：** 管理资源、场景、材质、脚本和编辑器功能。\n* **🤖 自动化：** 自动化重复的 Unity 工作流程。\n* **🧩 可扩展：** 设计为与各种 MCP 客户端协作。\n\n\u003cdetails open\u003e\n  \u003csummary\u003e\u003cstrong\u003e 可用工具 \u003c/strong\u003e\u003c/summary\u003e\n\n  您的大语言模型可以使用以下功能：\n\n  * `read_console`: 获取控制台消息或清除控制台。\n  * `manage_script`: 管理 C# 脚本（创建、读取、更新、删除）。\n  * `manage_editor`: 控制和查询编辑器的状态和设置。\n  * `manage_scene`: 管理场景（加载、保存、创建、获取层次结构等）。\n  * `manage_asset`: 执行资源操作（导入、创建、修改、删除等）。\n  * `manage_shader`: 执行着色器 CRUD 操作（创建、读取、修改、删除）。\n  * `manage_gameobject`: 管理游戏对象：创建、修改、删除、查找和组件操作。\n  * `execute_menu_item`: 执行 Unity 编辑器菜单项（例如，执行\"File/Save Project\"）。\n  * `apply_text_edits`: 具有前置条件哈希和原子多编辑批次的精确文本编辑。\n  * `script_apply_edits`: 结构化 C# 方法/类编辑（插入/替换/删除），具有更安全的边界。\n  * `validate_script`: 快速验证（基本/标准）以在写入前后捕获语法/结构问题。\n\u003c/details\u003e\n\n---\n\n## 工作原理\n\nMCP for Unity 使用两个组件连接您的工具：\n\n1. **MCP for Unity Bridge：** 在编辑器内运行的 Unity 包。（通过包管理器安装）。\n2. **MCP for Unity Server：** 本地运行的 Python 服务器，在 Unity Bridge 和您的 MCP 客户端之间进行通信。（首次运行时由包自动安装或通过自动设置；手动设置作为备选方案）。\n\n\u003cimg width=\"562\" height=\"121\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9abf9c66-70d1-4b82-9587-658e0d45dc3e\" /\u003e\n\n---\n\n## 安装 ⚙️\n\n### 前置要求\n\n* **Python：** 版本 3.12 或更新。[下载 Python](https://www.python.org/downloads/)\n* **Unity Hub 和编辑器：** 版本 2021.3 LTS 或更新。[下载 Unity](https://unity.com/download)\n* **uv（Python 工具链管理器）：**\n    ```bash\n    # macOS / Linux\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n\n    # Windows (PowerShell)\n    winget install --id=astral-sh.uv  -e\n\n    # 文档: https://docs.astral.sh/uv/getting-started/installation/\n    ```\n\n* **MCP 客户端：** [Claude Desktop](https://claude.ai/download) | [Claude Code](https://github.com/anthropics/claude-code) | [Cursor](https://www.cursor.com/en/downloads) | [Visual Studio Code Copilot](https://code.visualstudio.com/docs/copilot/overview) | [Windsurf](https://windsurf.com) | 其他客户端可通过手动配置使用\n\n* \u003cdetails\u003e \u003csummary\u003e\u003cstrong\u003e[可选] Roslyn 用于高级脚本验证\u003c/strong\u003e\u003c/summary\u003e\n\n    对于捕获未定义命名空间、类型和方法的**严格**验证级别：\n\n    **方法 1：Unity 的 NuGet（推荐）**\n    1. 安装 [NuGetForUnity](https://github.com/GlitchEnzo/NuGetForUnity)\n    2. 前往 `Window \u003e NuGet Package Manager`\n    3. 搜索 `Microsoft.CodeAnalysis.CSharp`，选择版本 3.11.0 并安装包\n    5. 前往 `Player Settings \u003e Scripting Define Symbols`\n    6. 添加 `USE_ROSLYN`\n    7. 重启 Unity\n\n    **方法 2：手动 DLL 安装**\n    1. 从 [NuGet](https://www.nuget.org/packages/Microsoft.CodeAnalysis.CSharp/) 下载 Microsoft.CodeAnalysis.CSharp.dll 和依赖项\n    2. 将 DLL 放置在 `Assets/Plugins/` 文件夹中\n    3. 确保 .NET 兼容性设置正确\n    4. 将 `USE_ROSLYN` 添加到脚本定义符号\n    5. 重启 Unity\n\n    **注意：** 没有 Roslyn 时，脚本验证会回退到基本结构检查。Roslyn 启用完整的 C# 编译器诊断和精确错误报告。\u003c/details\u003e\n\n---\n### 🌟 步骤 1：安装 Unity 包\n\n#### 通过 Git URL 安装\n\n1. 打开您的 Unity 项目。\n2. 前往 `Window \u003e Package Manager`。\n3. 点击 `+` -\u003e `Add package from git URL...`。\n4. 输入：\n    ```\n    https://github.com/CoplayDev/unity-mcp.git?path=/MCPForUnity\n    ```\n5. 点击 `Add`。\n6. MCP 服务器在首次运行时或通过自动设置由包自动安装。如果失败，请使用手动配置（如下）。\n\n#### 通过 OpenUPM 安装\n\n1. 安装 [OpenUPM CLI](https://openupm.com/docs/getting-started-cli.html)\n2. 打开终端（PowerShell、Terminal 等）并导航到您的 Unity 项目目录\n3. 运行 `openupm add com.coplaydev.unity-mcp`\n\n**注意：** 如果您在 Coplay 维护之前安装了 MCP 服务器，您需要在重新安装新版本之前卸载旧包。\n\n### 🛠️ 步骤 2：配置您的 MCP 客户端\n将您的 MCP 客户端（Claude、Cursor 等）连接到步骤 1（自动）设置的 Python 服务器或通过手动配置（如下）。\n\n\u003cimg width=\"648\" height=\"599\" alt=\"MCPForUnity-Readme-Image\" src=\"https://github.com/user-attachments/assets/b4a725da-5c43-4bd6-80d6-ee2e3cca9596\" /\u003e\n\n**选项 A：自动设置（推荐用于 Claude/Cursor/VSC Copilot）**\n\n1. 在 Unity 中，前往 `Window \u003e MCP for Unity`。\n2. 点击 `Auto-Setup`。\n3. 寻找绿色状态指示器 🟢 和\"Connected ✓\"。*（这会尝试自动修改 MCP 客户端的配置文件）。*\n\n\u003cdetails\u003e\u003csummary\u003e\u003cstrong\u003e客户端特定故障排除\u003c/strong\u003e\u003c/summary\u003e\n\n- **VSCode**：使用 `Code/User/mcp.json` 和顶级 `servers.unityMCP` 以及 `\"type\": \"stdio\"`。在 Windows 上，MCP for Unity 写入绝对路径 `uv.exe`（优先选择 WinGet Links shim）以避免 PATH 问题。\n- **Cursor / Windsurf** [(**帮助链接**)](https://github.com/CoplayDev/unity-mcp/wiki/1.-Fix-Unity-MCP-and-Cursor,-VSCode-\u0026-Windsurf)：如果缺少 `uv`，MCP for Unity 窗口会显示\"uv Not Found\"和快速 [HELP] 链接以及\"Choose `uv` Install Location\"按钮。\n- **Claude Code** [(**帮助链接**)](https://github.com/CoplayDev/unity-mcp/wiki/2.-Fix-Unity-MCP-and-Claude-Code)：如果找不到 `claude`，窗口会显示\"Claude Not Found\"和 [HELP] 以及\"Choose Claude Location\"按钮。注销现在会立即更新 UI。\u003c/details\u003e\n\n**选项 B：手动配置**\n\n如果自动设置失败或您使用不同的客户端：\n\n1. **找到您的 MCP 客户端配置文件。**（查看客户端文档）。\n    * *Claude 示例（macOS）：* `~/Library/Application Support/Claude/claude_desktop_config.json`\n    * *Claude 示例（Windows）：* `%APPDATA%\\Claude\\claude_desktop_config.json`\n2. **编辑文件** 以添加/更新 `mcpServers` 部分，使用步骤 1 中的*精确*路径。\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003e点击查看客户端特定的 JSON 配置片段...\u003c/strong\u003e\u003c/summary\u003e\n\n---\n**Claude Code**\n\n如果您正在使用 Claude Code，您可以使用以下命令注册 MCP 服务器：\n🚨**确保从您的 Unity 项目主目录运行这些命令**🚨\n\n**macOS：**\n\n```bash\nclaude mcp add UnityMCP -- uv --directory /Users/USERNAME/Library/AppSupport/UnityMCP/UnityMcpServer/src run server.py\n```\n\n**Windows：**\n\n```bash\nclaude mcp add UnityMCP -- \"C:/Users/USERNAME/AppData/Local/Microsoft/WinGet/Links/uv.exe\" --directory \"C:/Users/USERNAME/AppData/Local/UnityMCP/UnityMcpServer/src\" run server.py\n```\n**VSCode（所有操作系统）**\n\n```json\n{\n  \"servers\": {\n    \"unityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\"--directory\",\"\u003cABSOLUTE_PATH_TO\u003e/UnityMcpServer/src\",\"run\",\"server.py\"],\n      \"type\": \"stdio\"\n    }\n  }\n}\n```\n\n在 Windows 上，将 `command` 设置为绝对 shim，例如 `C:\\\\Users\\\\YOU\\\\AppData\\\\Local\\\\Microsoft\\\\WinGet\\\\Links\\\\uv.exe`。\n\n**Windows：**\n\n```json\n{\n  \"mcpServers\": {\n    \"UnityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"C:\\\\Users\\\\YOUR_USERNAME\\\\AppData\\\\Local\\\\UnityMCP\\\\UnityMcpServer\\\\src\",\n        \"server.py\"\n      ]\n    }\n    // ... 其他服务器可能在这里 ...\n  }\n}\n```\n\n（记得替换 YOUR_USERNAME 并使用双反斜杠 \\\\）\n\n**macOS：**\n\n```json\n{\n  \"mcpServers\": {\n    \"UnityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"/Users/YOUR_USERNAME/Library/AppSupport/UnityMCP/UnityMcpServer/src\",\n        \"server.py\"\n      ]\n    }\n    // ... 其他服务器可能在这里 ...\n  }\n}\n```\n\n（替换 YOUR_USERNAME。注意：AppSupport 是\"Application Support\"的符号链接，以避免引号问题）\n\n**Linux：**\n\n```json\n{\n  \"mcpServers\": {\n    \"UnityMCP\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"/home/YOUR_USERNAME/.local/share/UnityMCP/UnityMcpServer/src\",\n        \"server.py\"\n      ]\n    }\n    // ... 其他服务器可能在这里 ...\n  }\n}\n```\n\n（替换 YOUR_USERNAME）\n\n\u003c/details\u003e\n\n---\n\n## 使用方法 ▶️\n\n1. **打开您的 Unity 项目。** MCP for Unity 包应该自动连接。通过 Window \u003e MCP for Unity 检查状态。\n\n2. **启动您的 MCP 客户端**（Claude、Cursor 等）。它应该使用安装步骤 2 中的配置自动启动 MCP for Unity 服务器（Python）。\n\n3. **交互！** Unity 工具现在应该在您的 MCP 客户端中可用。\n\n    示例提示：`创建一个 3D 玩家控制器`，`创建一个 3D 井字游戏`，`创建一个酷炫的着色器并应用到立方体上`。\n\n---\n\n## 开发和贡献 🛠️\n\n### 添加自定义工具\n\nMCP for Unity 使用与 Unity 的 C# 脚本绑定的 Python MCP 服务器来实现工具功能。如果您想使用自己的工具扩展功能，请参阅 **[CUSTOM_TOOLS.md](docs/CUSTOM_TOOLS.md)** 了解如何操作。\n\n### 贡献项目\n\n如果您正在为 MCP for Unity 做贡献或想要测试核心更改，我们有开发工具来简化您的工作流程：\n\n- **开发部署脚本**：快速部署和测试您对 MCP for Unity Bridge 和 Python 服务器的更改\n- **自动备份系统**：具有简单回滚功能的安全测试\n- **热重载工作流程**：核心开发的快速迭代周期\n\n📖 **查看 [README-DEV.md](docs/README-DEV.md)** 获取完整的开发设置和工作流程文档。\n\n### 贡献 🤝\n\n帮助改进 MCP for Unity！\n\n1. **Fork** 主仓库。\n2. **创建分支**（`feature/your-idea` 或 `bugfix/your-fix`）。\n3. **进行更改。**\n4. **提交**（feat: Add cool new feature）。\n5. **推送** 您的分支。\n6. **对主分支开启拉取请求**。\n\n---\n\n## 📊 遥测和隐私\n\nMCP for Unity 包含**注重隐私的匿名遥测**来帮助我们改进产品。我们收集使用分析和性能数据，但**绝不**收集您的代码、项目名称或个人信息。\n\n- **🔒 匿名**：仅随机 UUID，无个人数据\n- **🚫 轻松退出**：设置 `DISABLE_TELEMETRY=true` 环境变量\n- **📖 透明**：查看 [TELEMETRY.md](docs/TELEMETRY.md) 获取完整详情\n\n您的隐私对我们很重要。所有遥测都是可选的，旨在尊重您的工作流程。\n\n---\n\n## 故障排除 ❓\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cstrong\u003e点击查看常见问题和修复方法...\u003c/strong\u003e\u003c/summary\u003e\n\n- **Unity Bridge 未运行/连接：**\n    - 确保 Unity 编辑器已打开。\n    - 检查状态窗口：Window \u003e MCP for Unity。\n    - 重启 Unity。\n- **MCP 客户端未连接/服务器未启动：**\n    - **验证服务器路径：** 双重检查您的 MCP 客户端 JSON 配置中的 --directory 路径。它必须完全匹配安装位置：\n      - **Windows：** `%USERPROFILE%\\AppData\\Local\\UnityMCP\\UnityMcpServer\\src`\n      - **macOS：** `~/Library/AppSupport/UnityMCP/UnityMcpServer\\src`\n      - **Linux：** `~/.local/share/UnityMCP/UnityMcpServer\\src`\n    - **验证 uv：** 确保 `uv` 已安装并正常工作（`uv --version`）。\n    - **手动运行：** 尝试直接从终端运行服务器以查看错误：\n      ```bash\n      cd /path/to/your/UnityMCP/UnityMcpServer/src\n      uv run server.py\n      ```\n- **自动配置失败：**\n    - 使用手动配置步骤。自动配置可能缺乏写入 MCP 客户端配置文件的权限。\n\n\u003c/details\u003e\n\n仍然卡住？[开启问题](https://github.com/CoplayDev/unity-mcp/issues) 或 [加入 Discord](https://discord.gg/y4p8KfzrN4)！\n\n---\n\n## 许可证 📜\n\nMIT 许可证。查看 [LICENSE](LICENSE) 文件。\n\n---\n\n## Star历史\n\n[![Star History Chart](https://api.star-history.com/svg?repos=CoplayDev/unity-mcp\u0026type=Date)](https://www.star-history.com/#CoplayDev/unity-mcp\u0026Date)\n\n## 赞助\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://www.coplay.dev/?ref=unity-mcp\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\n    \u003cimg src=\"logo.png\" alt=\"Coplay Logo\" width=\"100%\"\u003e\n  \u003c/a\u003e\n\u003c/p\u003e","stargazer_count":3582,"topics":["ai","ai-integration","mcp","unity","anthropic","claude","copilot","cursor","deepseek","game-development"],"uses_custom_opengraph_image":true}}}},{"name":"makenotion/notion-mcp-server","description":"Official MCP server for Notion API","status":"active","repository":{"url":"https://github.com/makenotion/notion-mcp-server","source":"github","id":"946169991","readme":"# Notion MCP Server\n\n\u003e [!NOTE] \n\u003e \n\u003e We’ve introduced **Notion MCP**, a remote MCP server with the following improvements:\n\u003e - Easy installation via standard OAuth. No need to fiddle with JSON or API token anymore.\n\u003e - Powerful tools tailored to AI agents. These tools are designed with optimized token consumption in mind.\n\u003e \n\u003e Learn more and try it out [here](https://developers.notion.com/docs/mcp)\n\n\n![notion-mcp-sm](https://github.com/user-attachments/assets/6c07003c-8455-4636-b298-d60ffdf46cd8)\n\nThis project implements an [MCP server](https://spec.modelcontextprotocol.io/) for the [Notion API](https://developers.notion.com/reference/intro). \n\n![mcp-demo](https://github.com/user-attachments/assets/e3ff90a7-7801-48a9-b807-f7dd47f0d3d6)\n\n### Installation\n\n#### 1. Setting up Integration in Notion:\nGo to [https://www.notion.so/profile/integrations](https://www.notion.so/profile/integrations) and create a new **internal** integration or select an existing one.\n\n![Creating a Notion Integration token](docs/images/integrations-creation.png)\n\nWhile we limit the scope of Notion API's exposed (for example, you will not be able to delete databases via MCP), there is a non-zero risk to workspace data by exposing it to LLMs. Security-conscious users may want to further configure the Integration's _Capabilities_. \n\nFor example, you can create a read-only integration token by giving only \"Read content\" access from the \"Configuration\" tab:\n\n![Notion Integration Token Capabilities showing Read content checked](docs/images/integrations-capabilities.png)\n\n#### 2. Connecting content to integration:\nEnsure relevant pages and databases are connected to your integration.\n\nTo do this, visit the **Access** tab in your internal integration settings. Edit access and select the pages you'd like to use.\n![Integration Access tab](docs/images/integration-access.png)\n\n![Edit integration access](docs/images/page-access-edit.png)\n\nAlternatively, you can grant page access individually. You'll need to visit the target page, and click on the 3 dots, and select \"Connect to integration\". \n\n![Adding Integration Token to Notion Connections](docs/images/connections.png)\n\n#### 3. Adding MCP config to your client:\n\n##### Using npm:\n\n**Cursor \u0026 Claude:**\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json` (MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`)\n\n**Option 1: Using NOTION_TOKEN (recommended)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Option 2: Using OPENAPI_MCP_HEADERS (for advanced use cases)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    }\n  }\n}\n```\n\n**Zed**\n\nAdd the following to your `settings.json`\n\n```json\n{\n  \"context_servers\": {\n    \"some-context-server\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n        \"env\": {\n          \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n        }\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n##### Using Docker:\n\nThere are two options for running the MCP server with Docker:\n\n###### Option 1: Using the official Docker Hub image:\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"NOTION_TOKEN\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"OPENAPI_MCP_HEADERS\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\":\\\"Bearer ntn_****\\\",\\\"Notion-Version\\\":\\\"2022-06-28\\\"}\"\n      }\n    }\n  }\n}\n```\n\nThis approach:\n- Uses the official Docker Hub image\n- Properly handles JSON escaping via environment variables\n- Provides a more reliable configuration method\n\n###### Option 2: Building the Docker image locally:\n\nYou can also build and run the Docker image locally. First, build the Docker image:\n\n```bash\ndocker compose build\n```\n\nThen, add the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"NOTION_TOKEN=ntn_****\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"OPENAPI_MCP_HEADERS={\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\"}\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\nDon't forget to replace `ntn_****` with your integration secret. Find it from your integration configuration tab:\n\n![Copying your Integration token from the Configuration tab in the developer portal](https://github.com/user-attachments/assets/67b44536-5333-49fa-809c-59581bf5370a)\n\n\n#### Installing via Smithery\n\n[![smithery badge](https://smithery.ai/badge/@makenotion/notion-mcp-server)](https://smithery.ai/server/@makenotion/notion-mcp-server)\n\nTo install Notion API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@makenotion/notion-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @makenotion/notion-mcp-server --client claude\n```\n\n### Transport Options\n\nThe Notion MCP Server supports two transport modes:\n\n#### STDIO Transport (Default)\nThe default transport mode uses standard input/output for communication. This is the standard MCP transport used by most clients like Claude Desktop.\n\n```bash\n# Run with default stdio transport\nnpx @notionhq/notion-mcp-server\n\n# Or explicitly specify stdio\nnpx @notionhq/notion-mcp-server --transport stdio\n```\n\n#### Streamable HTTP Transport\nFor web-based applications or clients that prefer HTTP communication, you can use the Streamable HTTP transport:\n\n```bash\n# Run with Streamable HTTP transport on port 3000 (default)\nnpx @notionhq/notion-mcp-server --transport http\n\n# Run on a custom port\nnpx @notionhq/notion-mcp-server --transport http --port 8080\n\n# Run with a custom authentication token\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\nWhen using Streamable HTTP transport, the server will be available at `http://0.0.0.0:\u003cport\u003e/mcp`.\n\n##### Authentication\nThe Streamable HTTP transport requires bearer token authentication for security. You have three options:\n\n**Option 1: Auto-generated token (recommended for development)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http\n```\nThe server will generate a secure random token and display it in the console:\n```\nGenerated auth token: a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\nUse this token in the Authorization header: Bearer a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\n```\n\n**Option 2: Custom token via command line (recommended for production)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\n**Option 3: Custom token via environment variable (recommended for production)**\n```bash\nAUTH_TOKEN=\"your-secret-token\" npx @notionhq/notion-mcp-server --transport http\n```\n\nThe command line argument `--auth-token` takes precedence over the `AUTH_TOKEN` environment variable if both are provided.\n\n##### Making HTTP Requests\nAll requests to the Streamable HTTP transport must include the bearer token in the Authorization header:\n\n```bash\n# Example request\ncurl -H \"Authorization: Bearer your-token-here\" \\\n     -H \"Content-Type: application/json\" \\\n     -H \"mcp-session-id: your-session-id\" \\\n     -d '{\"jsonrpc\": \"2.0\", \"method\": \"initialize\", \"params\": {}, \"id\": 1}' \\\n     http://localhost:3000/mcp\n```\n\n**Note:** Make sure to set either the `NOTION_TOKEN` environment variable (recommended) or the `OPENAPI_MCP_HEADERS` environment variable with your Notion integration token when using either transport mode.\n\n### Examples\n\n1. Using the following instruction\n```\nComment \"Hello MCP\" on page \"Getting started\"\n```\n\nAI will correctly plan two API calls, `v1/search` and `v1/comments`, to achieve the task\n\n2. Similarly, the following instruction will result in a new page named \"Notion MCP\" added to parent page \"Development\"\n```\nAdd a page titled \"Notion MCP\" to page \"Development\"\n```\n\n3. You may also reference content ID directly\n```\nGet the content of page 1a6b35e6e67f802fa7e1d27686f017f2\n```\n\n### Development\n\nBuild\n\n```\nnpm run build\n```\n\nExecute\n\n```\nnpx -y --prefix /path/to/local/notion-mcp-server @notionhq/notion-mcp-server\n```\n\nPublish\n\n```\nnpm publish --access public\n```\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:24Z","updated_at":"2025-10-22T11:20:41Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.notion.com/sse"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"2a045418-16b2-4ea6-bc36-113feb0df335","is_latest":true,"published_at":"2025-09-09T10:55:22.909499Z","updated_at":"2025-09-09T10:55:22.909499Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Notion","is_in_organization":true,"license":"MIT License","name":"notion-mcp-server","name_with_owner":"makenotion/notion-mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/8da90e1f75e5b777558b16313431b942fd05133875bf36dd3387c880c12d63bc/makenotion/notion-mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/4792552?v=4","preferred_image":"https://avatars.githubusercontent.com/u/4792552?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-08-22T19:08:45Z","readme":"# Notion MCP Server\n\n\u003e [!NOTE] \n\u003e \n\u003e We’ve introduced **Notion MCP**, a remote MCP server with the following improvements:\n\u003e - Easy installation via standard OAuth. No need to fiddle with JSON or API token anymore.\n\u003e - Powerful tools tailored to AI agents. These tools are designed with optimized token consumption in mind.\n\u003e \n\u003e Learn more and try it out [here](https://developers.notion.com/docs/mcp)\n\n\n![notion-mcp-sm](https://github.com/user-attachments/assets/6c07003c-8455-4636-b298-d60ffdf46cd8)\n\nThis project implements an [MCP server](https://spec.modelcontextprotocol.io/) for the [Notion API](https://developers.notion.com/reference/intro). \n\n![mcp-demo](https://github.com/user-attachments/assets/e3ff90a7-7801-48a9-b807-f7dd47f0d3d6)\n\n### Installation\n\n#### 1. Setting up Integration in Notion:\nGo to [https://www.notion.so/profile/integrations](https://www.notion.so/profile/integrations) and create a new **internal** integration or select an existing one.\n\n![Creating a Notion Integration token](docs/images/integrations-creation.png)\n\nWhile we limit the scope of Notion API's exposed (for example, you will not be able to delete databases via MCP), there is a non-zero risk to workspace data by exposing it to LLMs. Security-conscious users may want to further configure the Integration's _Capabilities_. \n\nFor example, you can create a read-only integration token by giving only \"Read content\" access from the \"Configuration\" tab:\n\n![Notion Integration Token Capabilities showing Read content checked](docs/images/integrations-capabilities.png)\n\n#### 2. Connecting content to integration:\nEnsure relevant pages and databases are connected to your integration.\n\nTo do this, visit the **Access** tab in your internal integration settings. Edit access and select the pages you'd like to use.\n![Integration Access tab](docs/images/integration-access.png)\n\n![Edit integration access](docs/images/page-access-edit.png)\n\nAlternatively, you can grant page access individually. You'll need to visit the target page, and click on the 3 dots, and select \"Connect to integration\". \n\n![Adding Integration Token to Notion Connections](docs/images/connections.png)\n\n#### 3. Adding MCP config to your client:\n\n##### Using npm:\n\n**Cursor \u0026 Claude:**\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json` (MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`)\n\n**Option 1: Using NOTION_TOKEN (recommended)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Option 2: Using OPENAPI_MCP_HEADERS (for advanced use cases)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    }\n  }\n}\n```\n\n**Zed**\n\nAdd the following to your `settings.json`\n\n```json\n{\n  \"context_servers\": {\n    \"some-context-server\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n        \"env\": {\n          \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n        }\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n##### Using Docker:\n\nThere are two options for running the MCP server with Docker:\n\n###### Option 1: Using the official Docker Hub image:\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"NOTION_TOKEN\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"OPENAPI_MCP_HEADERS\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\":\\\"Bearer ntn_****\\\",\\\"Notion-Version\\\":\\\"2022-06-28\\\"}\"\n      }\n    }\n  }\n}\n```\n\nThis approach:\n- Uses the official Docker Hub image\n- Properly handles JSON escaping via environment variables\n- Provides a more reliable configuration method\n\n###### Option 2: Building the Docker image locally:\n\nYou can also build and run the Docker image locally. First, build the Docker image:\n\n```bash\ndocker compose build\n```\n\nThen, add the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"NOTION_TOKEN=ntn_****\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"OPENAPI_MCP_HEADERS={\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\"}\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\nDon't forget to replace `ntn_****` with your integration secret. Find it from your integration configuration tab:\n\n![Copying your Integration token from the Configuration tab in the developer portal](https://github.com/user-attachments/assets/67b44536-5333-49fa-809c-59581bf5370a)\n\n\n#### Installing via Smithery\n\n[![smithery badge](https://smithery.ai/badge/@makenotion/notion-mcp-server)](https://smithery.ai/server/@makenotion/notion-mcp-server)\n\nTo install Notion API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@makenotion/notion-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @makenotion/notion-mcp-server --client claude\n```\n\n### Transport Options\n\nThe Notion MCP Server supports two transport modes:\n\n#### STDIO Transport (Default)\nThe default transport mode uses standard input/output for communication. This is the standard MCP transport used by most clients like Claude Desktop.\n\n```bash\n# Run with default stdio transport\nnpx @notionhq/notion-mcp-server\n\n# Or explicitly specify stdio\nnpx @notionhq/notion-mcp-server --transport stdio\n```\n\n#### Streamable HTTP Transport\nFor web-based applications or clients that prefer HTTP communication, you can use the Streamable HTTP transport:\n\n```bash\n# Run with Streamable HTTP transport on port 3000 (default)\nnpx @notionhq/notion-mcp-server --transport http\n\n# Run on a custom port\nnpx @notionhq/notion-mcp-server --transport http --port 8080\n\n# Run with a custom authentication token\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\nWhen using Streamable HTTP transport, the server will be available at `http://0.0.0.0:\u003cport\u003e/mcp`.\n\n##### Authentication\nThe Streamable HTTP transport requires bearer token authentication for security. You have three options:\n\n**Option 1: Auto-generated token (recommended for development)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http\n```\nThe server will generate a secure random token and display it in the console:\n```\nGenerated auth token: a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\nUse this token in the Authorization header: Bearer a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\n```\n\n**Option 2: Custom token via command line (recommended for production)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\n**Option 3: Custom token via environment variable (recommended for production)**\n```bash\nAUTH_TOKEN=\"your-secret-token\" npx @notionhq/notion-mcp-server --transport http\n```\n\nThe command line argument `--auth-token` takes precedence over the `AUTH_TOKEN` environment variable if both are provided.\n\n##### Making HTTP Requests\nAll requests to the Streamable HTTP transport must include the bearer token in the Authorization header:\n\n```bash\n# Example request\ncurl -H \"Authorization: Bearer your-token-here\" \\\n     -H \"Content-Type: application/json\" \\\n     -H \"mcp-session-id: your-session-id\" \\\n     -d '{\"jsonrpc\": \"2.0\", \"method\": \"initialize\", \"params\": {}, \"id\": 1}' \\\n     http://localhost:3000/mcp\n```\n\n**Note:** Make sure to set either the `NOTION_TOKEN` environment variable (recommended) or the `OPENAPI_MCP_HEADERS` environment variable with your Notion integration token when using either transport mode.\n\n### Examples\n\n1. Using the following instruction\n```\nComment \"Hello MCP\" on page \"Getting started\"\n```\n\nAI will correctly plan two API calls, `v1/search` and `v1/comments`, to achieve the task\n\n2. Similarly, the following instruction will result in a new page named \"Notion MCP\" added to parent page \"Development\"\n```\nAdd a page titled \"Notion MCP\" to page \"Development\"\n```\n\n3. You may also reference content ID directly\n```\nGet the content of page 1a6b35e6e67f802fa7e1d27686f017f2\n```\n\n### Development\n\nBuild\n\n```\nnpm run build\n```\n\nExecute\n\n```\nnpx -y --prefix /path/to/local/notion-mcp-server @notionhq/notion-mcp-server\n```\n\nPublish\n\n```\nnpm publish --access public\n```\n","stargazer_count":3356,"uses_custom_opengraph_image":false}}}},{"name":"azure/azure-mcp","description":"The Azure MCP Server, bringing the power of Azure to your agents.","status":"active","repository":{"url":"https://github.com/azure/azure-mcp","source":"github","id":"967503541","readme":"\u003e[!IMPORTANT]\n🚀 Active development has moved to [microsoft/mcp](https://github.com/microsoft/mcp/tree/main/servers/Azure.Mcp.Server) as of August 25, 2025\n--------\n\n# 🌟 Azure MCP Server\n\nThe Azure MCP Server implements the [MCP specification](https://modelcontextprotocol.io) to create a seamless connection between AI agents and Azure services.  Azure MCP Server can be used alone or with the [GitHub Copilot for Azure extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azure-github-copilot) in VS Code.  This project is in Public Preview and implementation may significantly change prior to our General Availability.\n\n\n\u003e[!WARNING]\n\u003e**Deprecation Notice: SSE transport mode has been removed in version [0.4.0 (2025-07-15)](https://github.com/Azure/azure-mcp/blob/main/CHANGELOG.md#breaking-changes-7).**\n\u003e\n\u003e SSE was deprecated in MCP `2025-03-26` due to [security vulnerabilities and architectural limitations](https://blog.fka.dev/blog/2025-06-06-why-mcp-deprecated-sse-and-go-with-streamable-http/). Users must discontinue use of SSE transport mode and upgrade to version `0.4.0` or newer to maintain compatibility with current MCP clients.\n\n\n### ✅ VS Code Install Guide (Recommended)\n\n1. Install either the stable or Insiders release of VS Code:\n   * [💫 Stable release](https://code.visualstudio.com/download)\n   * [🔮 Insiders release](https://code.visualstudio.com/insiders)\n1. Install the [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot) and [GitHub Copilot Chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat) extensions\n1. Install the [Azure MCP Server](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azure-mcp-server) extension\n\n### 🚀 Quick Start\n\n1. Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n1. Click `refresh` on the tools list\n    - You should see the Azure MCP Server in the list of tools\n1. Try a prompt that tells the agent to use the Azure MCP Server, such as `List my Azure Storage containers`\n    - The agent should be able to use the Azure MCP Server tools to complete your query\n1. Check out the [documentation](https://learn.microsoft.com/azure/developer/azure-mcp-server/) and review the [troubleshooting guide](https://github.com/Azure/azure-mcp/blob/main/TROUBLESHOOTING.md) for commonly asked questions\n1. We're building this in the open. Your feedback is much appreciated, and will help us shape the future of the Azure MCP server\n    - 👉 [Open an issue in the public repository](https://github.com/Azure/azure-mcp/issues/new/choose)\n\n\n## ✨ What can you do with the Azure MCP Server?\n\nThe Azure MCP Server supercharges your agents with Azure context. Here are some cool prompts you can try:\n\n### 🔎 Azure AI Search\n\n* \"What indexes do I have in my Azure AI Search service 'mysvc'?\"\n* \"Let's search this index for 'my search query'\"\n\n### ⚙️ Azure App Configuration\n\n* \"List my App Configuration stores\"\n* \"Show my key-value pairs in App Config\"\n\n### 📦 Azure Container Registry (ACR)\n\n* \"List all my Azure Container Registries\"\n* \"Show me my container registries in the 'myproject' resource group\"\n* \"List all my Azure Container Registry repositories\"\n\n### ☸️ Azure Kubernetes Service (AKS)\n\n* \"List my AKS clusters in my subscription\"\n* \"Show me all my Azure Kubernetes Service clusters\"\n\n### 📊 Azure Cosmos DB\n\n* \"Show me all my Cosmos DB databases\"\n* \"List containers in my Cosmos DB database\"\n\n### 🧮 Azure Data Explorer\n\n* \"Get Azure Data Explorer databases in cluster 'mycluster'\"\n* \"Sample 10 rows from table 'StormEvents' in Azure Data Explorer database 'db1'\"\n\n### ⚡ Azure Managed Lustre\n\n* \"List the Azure Managed Lustre clusters in resource group 'my-resourcegroup'\"\n* \"How many IP Addresses I need to create a 128 TiB cluster of AMLFS 500?\"\n\n### 📊 Azure Monitor\n\n* \"Query my Log Analytics workspace\"\n\n### 🔧 Azure Resource Management\n\n* \"List my resource groups\"\n* \"List my Azure CDN endpoints\"\n* \"Help me build an Azure application using Node.js\"\n\n### 🗄️ Azure SQL Database\n\n* \"Show me details about my Azure SQL database 'mydb'\"\n* \"List all databases in my Azure SQL server 'myserver'\"\n* \"List all firewall rules for my Azure SQL server 'myserver'\"\n* \"List all elastic pools in my Azure SQL server 'myserver'\"\n* \"List Active Directory administrators for my Azure SQL server 'myserver'\"\n\n### 💾 Azure Storage\n\n* \"List my Azure storage accounts\"\n* \"Get details about my storage account 'mystorageaccount'\"\n* \"Create a new storage account in East US with Data Lake support\"\n* \"Show me the tables in my Storage account\"\n* \"Get details about my Storage container\"\n* \"Upload my file to the blob container\"\n* \"List paths in my Data Lake file system\"\n* \"List files and directories in my File Share\"\n* \"Send a message to my storage queue\"\n\n## 🛠️ Currently Supported Tools\n\n\u003cdetails\u003e\n\u003csummary\u003eThe Azure MCP Server provides tools for interacting with the following Azure services\u003c/summary\u003e\n\n### 🔎 Azure AI Search (search engine/vector database)\n\n* List Azure AI Search services\n* List indexes and look at their schema and configuration\n* Query search indexes\n\n### ⚙️ Azure App Configuration\n\n* List App Configuration stores\n* Manage key-value pairs\n* Handle labeled configurations\n* Lock/unlock configuration settings\n\n### 🛡️ Azure Best Practices\n\n* Get secure, production-grade Azure SDK best practices for effective code generation.\n\n### 🖥️ Azure CLI Extension\n\n* Execute Azure CLI commands directly\n* Support for all Azure CLI functionality\n\n### 📦 Azure Container Registry (ACR)\n\n* List Azure Container Registries and repositories in a subscription\n* Filter container registries and repositories by resource group\n* JSON output formatting\n* Cross-platform compatibility\n\n### 📊 Azure Cosmos DB (NoSQL Databases)\n\n* List Cosmos DB accounts\n* List and query databases\n* Manage containers and items\n* Execute SQL queries against containers\n\n### 🧮 Azure Data Explorer\n\n* List Azure Data Explorer clusters\n* List databases\n* List tables\n* Get schema for a table\n* Sample rows from a table\n* Query using KQL\n\n### 🐬 Azure Database for MySQL - Flexible Server\n\n* List and query databases.\n* List and get schema for tables.\n* List, get configuration and get parameters for servers.\n\n### 🐘 Azure Database for PostgreSQL - Flexible Server\n\n* List and query databases.\n* List and get schema for tables.\n* List, get configuration and get/set parameters for servers.\n\n### 🛠️ Azure Developer CLI (azd) Extension\n\n* Execute Azure Developer CLI commands directly\n* Support for template discovery, template initialization, provisioning and deployment\n* Cross-platform compatibility\n\n### 🚀 Azure Deploy\n\n* Generate Azure service architecture diagrams from source code\n* Create a deploy plan for provisioning and deploying the application\n* Get the application service log for a specific azd environment\n* Get the bicep or terraform file generation rules for an application\n* Get the GitHub pipeline creation guideline for an application\n\n### 🧮 Azure Foundry\n\n* List Azure Foundry models\n* Deploy foundry models\n* List foundry model deployments\n* List knowledge indexes\n\n### ☁️ Azure Function App\n\n* List Azure Function Apps\n* Get details for a specific Function App\n\n### 🔑 Azure Key Vault\n\n* List, create, and import certificates\n* List and create keys\n* List and create secrets\n\n### ☸️ Azure Kubernetes Service (AKS)\n\n* List Azure Kubernetes Service clusters\n\n### 📦 Azure Load Testing\n\n* List, create load test resources\n* List, create load tests\n* Get, list, (create) run and rerun, update load test runs\n\n\n### 🚀 Azure Managed Grafana\n\n* List Azure Managed Grafana\n\n### ⚡ Azure Managed Lustre\n\n* List Azure Managed Lustre filesystems\n* Get the number of IP addresses required for a specific SKU and size of Azure Managed Lustre filesystem\n\n### 🏪 Azure Marketplace\n\n* Get details about Marketplace products\n\n### 📈 Azure Monitor\n\n#### Log Analytics\n\n* List Log Analytics workspaces\n* Query logs using KQL\n* List available tables\n\n#### Health Models\n\n* Get health of an entity\n\n#### Metrics\n\n* Query Azure Monitor metrics for resources with time series data\n* List available metric definitions for resources\n\n### 🏥 Azure Service Health\n\n* Get the availability status for a specific resource\n* List availability statuses for all resources in a subscription or resource group\n\n### ⚙️ Azure Native ISV Services\n\n* List Monitored Resources in a Datadog Monitor\n\n### 🛡️ Azure Quick Review CLI Extension\n\n* Scan Azure resources for compliance related recommendations\n\n### 📊 Azure Quota\n\n* List available regions\n* Check quota usage\n\n### 🔴 Azure Redis Cache\n\n* List Redis Cluster resources\n* List databases in Redis Clusters\n* List Redis Cache resources\n* List access policies for Redis Caches\n\n### 🏗️ Azure Resource Groups\n\n* List resource groups\n\n### 🎭 Azure Role-Based Access Control (RBAC)\n\n* List role assignments\n\n### 🚌 Azure Service Bus\n\n* Examine properties and runtime information about queues, topics, and subscriptions\n\n### 🗄️ Azure SQL Database\n\n* Show database details and properties\n* List the details and properties of all databases\n* List SQL server firewall rules\n\n### 🗄️ Azure SQL Elastic Pool\n\n* List elastic pools in SQL servers\n\n### 🗄️ Azure SQL Server\n\n* List Microsoft Entra ID administrators for SQL servers\n\n### 💾 Azure Storage\n\n* List and create Storage accounts\n* Get detailed information about specific Storage accounts\n* Manage blob containers and blobs\n* Upload files to blob containers\n* List and query Storage tables\n* List paths in Data Lake file systems\n* Get container properties and metadata\n* List files and directories in File Shares\n\n### 📋 Azure Subscription\n\n* List Azure subscriptions\n\n### 🏗️ Azure Terraform Best Practices\n\n* Get secure, production-grade Azure Terraform best practices for effective code generation and command execution\n\n### 🖥️ Azure Virtual Desktop\n\n* List Azure Virtual Desktop host pools\n* List session hosts in host pools\n* List user sessions on a session host\n\n### 📊 Azure Workbooks\n\n* List workbooks in resource groups\n* Create new workbooks with custom visualizations\n* Update existing workbook configurations\n* Get workbook details and metadata\n* Delete workbooks when no longer needed\n\n### 🏗️ Bicep\n\n* Get the Bicep schema for specific Azure resource types\n\n### 🏗️ Cloud Architect\n\n* Design Azure cloud architectures through guided questions\n\nAgents and models can discover and learn best practices and usage guidelines for the `azd` MCP tool. For more information, see [AZD Best Practices](https://github.com/Azure/azure-mcp/tree/main/areas/extension/src/AzureMcp.Extension/Resources/azd-best-practices.txt).\n\n\u003c/details\u003e\n\nFor detailed command documentation and examples, see [Azure MCP Commands](https://github.com/Azure/azure-mcp/blob/main/docs/azmcp-commands.md).\n\n## 🔄️ Upgrading Existing Installs to the Latest Version\n\n\u003cdetails\u003e\n\u003csummary\u003eHow to stay current with releases of Azure MCP Server\u003c/summary\u003e\n\n#### NPX\n\nIf you use the default package spec of `@azure/mcp@latest`, npx will look for a new version on each server start. If you use just `@azure/mcp`, npx will continue to use its cached version until its cache is cleared.\n\n#### NPM\n\nIf you globally install the cli via `npm install -g @azure/mcp` it will use the installed version until you manually update it with `npm update -g @azure/mcp`.\n\n#### Docker\n\nThere is no version update built into the docker image.  To update, just pull the latest from the repo and repeat the [docker installation instructions](#docker-install).\n\n#### VS Code\n\nInstallation in VS Code should be in one of the previous forms and the update instructions are the same. If you installed the mcp server with the `npx` command and  `-y @azure/mcp@latest` args, npx will check for package updates each time VS Code starts the server. Using a docker container in VS Code has the same no-update limitation described above.\n\u003c/details\u003e\n\n## ⚙️ Advanced Install Scenarios (Optional)\n\n\u003cdetails\u003e\n\u003csummary\u003eDocker containers, custom MCP clients, and manual install options\u003c/summary\u003e\n\n### 🐋 Docker Install Steps (Optional)\n\nMicrosoft publishes an official Azure MCP Server Docker container on the [Microsoft Artifact Registry](https://mcr.microsoft.com/artifact/mar/azure-sdk/azure-mcp).\n\nFor a step-by-step Docker installation, follow these instructions:\n\n1. Create an `.env` file with environment variables that [match one of the `EnvironmentCredential`](https://learn.microsoft.com/dotnet/api/azure.identity.environmentcredential) sets.  For example, a `.env` file using a service principal could look like:\n\n    ```bash\n    AZURE_TENANT_ID={YOUR_AZURE_TENANT_ID}\n    AZURE_CLIENT_ID={YOUR_AZURE_CLIENT_ID}\n    AZURE_CLIENT_SECRET={YOUR_AZURE_CLIENT_SECRET}\n    ```\n\n2. Add `.vscode/mcp.json` or update existing MCP configuration. Replace `/full/path/to/.env` with a path to your `.env` file.\n\n    ```json\n    {\n      \"servers\": {\n        \"Azure MCP Server\": {\n          \"command\": \"docker\",\n          \"args\": [\n            \"run\",\n            \"-i\",\n            \"--rm\",\n            \"--env-file\",\n            \"/full/path/to/.env\"\n            \"mcr.microsoft.com/azure-sdk/azure-mcp:latest\",\n          ]\n        }\n      }\n    }\n    ```\n\nOptionally, use `--env` or `--volume` to pass authentication values.\n\n### 🤖 Custom MCP Client Install Steps (Optional)\n\nYou can easily configure your MCP client to use the Azure MCP Server. Have your client run the following command and access it via standard IO.\n\n```bash\nnpx -y @azure/mcp@latest server start\n```\n\n### 🔧 Manual Install Steps (Optional)\n\nFor a step-by-step installation, follow these instructions:\n\n1. Add `.vscode/mcp.json`:\n\n    ```json\n    {\n      \"servers\": {\n        \"Azure MCP Server\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\",\n            \"@azure/mcp@latest\",\n            \"server\",\n            \"start\"\n          ]\n        }\n      }\n    }\n    ```\n\n    You can optionally set the `--namespace \u003cnamespace\u003e` flag to install tools for the specified Azure product or service.\n\n1. Add `.vscode/mcp.json`:\n\n    ```json\n    {\n      \"servers\": {\n        \"Azure Best Practices\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\",\n            \"@azure/mcp@latest\",\n            \"server\",\n            \"start\",\n            \"--namespace\",\n            \"bestpractices\" // Any of the available MCP servers can be referenced here.\n          ]\n        }\n      }\n    }\n    ```\n\nMore end-to-end MCP client/agent guides are coming soon!\n\u003c/details\u003e\n\n## Data Collection\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's [privacy statement](https://www.microsoft.com/privacy/privacystatement). You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n### Telemetry Configuration\n\nTelemetry collection is on by default.\n\nTo opt out, set the environment variable `AZURE_MCP_COLLECT_TELEMETRY` to `false` in your environment.\n\n## 📝 Troubleshooting\n\nSee [Troubleshooting guide](https://github.com/Azure/azure-mcp/blob/main/TROUBLESHOOTING.md#128-tool-limit-issue) for help with common issues and logging.\n\n### 🔑 Authentication\n\n\u003cdetails\u003e\n\u003csummary\u003eAuthentication options including DefaultAzureCredential flow, RBAC permissions, troubleshooting, and production credentials\u003c/summary\u003e\n\nThe Azure MCP Server uses the Azure Identity library for .NET to authenticate to Microsoft Entra ID. For detailed information, see [Authentication Fundamentals](https://github.com/Azure/azure-mcp/blob/main/docs/Authentication.md#authentication-fundamentals).\n\nIf you're running into any issues with authentication, visit our [troubleshooting guide](https://github.com/Azure/azure-mcp/blob/main/TROUBLESHOOTING.md#authentication).\n\nFor enterprise authentication scenarios, including network restrictions, security policies, and protected resources, see [Authentication Scenarios in Enterprise Environments](https://github.com/Azure/azure-mcp/blob/main/docs/Authentication.md#authentication-scenarios-in-enterprise-environments).\n\u003c/details\u003e\n\n## 🛡️ Security Note\n\nYour credentials are always handled securely through the official [Azure Identity SDK](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/identity/Azure.Identity/README.md) - **we never store or manage tokens directly**.\n\nMCP as a phenomenon is very novel and cutting-edge. As with all new technology standards, consider doing a security review to ensure any systems that integrate with MCP servers follow all regulations and standards your system is expected to adhere to. This includes not only the Azure MCP Server, but any MCP client/agent that you choose to implement down to the model provider.\n\n## 👥 Contributing\n\nWe welcome contributions to the Azure MCP Server! Whether you're fixing bugs, adding new features, or improving documentation, your contributions are welcome.\n\nPlease read our [Contributing Guide](https://github.com/Azure/azure-mcp/blob/main/CONTRIBUTING.md) for guidelines on:\n\n* 🛠️ Setting up your development environment\n* ✨ Adding new commands\n* 📝 Code style and testing requirements\n* 🔄 Making pull requests\n\n## 🤝 Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [open@microsoft.com](mailto:open@microsoft.com)\nwith any additional questions or comments.\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:27Z","updated_at":"2025-10-22T11:21:41Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"@azure/mcp","version":"latest","runtime_hint":"npx","runtime_arguments":[{"is_required":true,"format":"string","value":"-y","type":"positional","name":"-y","value_hint":"noninteractive_mode"}],"package_arguments":[{"is_required":true,"format":"string","value":"server","type":"positional","name":"server","value_hint":"cli_command"},{"is_required":true,"format":"string","value":"start","type":"positional","name":"start","value_hint":"cli_command"}],"environment_variables":[{"value":"{azure_tenant_id}","variables":{"azure_tenant_id":{"description":"Tenant ID (for service principal / workload identity auth)."}},"name":"AZURE_TENANT_ID"},{"value":"{azure_client_id}","variables":{"azure_client_id":{"description":"Client ID (service principal or managed identity)."}},"name":"AZURE_CLIENT_ID"},{"value":"{azure_client_secret}","variables":{"azure_client_secret":{"description":"Client secret (if using service principal secret).","is_secret":true}},"name":"AZURE_CLIENT_SECRET"},{"value":"{azure_subscription_id}","variables":{"azure_subscription_id":{"description":"Optional: prefer a specific subscription by default."}},"name":"AZURE_SUBSCRIPTION_ID"},{"value":"{azure_mcp_collect_telemetry}","variables":{"azure_mcp_collect_telemetry":{"description":"Set to 'false' to opt out of telemetry (default is true)."}},"name":"AZURE_MCP_COLLECT_TELEMETRY"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"6cf2c8d0-6872-406b-bdeb-495272df709d","is_latest":true,"published_at":"2025-08-20T00:00:00Z","updated_at":"2025-08-20T00:00:00Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Azure","homepage_url":"https://learn.microsoft.com/azure/developer/azure-mcp-server/","is_in_organization":true,"license":"MIT License","name":"azure-mcp","name_with_owner":"Azure/azure-mcp","opengraph_image_url":"https://opengraph.githubassets.com/ae53e47c19b6f496afa3c67a16f00d52095f580d7465944b26309f8f896d6f6e/Azure/azure-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/6844498?v=4","preferred_image":"https://avatars.githubusercontent.com/u/6844498?v=4","primary_language":"C#","primary_language_color":"#178600","pushed_at":"2025-09-03T16:20:30Z","readme":"\u003e[!IMPORTANT]\n🚀 Active development has moved to [microsoft/mcp](https://github.com/microsoft/mcp/tree/main/servers/Azure.Mcp.Server) as of August 25, 2025\n--------\n\n# 🌟 Azure MCP Server\n\nThe Azure MCP Server implements the [MCP specification](https://modelcontextprotocol.io) to create a seamless connection between AI agents and Azure services.  Azure MCP Server can be used alone or with the [GitHub Copilot for Azure extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azure-github-copilot) in VS Code.  This project is in Public Preview and implementation may significantly change prior to our General Availability.\n\n\n\u003e[!WARNING]\n\u003e**Deprecation Notice: SSE transport mode has been removed in version [0.4.0 (2025-07-15)](https://github.com/Azure/azure-mcp/blob/main/CHANGELOG.md#breaking-changes-7).**\n\u003e\n\u003e SSE was deprecated in MCP `2025-03-26` due to [security vulnerabilities and architectural limitations](https://blog.fka.dev/blog/2025-06-06-why-mcp-deprecated-sse-and-go-with-streamable-http/). Users must discontinue use of SSE transport mode and upgrade to version `0.4.0` or newer to maintain compatibility with current MCP clients.\n\n\n### ✅ VS Code Install Guide (Recommended)\n\n1. Install either the stable or Insiders release of VS Code:\n   * [💫 Stable release](https://code.visualstudio.com/download)\n   * [🔮 Insiders release](https://code.visualstudio.com/insiders)\n1. Install the [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot) and [GitHub Copilot Chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat) extensions\n1. Install the [Azure MCP Server](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azure-mcp-server) extension\n\n### 🚀 Quick Start\n\n1. Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n1. Click `refresh` on the tools list\n    - You should see the Azure MCP Server in the list of tools\n1. Try a prompt that tells the agent to use the Azure MCP Server, such as `List my Azure Storage containers`\n    - The agent should be able to use the Azure MCP Server tools to complete your query\n1. Check out the [documentation](https://learn.microsoft.com/azure/developer/azure-mcp-server/) and review the [troubleshooting guide](https://github.com/Azure/azure-mcp/blob/main/TROUBLESHOOTING.md) for commonly asked questions\n1. We're building this in the open. Your feedback is much appreciated, and will help us shape the future of the Azure MCP server\n    - 👉 [Open an issue in the public repository](https://github.com/Azure/azure-mcp/issues/new/choose)\n\n\n## ✨ What can you do with the Azure MCP Server?\n\nThe Azure MCP Server supercharges your agents with Azure context. Here are some cool prompts you can try:\n\n### 🔎 Azure AI Search\n\n* \"What indexes do I have in my Azure AI Search service 'mysvc'?\"\n* \"Let's search this index for 'my search query'\"\n\n### ⚙️ Azure App Configuration\n\n* \"List my App Configuration stores\"\n* \"Show my key-value pairs in App Config\"\n\n### 📦 Azure Container Registry (ACR)\n\n* \"List all my Azure Container Registries\"\n* \"Show me my container registries in the 'myproject' resource group\"\n* \"List all my Azure Container Registry repositories\"\n\n### ☸️ Azure Kubernetes Service (AKS)\n\n* \"List my AKS clusters in my subscription\"\n* \"Show me all my Azure Kubernetes Service clusters\"\n\n### 📊 Azure Cosmos DB\n\n* \"Show me all my Cosmos DB databases\"\n* \"List containers in my Cosmos DB database\"\n\n### 🧮 Azure Data Explorer\n\n* \"Get Azure Data Explorer databases in cluster 'mycluster'\"\n* \"Sample 10 rows from table 'StormEvents' in Azure Data Explorer database 'db1'\"\n\n### ⚡ Azure Managed Lustre\n\n* \"List the Azure Managed Lustre clusters in resource group 'my-resourcegroup'\"\n* \"How many IP Addresses I need to create a 128 TiB cluster of AMLFS 500?\"\n\n### 📊 Azure Monitor\n\n* \"Query my Log Analytics workspace\"\n\n### 🔧 Azure Resource Management\n\n* \"List my resource groups\"\n* \"List my Azure CDN endpoints\"\n* \"Help me build an Azure application using Node.js\"\n\n### 🗄️ Azure SQL Database\n\n* \"Show me details about my Azure SQL database 'mydb'\"\n* \"List all databases in my Azure SQL server 'myserver'\"\n* \"List all firewall rules for my Azure SQL server 'myserver'\"\n* \"List all elastic pools in my Azure SQL server 'myserver'\"\n* \"List Active Directory administrators for my Azure SQL server 'myserver'\"\n\n### 💾 Azure Storage\n\n* \"List my Azure storage accounts\"\n* \"Get details about my storage account 'mystorageaccount'\"\n* \"Create a new storage account in East US with Data Lake support\"\n* \"Show me the tables in my Storage account\"\n* \"Get details about my Storage container\"\n* \"Upload my file to the blob container\"\n* \"List paths in my Data Lake file system\"\n* \"List files and directories in my File Share\"\n* \"Send a message to my storage queue\"\n\n## 🛠️ Currently Supported Tools\n\n\u003cdetails\u003e\n\u003csummary\u003eThe Azure MCP Server provides tools for interacting with the following Azure services\u003c/summary\u003e\n\n### 🔎 Azure AI Search (search engine/vector database)\n\n* List Azure AI Search services\n* List indexes and look at their schema and configuration\n* Query search indexes\n\n### ⚙️ Azure App Configuration\n\n* List App Configuration stores\n* Manage key-value pairs\n* Handle labeled configurations\n* Lock/unlock configuration settings\n\n### 🛡️ Azure Best Practices\n\n* Get secure, production-grade Azure SDK best practices for effective code generation.\n\n### 🖥️ Azure CLI Extension\n\n* Execute Azure CLI commands directly\n* Support for all Azure CLI functionality\n\n### 📦 Azure Container Registry (ACR)\n\n* List Azure Container Registries and repositories in a subscription\n* Filter container registries and repositories by resource group\n* JSON output formatting\n* Cross-platform compatibility\n\n### 📊 Azure Cosmos DB (NoSQL Databases)\n\n* List Cosmos DB accounts\n* List and query databases\n* Manage containers and items\n* Execute SQL queries against containers\n\n### 🧮 Azure Data Explorer\n\n* List Azure Data Explorer clusters\n* List databases\n* List tables\n* Get schema for a table\n* Sample rows from a table\n* Query using KQL\n\n### 🐬 Azure Database for MySQL - Flexible Server\n\n* List and query databases.\n* List and get schema for tables.\n* List, get configuration and get parameters for servers.\n\n### 🐘 Azure Database for PostgreSQL - Flexible Server\n\n* List and query databases.\n* List and get schema for tables.\n* List, get configuration and get/set parameters for servers.\n\n### 🛠️ Azure Developer CLI (azd) Extension\n\n* Execute Azure Developer CLI commands directly\n* Support for template discovery, template initialization, provisioning and deployment\n* Cross-platform compatibility\n\n### 🚀 Azure Deploy\n\n* Generate Azure service architecture diagrams from source code\n* Create a deploy plan for provisioning and deploying the application\n* Get the application service log for a specific azd environment\n* Get the bicep or terraform file generation rules for an application\n* Get the GitHub pipeline creation guideline for an application\n\n### 🧮 Azure Foundry\n\n* List Azure Foundry models\n* Deploy foundry models\n* List foundry model deployments\n* List knowledge indexes\n\n### ☁️ Azure Function App\n\n* List Azure Function Apps\n* Get details for a specific Function App\n\n### 🔑 Azure Key Vault\n\n* List, create, and import certificates\n* List and create keys\n* List and create secrets\n\n### ☸️ Azure Kubernetes Service (AKS)\n\n* List Azure Kubernetes Service clusters\n\n### 📦 Azure Load Testing\n\n* List, create load test resources\n* List, create load tests\n* Get, list, (create) run and rerun, update load test runs\n\n\n### 🚀 Azure Managed Grafana\n\n* List Azure Managed Grafana\n\n### ⚡ Azure Managed Lustre\n\n* List Azure Managed Lustre filesystems\n* Get the number of IP addresses required for a specific SKU and size of Azure Managed Lustre filesystem\n\n### 🏪 Azure Marketplace\n\n* Get details about Marketplace products\n\n### 📈 Azure Monitor\n\n#### Log Analytics\n\n* List Log Analytics workspaces\n* Query logs using KQL\n* List available tables\n\n#### Health Models\n\n* Get health of an entity\n\n#### Metrics\n\n* Query Azure Monitor metrics for resources with time series data\n* List available metric definitions for resources\n\n### 🏥 Azure Service Health\n\n* Get the availability status for a specific resource\n* List availability statuses for all resources in a subscription or resource group\n\n### ⚙️ Azure Native ISV Services\n\n* List Monitored Resources in a Datadog Monitor\n\n### 🛡️ Azure Quick Review CLI Extension\n\n* Scan Azure resources for compliance related recommendations\n\n### 📊 Azure Quota\n\n* List available regions\n* Check quota usage\n\n### 🔴 Azure Redis Cache\n\n* List Redis Cluster resources\n* List databases in Redis Clusters\n* List Redis Cache resources\n* List access policies for Redis Caches\n\n### 🏗️ Azure Resource Groups\n\n* List resource groups\n\n### 🎭 Azure Role-Based Access Control (RBAC)\n\n* List role assignments\n\n### 🚌 Azure Service Bus\n\n* Examine properties and runtime information about queues, topics, and subscriptions\n\n### 🗄️ Azure SQL Database\n\n* Show database details and properties\n* List the details and properties of all databases\n* List SQL server firewall rules\n\n### 🗄️ Azure SQL Elastic Pool\n\n* List elastic pools in SQL servers\n\n### 🗄️ Azure SQL Server\n\n* List Microsoft Entra ID administrators for SQL servers\n\n### 💾 Azure Storage\n\n* List and create Storage accounts\n* Get detailed information about specific Storage accounts\n* Manage blob containers and blobs\n* Upload files to blob containers\n* List and query Storage tables\n* List paths in Data Lake file systems\n* Get container properties and metadata\n* List files and directories in File Shares\n\n### 📋 Azure Subscription\n\n* List Azure subscriptions\n\n### 🏗️ Azure Terraform Best Practices\n\n* Get secure, production-grade Azure Terraform best practices for effective code generation and command execution\n\n### 🖥️ Azure Virtual Desktop\n\n* List Azure Virtual Desktop host pools\n* List session hosts in host pools\n* List user sessions on a session host\n\n### 📊 Azure Workbooks\n\n* List workbooks in resource groups\n* Create new workbooks with custom visualizations\n* Update existing workbook configurations\n* Get workbook details and metadata\n* Delete workbooks when no longer needed\n\n### 🏗️ Bicep\n\n* Get the Bicep schema for specific Azure resource types\n\n### 🏗️ Cloud Architect\n\n* Design Azure cloud architectures through guided questions\n\nAgents and models can discover and learn best practices and usage guidelines for the `azd` MCP tool. For more information, see [AZD Best Practices](https://github.com/Azure/azure-mcp/tree/main/areas/extension/src/AzureMcp.Extension/Resources/azd-best-practices.txt).\n\n\u003c/details\u003e\n\nFor detailed command documentation and examples, see [Azure MCP Commands](https://github.com/Azure/azure-mcp/blob/main/docs/azmcp-commands.md).\n\n## 🔄️ Upgrading Existing Installs to the Latest Version\n\n\u003cdetails\u003e\n\u003csummary\u003eHow to stay current with releases of Azure MCP Server\u003c/summary\u003e\n\n#### NPX\n\nIf you use the default package spec of `@azure/mcp@latest`, npx will look for a new version on each server start. If you use just `@azure/mcp`, npx will continue to use its cached version until its cache is cleared.\n\n#### NPM\n\nIf you globally install the cli via `npm install -g @azure/mcp` it will use the installed version until you manually update it with `npm update -g @azure/mcp`.\n\n#### Docker\n\nThere is no version update built into the docker image.  To update, just pull the latest from the repo and repeat the [docker installation instructions](#docker-install).\n\n#### VS Code\n\nInstallation in VS Code should be in one of the previous forms and the update instructions are the same. If you installed the mcp server with the `npx` command and  `-y @azure/mcp@latest` args, npx will check for package updates each time VS Code starts the server. Using a docker container in VS Code has the same no-update limitation described above.\n\u003c/details\u003e\n\n## ⚙️ Advanced Install Scenarios (Optional)\n\n\u003cdetails\u003e\n\u003csummary\u003eDocker containers, custom MCP clients, and manual install options\u003c/summary\u003e\n\n### 🐋 Docker Install Steps (Optional)\n\nMicrosoft publishes an official Azure MCP Server Docker container on the [Microsoft Artifact Registry](https://mcr.microsoft.com/artifact/mar/azure-sdk/azure-mcp).\n\nFor a step-by-step Docker installation, follow these instructions:\n\n1. Create an `.env` file with environment variables that [match one of the `EnvironmentCredential`](https://learn.microsoft.com/dotnet/api/azure.identity.environmentcredential) sets.  For example, a `.env` file using a service principal could look like:\n\n    ```bash\n    AZURE_TENANT_ID={YOUR_AZURE_TENANT_ID}\n    AZURE_CLIENT_ID={YOUR_AZURE_CLIENT_ID}\n    AZURE_CLIENT_SECRET={YOUR_AZURE_CLIENT_SECRET}\n    ```\n\n2. Add `.vscode/mcp.json` or update existing MCP configuration. Replace `/full/path/to/.env` with a path to your `.env` file.\n\n    ```json\n    {\n      \"servers\": {\n        \"Azure MCP Server\": {\n          \"command\": \"docker\",\n          \"args\": [\n            \"run\",\n            \"-i\",\n            \"--rm\",\n            \"--env-file\",\n            \"/full/path/to/.env\"\n            \"mcr.microsoft.com/azure-sdk/azure-mcp:latest\",\n          ]\n        }\n      }\n    }\n    ```\n\nOptionally, use `--env` or `--volume` to pass authentication values.\n\n### 🤖 Custom MCP Client Install Steps (Optional)\n\nYou can easily configure your MCP client to use the Azure MCP Server. Have your client run the following command and access it via standard IO.\n\n```bash\nnpx -y @azure/mcp@latest server start\n```\n\n### 🔧 Manual Install Steps (Optional)\n\nFor a step-by-step installation, follow these instructions:\n\n1. Add `.vscode/mcp.json`:\n\n    ```json\n    {\n      \"servers\": {\n        \"Azure MCP Server\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\",\n            \"@azure/mcp@latest\",\n            \"server\",\n            \"start\"\n          ]\n        }\n      }\n    }\n    ```\n\n    You can optionally set the `--namespace \u003cnamespace\u003e` flag to install tools for the specified Azure product or service.\n\n1. Add `.vscode/mcp.json`:\n\n    ```json\n    {\n      \"servers\": {\n        \"Azure Best Practices\": {\n          \"command\": \"npx\",\n          \"args\": [\n            \"-y\",\n            \"@azure/mcp@latest\",\n            \"server\",\n            \"start\",\n            \"--namespace\",\n            \"bestpractices\" // Any of the available MCP servers can be referenced here.\n          ]\n        }\n      }\n    }\n    ```\n\nMore end-to-end MCP client/agent guides are coming soon!\n\u003c/details\u003e\n\n## Data Collection\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's [privacy statement](https://www.microsoft.com/privacy/privacystatement). You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n### Telemetry Configuration\n\nTelemetry collection is on by default.\n\nTo opt out, set the environment variable `AZURE_MCP_COLLECT_TELEMETRY` to `false` in your environment.\n\n## 📝 Troubleshooting\n\nSee [Troubleshooting guide](https://github.com/Azure/azure-mcp/blob/main/TROUBLESHOOTING.md#128-tool-limit-issue) for help with common issues and logging.\n\n### 🔑 Authentication\n\n\u003cdetails\u003e\n\u003csummary\u003eAuthentication options including DefaultAzureCredential flow, RBAC permissions, troubleshooting, and production credentials\u003c/summary\u003e\n\nThe Azure MCP Server uses the Azure Identity library for .NET to authenticate to Microsoft Entra ID. For detailed information, see [Authentication Fundamentals](https://github.com/Azure/azure-mcp/blob/main/docs/Authentication.md#authentication-fundamentals).\n\nIf you're running into any issues with authentication, visit our [troubleshooting guide](https://github.com/Azure/azure-mcp/blob/main/TROUBLESHOOTING.md#authentication).\n\nFor enterprise authentication scenarios, including network restrictions, security policies, and protected resources, see [Authentication Scenarios in Enterprise Environments](https://github.com/Azure/azure-mcp/blob/main/docs/Authentication.md#authentication-scenarios-in-enterprise-environments).\n\u003c/details\u003e\n\n## 🛡️ Security Note\n\nYour credentials are always handled securely through the official [Azure Identity SDK](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/identity/Azure.Identity/README.md) - **we never store or manage tokens directly**.\n\nMCP as a phenomenon is very novel and cutting-edge. As with all new technology standards, consider doing a security review to ensure any systems that integrate with MCP servers follow all regulations and standards your system is expected to adhere to. This includes not only the Azure MCP Server, but any MCP client/agent that you choose to implement down to the model provider.\n\n## 👥 Contributing\n\nWe welcome contributions to the Azure MCP Server! Whether you're fixing bugs, adding new features, or improving documentation, your contributions are welcome.\n\nPlease read our [Contributing Guide](https://github.com/Azure/azure-mcp/blob/main/CONTRIBUTING.md) for guidelines on:\n\n* 🛠️ Setting up your development environment\n* ✨ Adding new commands\n* 📝 Code style and testing requirements\n* 🔄 Making pull requests\n\n## 🤝 Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [open@microsoft.com](mailto:open@microsoft.com)\nwith any additional questions or comments.\n","stargazer_count":1166,"uses_custom_opengraph_image":false}}}},{"name":"hashicorp/terraform-mcp-server","description":"Seamlessly integrate with Terraform ecosystem, enabling advanced automation and interaction capabilities for Infrastructure as Code (IaC) development powered by Terraform","status":"active","repository":{"url":"https://github.com/hashicorp/terraform-mcp-server","source":"github","id":"969282615","readme":"# \u003cimg src=\"public/images/Terraform-LogoMark_onDark.svg\" width=\"30\" align=\"left\" style=\"margin-right: 12px;\"/\u003e Terraform MCP Server\n\nThe Terraform MCP Server is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)\nserver that provides seamless integration with Terraform Registry APIs, enabling advanced\nautomation and interaction capabilities for Infrastructure as Code (IaC) development.\n\n## Features\n\n- **Dual Transport Support**: Both Stdio and StreamableHTTP transports with configurable endpoints\n- **Terraform Registry Integration**: Direct integration with public Terraform Registry APIs for providers, modules, and policies\n- **HCP Terraform \u0026 Terraform Enterprise Support**: Full workspace management, organization/project listing, and private registry access\n- **Workspace Operations**: Create, update, delete workspaces with support for variables, tags, and run management\n\n\u003e **Security Note:** At this stage, the MCP server is intended for local use only. If using the StreamableHTTP transport, always configure the MCP_ALLOWED_ORIGINS environment variable to restrict access to trusted origins only. This helps prevent DNS rebinding attacks and other cross-origin vulnerabilities.\n\n\u003e **Security Note:** Depending on the query, the MCP server may expose certain Terraform data to the MCP client and LLM. Do not use the MCP server with untrusted MCP clients or LLMs.\n\n\u003e **Legal Note:** Your use of a third party MCP Client/LLM is subject solely to the terms of use for such MCP/LLM, and IBM is not responsible for the performance of such third party tools. IBM expressly disclaims any and all warranties and liability for third party MCP Clients/LLMs, and may not be able to provide support to resolve issues which are caused by the third party tools.\n\n\u003e **Caution:**  The outputs and recommendations provided by the MCP server are generated dynamically and may vary based on the query, model, and the connected MCP client. Users should thoroughly review all outputs/recommendations to ensure they align with their organization’s security best practices, cost-efficiency goals, and compliance requirements before implementation.\n\n## Prerequisites\n\n1. Ensure [Docker](https://www.docker.com/) is installed and running to use the server in a containerized environment.\n1. Install an AI assistant that supports the Model Context Protocol (MCP).\n\n## Command Line Options\n\n**Environment Variables:**\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `TFE_ADDRESS` | HCP Terraform or TFE address | `\"https://app.terraform.io\"` |\n| `TFE_TOKEN` | Terraform Enterprise API token | `\"\"` (empty) |\n| `TFE_SKIP_TLS_VERIFY` | Skip HCP Terraform or Terraform Enterprise TLS verification | `false` |\n| `TRANSPORT_MODE` | Set to `streamable-http` to enable HTTP transport (legacy `http` value still supported) | `stdio` |\n| `TRANSPORT_HOST` | Host to bind the HTTP server | `127.0.0.1` |\n| `TRANSPORT_PORT` | HTTP server port | `8080` |\n| `MCP_ENDPOINT` | HTTP server endpoint path | `/mcp` |\n| `MCP_SESSION_MODE` | Session mode: `stateful` or `stateless` | `stateful` |\n| `MCP_ALLOWED_ORIGINS` | Comma-separated list of allowed origins for CORS | `\"\"` (empty) |\n| `MCP_CORS_MODE` | CORS mode: `strict`, `development`, or `disabled` | `strict` |\n| `MCP_TLS_CERT_FILE` | Path to TLS cert file, required for non-localhost deployment (e.g. `/path/to/cert.pem`) | `\"\"` (empty) |\n| `MCP_TLS_KEY_FILE` |  Path to TLS key file, required for non-localhost deployment (e.g. `/path/to/key.pem`)| `\"\"` (empty) |\n| `MCP_RATE_LIMIT_GLOBAL` | Global rate limit (format: `rps:burst`) | `10:20` |\n| `MCP_RATE_LIMIT_SESSION` | Per-session rate limit (format: `rps:burst`) | `5:10` |\n| `ENABLE_TF_OPERATIONS` | Enable tools that require explicit approval | `false` |\n\n```bash\n# Stdio mode\nterraform-mcp-server stdio [--log-file /path/to/log]\n\n# StreamableHTTP mode\nterraform-mcp-server streamable-http [--transport-port 8080] [--transport-host 127.0.0.1] [--mcp-endpoint /mcp] [--log-file /path/to/log]\n```\n\n## Instructions\n\nDefault instructions for the MCP server is located in `cmd/terraform-mcp-server/instructions.md`, if those do not seem appropriate for your organization's Terraform practices or if the MCP server is producing inaccurate responses, please replace them with your own instructions and rebuild the container or binary. An example of such instruction is located in `instructions/example-mcp-instructions.md`\n\n`AGENTS.md` essentially behaves as READMEs for coding agents: a dedicated, predictable place to provide the context and instructions to help AI coding agents work on your project. One `AGENTS.md` file works with different coding agents. An example of such instruction is located in `instructions/example-AGENTS.md`, in order to use it commit a file name `AGENTS.md` to the directory where your Terraform configurations reside.\n\n## Installation\n\n### Usage with Visual Studio Code\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nMore about using MCP server tools in VS Code's [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\", \"TFE_TOKEN=${input:tfe_token}\",\n          \"-e\", \"TFE_ADDRESS=${input:tfe_address}\",\n          \"hashicorp/terraform-mcp-server:0.3.0\"\n        ]\n      }\n    },\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"tfe_token\",\n        \"description\": \"Terraform API Token\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"tfe_address\",\n        \"description\": \"Terraform Address\",\n        \"password\": false\n      }\n    ]\n  }\n}\n```\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"hashicorp/terraform-mcp-server:0.2.3\"\n        ]\n      }\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\nOptionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"TFE_TOKEN=${input:tfe_token}\",\n        \"-e\", \"TFE_ADDRESS=${input:tfe_address}\",\n        \"hashicorp/terraform-mcp-server:0.3.0\"\n      ]\n    }\n  },\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"tfe_token\",\n      \"description\": \"Terraform API Token\",\n      \"password\": true\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"tfe_address\",\n      \"description\": \"Terraform Address\",\n      \"password\": false\n    }\n  ]\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server:0.2.3\"\n      ]\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n\n[\u003cimg alt=\"Install in VS Code (docker)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Terraform%20MCP\u0026color=0098FF\"\u003e](https://vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22terraform%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22hashicorp%2Fterraform-mcp-server%22%5D%7D)\n[\u003cimg alt=\"Install in VS Code Insiders (docker)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Terraform%20MCP\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22terraform%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22hashicorp%2Fterraform-mcp-server%22%5D%7D)\n\n### Usage with Cursor\n\nAdd this to your Cursor config (`~/.cursor/mcp.json`) or via Settings → Cursor Settings → MCP:\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"TFE_ADDRESS=\u003c\u003cPASTE_TFE_ADDRESS_HERE\u003e\u003e\",\n        \"-e\", \"TFE_TOKEN=\u003c\u003cPASTE_TFE_TOKEN_HERE\u003e\u003e\",\n        \"hashicorp/terraform-mcp-server:0.3.0\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server:0.2.3\"\n      ]\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n\u003ca href=\"cursor://anysphere.cursor-deeplink/mcp/install?name=terraform\u0026config=eyJjb21tYW5kIjoiZG9ja2VyIiwiYXJncyI6WyJydW4iLCItaSIsIi0tcm0iLCJoYXNoaWNvcnAvdGVycmFmb3JtLW1jcC1zZXJ2ZXIiXX0%3D\"\u003e\n  \u003cimg alt=\"Add terraform MCP server to Cursor\" src=\"https://cursor.com/deeplink/mcp-install-dark.png\" height=\"32\" /\u003e\n\u003c/a\u003e\n\n### Usage with Claude Desktop / Amazon Q Developer / Amazon Q CLI\n\nMore about using MCP server tools in Claude Desktop [user documentation](https://modelcontextprotocol.io/quickstart/user). Read more about using MCP server in Amazon Q from the [documentation](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/qdev-mcp.html).\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"TFE_ADDRESS=\u003c\u003cPASTE_TFE_ADDRESS_HERE\u003e\u003e\",\n        \"-e\", \"TFE_TOKEN=\u003c\u003cPASTE_TFE_TOKEN_HERE\u003e\u003e\",\n        \"hashicorp/terraform-mcp-server:0.3.0\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server:0.2.3\"\n      ]\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n### Usage with Claude Code\n\nMore about using and adding MCP server tools in Claude Code [user documentation](https://docs.claude.com/en/docs/claude-code/mcp)\n\n- Local (`stdio`) Transport\n\n```sh\nclaude mcp add terraform -s user -t stdio -- docker run -i --rm hashicorp/terraform-mcp-server\n```\n\n- Remote (`streamable-http`) Transport\n\n```sh\n# Run server (example)\ndocker run -p 8080:8080 --rm -e TRANSPORT_MODE=streamable-http -e TRANSPORT_HOST=0.0.0.0 hashicorp/terraform-mcp-server\n\n# Add to Claude Code\nclaude mcp add --transport http terraform http://localhost:8080/mcp\n```\n\n### Usage with Gemini extensions\n\nFor security, avoid hardcoding your credentials, create or update `~/.gemini/.env` (where ~ is your home or project directory) for storing HCP Terraform or Terraform Enterprise credentials\n\n```\n# ~/.gemini/.env\nTFE_ADDRESS=your_tfe_address_here\nTFE_TOKEN=your_tfe_token_here\n```\n\nInstall the extension \u0026 run Gemini\n\n```\ngemini extensions install https://github.com/hashicorp/terraform-mcp-server\ngemini\n```\n\n## Install from source\n\nUse the latest release version:\n\n```console\ngo install github.com/hashicorp/terraform-mcp-server/cmd/terraform-mcp-server@latest\n```\n\nUse the main branch:\n\n```console\ngo install github.com/hashicorp/terraform-mcp-server/cmd/terraform-mcp-server@main\n```\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"type\": \"stdio\",\n        \"command\": \"/path/to/terraform-mcp-server\",\n        \"env\": {\n          \"TFE_TOKEN\": \"\u003c\u003cTFE_TOKEN_HERE\u003e\u003e\"\n        },\n      }\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"type\": \"stdio\",\n        \"command\": \"/path/to/terraform-mcp-server\"\n      }\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n## Building the Docker Image locally\n\nBefore using the server, you need to build the Docker image locally:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/hashicorp/terraform-mcp-server.git\ncd terraform-mcp-server\n```\n\n2. Build the Docker image:\n```bash\nmake docker-build\n```\n\n3. This will create a local Docker image that you can use in the following configuration.\n\n```bash\n# Run in stdio mode\ndocker run -i --rm terraform-mcp-server:dev\n\n# Run in streamable-http mode\ndocker run -p 8080:8080 --rm -e TRANSPORT_MODE=streamable-http -e TRANSPORT_HOST=0.0.0.0 terraform-mcp-server:dev\n```\n\n\u003e **Note:** When running in Docker, you should set `TRANSPORT_HOST=0.0.0.0` to allow connections from outside the container.\n\n4. (Optional) Test connection in http mode\n\n```bash\n# Test the connection\ncurl http://localhost:8080/health\n```\n\n5. You can use it on your AI assistant as follow:\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"terraform-mcp-server:dev\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\n[Check out available tools here :link:](https://developer.hashicorp.com/terraform/docs/tools/mcp-server/reference#available-tools)\n\n## Available Resources\n\n[Check out available resources here :link:](https://developer.hashicorp.com/terraform/docs/tools/mcp-server/reference#available-tools)\n\n## Transport Support\n\nThe Terraform MCP Server supports multiple transport protocols:\n\n### 1. Stdio Transport (Default)\nStandard input/output communication using JSON-RPC messages. Ideal for local development and direct integration with MCP clients.\n\n### 2. StreamableHTTP Transport\nModern HTTP-based transport supporting both direct HTTP requests and Server-Sent Events (SSE) streams. This is the recommended transport for remote/distributed setups.\n\n**Features:**\n- **Endpoint**: `http://{hostname}:8080/mcp`\n- **Health Check**: `http://{hostname}:8080/health`\n- **Environment Configuration**: Set `TRANSPORT_MODE=http` or `TRANSPORT_PORT=8080` to enable\n\n## Session Modes\n\nThe Terraform MCP Server supports two session modes when using the StreamableHTTP transport:\n\n- **Stateful Mode (Default)**: Maintains session state between requests, enabling context-aware operations.\n- **Stateless Mode**: Each request is processed independently without maintaining session state, which can be useful for high-availability deployments or when using load balancers.\n\nTo enable stateless mode, set the environment variable:\n```bash\nexport MCP_SESSION_MODE=stateless\n```\n\n## Development\n\n### Prerequisites\n- Go (check [go.mod](./go.mod) file for specific version)\n- Docker (optional, for container builds)\n\n### Available Make Commands\n\n| Command | Description |\n|---------|-------------|\n| `make build` | Build the binary |\n| `make test` | Run all tests |\n| `make test-e2e` | Run end-to-end tests |\n| `make docker-build` | Build Docker image |\n| `make run-http` | Run HTTP server locally |\n| `make docker-run-http` | Run HTTP server in Docker |\n| `make test-http` | Test HTTP health endpoint |\n| `make clean` | Remove build artifacts |\n| `make help` | Show all available commands |\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Make your changes\n4. Run tests\n5. Submit a pull request\n\n## License\n\nThis project is licensed under the terms of the MPL-2.0 open source license. Please refer to [LICENSE](./LICENSE) file for the full terms.\n\n## Security\n\nFor security issues, please contact security@hashicorp.com or follow our [security policy](https://www.hashicorp.com/en/trust/security/vulnerability-management).\n\n## Support\n\nFor bug reports and feature requests, please open an issue on GitHub.\n\nFor general questions and discussions, open a GitHub Discussion.\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:42Z","updated_at":"2025-10-22T11:20:58Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"docker","identifier":"hashicorp/terraform-mcp-server","version":"latest","runtime_hint":"docker","runtime_arguments":[{"is_required":true,"format":"string","value":"run","type":"positional","name":"run"},{"is_required":true,"format":"string","type":"named","name":"-i","value_hint":"interactive_flag"},{"is_required":true,"format":"string","type":"named","name":"--rm","value_hint":"remove_flag"},{"format":"string","type":"named","name":"-e","value_hint":"environment_variable_flag"},{"format":"string","value":"TFE_TOKEN={tfe_token}","variables":{"tfe_token":{"description":"Terraform API Token","is_secret":true}},"type":"positional","name":"TFE_TOKEN","value_hint":"environment_variable_value"},{"format":"string","type":"named","name":"-e","value_hint":"environment_variable_flag"},{"format":"string","value":"TFE_HOSTNAME={tfe_hostname}","variables":{"tfe_hostname":{"description":"Terraform hostname"}},"type":"positional","name":"TFE_HOSTNAME","value_hint":"environment_variable_value"},{"is_required":true,"format":"string","value":"hashicorp/terraform-mcp-server","type":"positional","name":"hashicorp/terraform-mcp-server"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"34cd3839-461a-404a-a290-3d3bc9d8bee3","is_latest":true,"published_at":"2025-09-09T10:55:22.910353Z","updated_at":"2025-09-09T10:55:22.910353Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Terraform","is_in_organization":true,"license":"Mozilla Public License 2.0","name":"terraform-mcp-server","name_with_owner":"hashicorp/terraform-mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/b95660f7913599ebf1037ef9288055af1b40679a943f0666fef92af66f5eb3b5/hashicorp/terraform-mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/761456?v=4","preferred_image":"https://avatars.githubusercontent.com/u/761456?v=4","primary_language":"Go","primary_language_color":"#00ADD8","pushed_at":"2025-10-21T12:24:38Z","readme":"# \u003cimg src=\"public/images/Terraform-LogoMark_onDark.svg\" width=\"30\" align=\"left\" style=\"margin-right: 12px;\"/\u003e Terraform MCP Server\n\nThe Terraform MCP Server is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)\nserver that provides seamless integration with Terraform Registry APIs, enabling advanced\nautomation and interaction capabilities for Infrastructure as Code (IaC) development.\n\n## Features\n\n- **Dual Transport Support**: Both Stdio and StreamableHTTP transports with configurable endpoints\n- **Terraform Registry Integration**: Direct integration with public Terraform Registry APIs for providers, modules, and policies\n- **HCP Terraform \u0026 Terraform Enterprise Support**: Full workspace management, organization/project listing, and private registry access\n- **Workspace Operations**: Create, update, delete workspaces with support for variables, tags, and run management\n\n\u003e **Security Note:** At this stage, the MCP server is intended for local use only. If using the StreamableHTTP transport, always configure the MCP_ALLOWED_ORIGINS environment variable to restrict access to trusted origins only. This helps prevent DNS rebinding attacks and other cross-origin vulnerabilities.\n\n\u003e **Security Note:** Depending on the query, the MCP server may expose certain Terraform data to the MCP client and LLM. Do not use the MCP server with untrusted MCP clients or LLMs.\n\n\u003e **Legal Note:** Your use of a third party MCP Client/LLM is subject solely to the terms of use for such MCP/LLM, and IBM is not responsible for the performance of such third party tools. IBM expressly disclaims any and all warranties and liability for third party MCP Clients/LLMs, and may not be able to provide support to resolve issues which are caused by the third party tools.\n\n\u003e **Caution:**  The outputs and recommendations provided by the MCP server are generated dynamically and may vary based on the query, model, and the connected MCP client. Users should thoroughly review all outputs/recommendations to ensure they align with their organization’s security best practices, cost-efficiency goals, and compliance requirements before implementation.\n\n## Prerequisites\n\n1. Ensure [Docker](https://www.docker.com/) is installed and running to use the server in a containerized environment.\n1. Install an AI assistant that supports the Model Context Protocol (MCP).\n\n## Command Line Options\n\n**Environment Variables:**\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `TFE_ADDRESS` | HCP Terraform or TFE address | `\"https://app.terraform.io\"` |\n| `TFE_TOKEN` | Terraform Enterprise API token | `\"\"` (empty) |\n| `TFE_SKIP_TLS_VERIFY` | Skip HCP Terraform or Terraform Enterprise TLS verification | `false` |\n| `TRANSPORT_MODE` | Set to `streamable-http` to enable HTTP transport (legacy `http` value still supported) | `stdio` |\n| `TRANSPORT_HOST` | Host to bind the HTTP server | `127.0.0.1` |\n| `TRANSPORT_PORT` | HTTP server port | `8080` |\n| `MCP_ENDPOINT` | HTTP server endpoint path | `/mcp` |\n| `MCP_SESSION_MODE` | Session mode: `stateful` or `stateless` | `stateful` |\n| `MCP_ALLOWED_ORIGINS` | Comma-separated list of allowed origins for CORS | `\"\"` (empty) |\n| `MCP_CORS_MODE` | CORS mode: `strict`, `development`, or `disabled` | `strict` |\n| `MCP_TLS_CERT_FILE` | Path to TLS cert file, required for non-localhost deployment (e.g. `/path/to/cert.pem`) | `\"\"` (empty) |\n| `MCP_TLS_KEY_FILE` |  Path to TLS key file, required for non-localhost deployment (e.g. `/path/to/key.pem`)| `\"\"` (empty) |\n| `MCP_RATE_LIMIT_GLOBAL` | Global rate limit (format: `rps:burst`) | `10:20` |\n| `MCP_RATE_LIMIT_SESSION` | Per-session rate limit (format: `rps:burst`) | `5:10` |\n| `ENABLE_TF_OPERATIONS` | Enable tools that require explicit approval | `false` |\n\n```bash\n# Stdio mode\nterraform-mcp-server stdio [--log-file /path/to/log]\n\n# StreamableHTTP mode\nterraform-mcp-server streamable-http [--transport-port 8080] [--transport-host 127.0.0.1] [--mcp-endpoint /mcp] [--log-file /path/to/log]\n```\n\n## Instructions\n\nDefault instructions for the MCP server is located in `cmd/terraform-mcp-server/instructions.md`, if those do not seem appropriate for your organization's Terraform practices or if the MCP server is producing inaccurate responses, please replace them with your own instructions and rebuild the container or binary. An example of such instruction is located in `instructions/example-mcp-instructions.md`\n\n`AGENTS.md` essentially behaves as READMEs for coding agents: a dedicated, predictable place to provide the context and instructions to help AI coding agents work on your project. One `AGENTS.md` file works with different coding agents. An example of such instruction is located in `instructions/example-AGENTS.md`, in order to use it commit a file name `AGENTS.md` to the directory where your Terraform configurations reside.\n\n## Installation\n\n### Usage with Visual Studio Code\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nMore about using MCP server tools in VS Code's [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\", \"TFE_TOKEN=${input:tfe_token}\",\n          \"-e\", \"TFE_ADDRESS=${input:tfe_address}\",\n          \"hashicorp/terraform-mcp-server:0.3.0\"\n        ]\n      }\n    },\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"tfe_token\",\n        \"description\": \"Terraform API Token\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"tfe_address\",\n        \"description\": \"Terraform Address\",\n        \"password\": false\n      }\n    ]\n  }\n}\n```\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"hashicorp/terraform-mcp-server:0.2.3\"\n        ]\n      }\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\nOptionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"TFE_TOKEN=${input:tfe_token}\",\n        \"-e\", \"TFE_ADDRESS=${input:tfe_address}\",\n        \"hashicorp/terraform-mcp-server:0.3.0\"\n      ]\n    }\n  },\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"tfe_token\",\n      \"description\": \"Terraform API Token\",\n      \"password\": true\n    },\n    {\n      \"type\": \"promptString\",\n      \"id\": \"tfe_address\",\n      \"description\": \"Terraform Address\",\n      \"password\": false\n    }\n  ]\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server:0.2.3\"\n      ]\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n\n[\u003cimg alt=\"Install in VS Code (docker)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square\u0026label=Install%20Terraform%20MCP\u0026color=0098FF\"\u003e](https://vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22terraform%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22hashicorp%2Fterraform-mcp-server%22%5D%7D)\n[\u003cimg alt=\"Install in VS Code Insiders (docker)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square\u0026label=Install%20Terraform%20MCP\u0026color=24bfa5\"\u003e](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22terraform%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22hashicorp%2Fterraform-mcp-server%22%5D%7D)\n\n### Usage with Cursor\n\nAdd this to your Cursor config (`~/.cursor/mcp.json`) or via Settings → Cursor Settings → MCP:\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"TFE_ADDRESS=\u003c\u003cPASTE_TFE_ADDRESS_HERE\u003e\u003e\",\n        \"-e\", \"TFE_TOKEN=\u003c\u003cPASTE_TFE_TOKEN_HERE\u003e\u003e\",\n        \"hashicorp/terraform-mcp-server:0.3.0\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"servers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server:0.2.3\"\n      ]\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n\u003ca href=\"cursor://anysphere.cursor-deeplink/mcp/install?name=terraform\u0026config=eyJjb21tYW5kIjoiZG9ja2VyIiwiYXJncyI6WyJydW4iLCItaSIsIi0tcm0iLCJoYXNoaWNvcnAvdGVycmFmb3JtLW1jcC1zZXJ2ZXIiXX0%3D\"\u003e\n  \u003cimg alt=\"Add terraform MCP server to Cursor\" src=\"https://cursor.com/deeplink/mcp-install-dark.png\" height=\"32\" /\u003e\n\u003c/a\u003e\n\n### Usage with Claude Desktop / Amazon Q Developer / Amazon Q CLI\n\nMore about using MCP server tools in Claude Desktop [user documentation](https://modelcontextprotocol.io/quickstart/user). Read more about using MCP server in Amazon Q from the [documentation](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/qdev-mcp.html).\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\", \"TFE_ADDRESS=\u003c\u003cPASTE_TFE_ADDRESS_HERE\u003e\u003e\",\n        \"-e\", \"TFE_TOKEN=\u003c\u003cPASTE_TFE_TOKEN_HERE\u003e\u003e\",\n        \"hashicorp/terraform-mcp-server:0.3.0\"\n      ]\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server:0.2.3\"\n      ]\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n### Usage with Claude Code\n\nMore about using and adding MCP server tools in Claude Code [user documentation](https://docs.claude.com/en/docs/claude-code/mcp)\n\n- Local (`stdio`) Transport\n\n```sh\nclaude mcp add terraform -s user -t stdio -- docker run -i --rm hashicorp/terraform-mcp-server\n```\n\n- Remote (`streamable-http`) Transport\n\n```sh\n# Run server (example)\ndocker run -p 8080:8080 --rm -e TRANSPORT_MODE=streamable-http -e TRANSPORT_HOST=0.0.0.0 hashicorp/terraform-mcp-server\n\n# Add to Claude Code\nclaude mcp add --transport http terraform http://localhost:8080/mcp\n```\n\n### Usage with Gemini extensions\n\nFor security, avoid hardcoding your credentials, create or update `~/.gemini/.env` (where ~ is your home or project directory) for storing HCP Terraform or Terraform Enterprise credentials\n\n```\n# ~/.gemini/.env\nTFE_ADDRESS=your_tfe_address_here\nTFE_TOKEN=your_tfe_token_here\n```\n\nInstall the extension \u0026 run Gemini\n\n```\ngemini extensions install https://github.com/hashicorp/terraform-mcp-server\ngemini\n```\n\n## Install from source\n\nUse the latest release version:\n\n```console\ngo install github.com/hashicorp/terraform-mcp-server/cmd/terraform-mcp-server@latest\n```\n\nUse the main branch:\n\n```console\ngo install github.com/hashicorp/terraform-mcp-server/cmd/terraform-mcp-server@main\n```\n\n\u003ctable\u003e\n\u003ctr\u003e\u003cth\u003eVersion 0.3.0+ or greater\u003c/th\u003e\u003cth\u003eVersion 0.2.3 or lower\u003c/th\u003e\u003c/tr\u003e\n\u003ctr valign=top\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"type\": \"stdio\",\n        \"command\": \"/path/to/terraform-mcp-server\",\n        \"env\": {\n          \"TFE_TOKEN\": \"\u003c\u003cTFE_TOKEN_HERE\u003e\u003e\"\n        },\n      }\n    }\n  }\n}\n```\n\n\u003c/td\u003e\n\u003ctd\u003e\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"type\": \"stdio\",\n        \"command\": \"/path/to/terraform-mcp-server\"\n      }\n    }\n  }\n}\n```\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\n## Building the Docker Image locally\n\nBefore using the server, you need to build the Docker image locally:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/hashicorp/terraform-mcp-server.git\ncd terraform-mcp-server\n```\n\n2. Build the Docker image:\n```bash\nmake docker-build\n```\n\n3. This will create a local Docker image that you can use in the following configuration.\n\n```bash\n# Run in stdio mode\ndocker run -i --rm terraform-mcp-server:dev\n\n# Run in streamable-http mode\ndocker run -p 8080:8080 --rm -e TRANSPORT_MODE=streamable-http -e TRANSPORT_HOST=0.0.0.0 terraform-mcp-server:dev\n```\n\n\u003e **Note:** When running in Docker, you should set `TRANSPORT_HOST=0.0.0.0` to allow connections from outside the container.\n\n4. (Optional) Test connection in http mode\n\n```bash\n# Test the connection\ncurl http://localhost:8080/health\n```\n\n5. You can use it on your AI assistant as follow:\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"terraform-mcp-server:dev\"\n      ]\n    }\n  }\n}\n```\n\n## Available Tools\n\n[Check out available tools here :link:](https://developer.hashicorp.com/terraform/docs/tools/mcp-server/reference#available-tools)\n\n## Available Resources\n\n[Check out available resources here :link:](https://developer.hashicorp.com/terraform/docs/tools/mcp-server/reference#available-tools)\n\n## Transport Support\n\nThe Terraform MCP Server supports multiple transport protocols:\n\n### 1. Stdio Transport (Default)\nStandard input/output communication using JSON-RPC messages. Ideal for local development and direct integration with MCP clients.\n\n### 2. StreamableHTTP Transport\nModern HTTP-based transport supporting both direct HTTP requests and Server-Sent Events (SSE) streams. This is the recommended transport for remote/distributed setups.\n\n**Features:**\n- **Endpoint**: `http://{hostname}:8080/mcp`\n- **Health Check**: `http://{hostname}:8080/health`\n- **Environment Configuration**: Set `TRANSPORT_MODE=http` or `TRANSPORT_PORT=8080` to enable\n\n## Session Modes\n\nThe Terraform MCP Server supports two session modes when using the StreamableHTTP transport:\n\n- **Stateful Mode (Default)**: Maintains session state between requests, enabling context-aware operations.\n- **Stateless Mode**: Each request is processed independently without maintaining session state, which can be useful for high-availability deployments or when using load balancers.\n\nTo enable stateless mode, set the environment variable:\n```bash\nexport MCP_SESSION_MODE=stateless\n```\n\n## Development\n\n### Prerequisites\n- Go (check [go.mod](./go.mod) file for specific version)\n- Docker (optional, for container builds)\n\n### Available Make Commands\n\n| Command | Description |\n|---------|-------------|\n| `make build` | Build the binary |\n| `make test` | Run all tests |\n| `make test-e2e` | Run end-to-end tests |\n| `make docker-build` | Build Docker image |\n| `make run-http` | Run HTTP server locally |\n| `make docker-run-http` | Run HTTP server in Docker |\n| `make test-http` | Test HTTP health endpoint |\n| `make clean` | Remove build artifacts |\n| `make help` | Show all available commands |\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Make your changes\n4. Run tests\n5. Submit a pull request\n\n## License\n\nThis project is licensed under the terms of the MPL-2.0 open source license. Please refer to [LICENSE](./LICENSE) file for the full terms.\n\n## Security\n\nFor security issues, please contact security@hashicorp.com or follow our [security policy](https://www.hashicorp.com/en/trust/security/vulnerability-management).\n\n## Support\n\nFor bug reports and feature requests, please open an issue on GitHub.\n\nFor general questions and discussions, open a GitHub Discussion.\n","stargazer_count":1013,"uses_custom_opengraph_image":false}}}},{"name":"microsoftdocs/mcp","description":"Enables clients like GitHub Copilot and other AI agents to bring trusted and up-to-date information directly from Microsoft's official documentation.","status":"active","repository":{"url":"https://github.com/microsoftdocs/mcp","source":"github","id":"998658053","readme":"# 🌟 Microsoft Learn MCP Server\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Microsoft_Docs_MCP-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Microsoft_Docs_MCP-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D\u0026quality=insiders)\n\nThe Microsoft Learn MCP Server is a remote MCP Server that enables clients like GitHub Copilot and other AI agents to bring trusted and up-to-date information directly from Microsoft's official documentation. It supports streamable http transport, which is lightweight for clients to use.\n\n\u003e Please note that this project is in Public Preview and implementation may significantly change prior to our General Availability.\n\n## 📑 Table of contents\n1. [🎯 Overview](#-overview)\n2. [🌐 The Microsoft Learn MCP Server Endpoint](#-the-microsoft-learn-mcp-server-endpoint)\n3. [🛠️ Currently Supported Tools](#%EF%B8%8F-currently-supported-tools)\n4. [🔌 Installation \u0026 Getting Started](#-installation--getting-started)\n5. [❓ Troubleshooting](#-troubleshooting)\n6. [🔮 Future Enhancements](#-future-enhancements)\n7. [📚 Additional Resources](#-additional-resources)\n\n## 🎯 Overview\n\n### ✨ Example Prompts: Your Source of Truth\n\nYour AI assistant should automatically use these tools for Microsoft-related topics. With both search and fetch capabilities, you can get quick answers or comprehensive deep dives. To ensure that it always consults the official documentation, you can add phrases like `search Microsoft docs`, `deep dive`, `fetch full doc`.\n\n#### **Quick Search \u0026 Reference**\n\n\u003e \"Give me the Azure CLI commands to create an Azure Container App with a managed identity. **search Microsoft docs**\"\n\n\u003e \"Is gpt-4.1-mini available in EU regions? **fetch full doc**\"\n\n#### **Code Verification \u0026 Best Practices**\n\n\u003e \"Are you sure this is the right way to implement `IHttpClientFactory` in a .NET 8 minimal API? **search Microsoft docs and fetch full doc**\"\n\n\u003e \"Show me the complete guide for implementing authentication in ASP.NET Core. **fetch full doc**\"\n\n\u003e \"show me detailed, runnable python code sample to do harms eval using azure ai foundry evaluation sdk\"\n\n#### **Comprehensive Learning \u0026 Deep Dive**\n\n\u003e \"I need to understand Azure Functions end-to-end. **search Microsoft docs and deep dive**\"\n\n\u003e \"Get me the full step-by-step tutorial for deploying a .NET application to Azure App Service. **search Microsoft docs and deep dive**\"\n\n### 📊 Key Capabilities\n\n- **High-Quality Content Retrieval**: Search and retrieve relevant content from Microsoft's official documentation in markdown format.\n- **Code Sample Discovery**: Find official Microsoft/Azure code snippets and examples with language-specific filtering.\n- **Semantic Understanding**: Uses advanced vector search to find the most contextually relevant documentation for any query.\n- **Real-time Updates**: Access the latest Microsoft documentation as it's published.\n\n## 🌐 The Microsoft Learn MCP Server Endpoint\n\nThe Microsoft Learn MCP Server is accessible to any IDE, agent, or tool that supports the Model Context Protocol (MCP). Any compatible client can connect to the following **remote MCP endpoint**:\n\n```\nhttps://learn.microsoft.com/api/mcp\n```\n\u003e **Note:** This URL is intended for use **within a compliant MCP client** via Streamable HTTP, such as the recommended clients listed in our [Getting Started](#-installation--getting-started) section. It does not support direct access from a web browser and may return a `405 Method Not Allowed` error if accessed manually. For developers who need to build their own solution, please follow the mandatory guidelines in the [Building a Custom Client](#%EF%B8%8F-building-a-custom-client) section to ensure your implementation is resilient and supported.\n\n**Example JSON configuration:**\n```json\n{\n  \"microsoft.docs.mcp\": {\n    \"type\": \"http\",\n    \"url\": \"https://learn.microsoft.com/api/mcp\"\n  }\n}\n```\n\n## 🛠️ Currently Supported Tools\n\n| Tool Name | Description | Input Parameters |\n|-----------|-------------|------------------|\n| `microsoft_docs_search` | Performs semantic search against Microsoft official technical documentation | `query` (string): The search query for retrieval |\n| `microsoft_docs_fetch` | Fetch and convert a Microsoft documentation page into markdown format | `url` (string): URL of the documentation page to read |\n| `microsoft_code_sample_search` | Search for official Microsoft/Azure code snippets and examples | `query` (string): Search query for Microsoft/Azure code snippets\u003cbr/\u003e`language` (string, optional): Programming language filter.|\n\n\n## 🔌 Installation \u0026 Getting Started\n\nThe Microsoft Learn MCP Server supports quick installation across multiple development environments. Choose your preferred client below for streamlined setup:\n\n| Client | One-click Installation | MCP Guide |\n|--------|----------------------|-------------------|\n| **VS Code** | [![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Microsoft_Docs_MCP-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Microsoft_Docs_MCP-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D\u0026quality=insiders) | [VS Code MCP Official Guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) |\n| **Claude Desktop** | \u003cdetails\u003e\u003csummary\u003eView Instructions\u003c/summary\u003e1. Open Claude Desktop\u003cbr/\u003e2. Go to **Settings → Integrations**\u003cbr/\u003e3. Click **Add Integration**\u003cbr/\u003e4. Enter URL: `https://learn.microsoft.com/api/mcp`\u003cbr/\u003e5. Click **Connect**\u003c/details\u003e | [Claude Desktop Remote MCP Guide](https://support.anthropic.com/en/articles/11503834-building-custom-integrations-via-remote-mcp-servers) |\n| **Claude Code** | \u003cdetails\u003e\u003csummary\u003eView Instructions\u003c/summary\u003e1. Open a CLI\u003cbr/\u003e2. Type `claude mcp add --transport http microsoft_docs_mcp https://learn.microsoft.com/api/mcp` and press enter\u003cbr/\u003e3. (optional) Type `--scope user` directly after `claude mcp add` to make this MCP server available in Claude Code for all of your projects\u003c/details\u003e | [Claude Code Remote MCP Guide](https://docs.anthropic.com/en/docs/claude-code/mcp) |\n| **Visual Studio** | Manual configuration required\u003cbr/\u003eUse `\"type\": \"http\"` | [Visual Studio MCP Official Guide](https://learn.microsoft.com/en-us/visualstudio/ide/mcp-servers?view=vs-2022) |\n| **Cursor IDE** | [![Install in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=microsoft.docs.mcp\u0026config=eyJ0eXBlIjoiaHR0cCIsInVybCI6Imh0dHBzOi8vbGVhcm4ubWljcm9zb2Z0LmNvbS9hcGkvbWNwIn0%3D) | [Cursor MCP Official Guide](https://docs.cursor.com/context/model-context-protocol) |\n| **Roo Code** | Manual configuration required\u003cbr/\u003eUse `\"type\": \"streamable-http\"` | [Roo Code MCP Official Guide](https://docs.roocode.com/features/mcp/using-mcp-in-roo) |\n| **Cline** | Manual configuration required\u003cbr/\u003eUse `\"type\": \"streamableHttp\"` | [Cline MCP Official Guide](https://docs.cline.bot/mcp/connecting-to-a-remote-server) |\n| **Gemini CLI** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Add an `mcpServer` object to `.gemini/settings.json` file\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"Microsoft Learn MCP Server\": {\u003cbr/\u003e     \"httpUrl\": \"https://learn.microsoft.com/api/mcp\" \u003cbr/\u003e   }\u003cbr/\u003e}\u003c/pre\u003e\u003c/details\u003e  | [How to set up your MCP server](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server)|\n| **Qwen Code** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Add an `mcpServer` object to `.qwen/settings.json` file\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"Microsoft Learn MCP Server\": {\u003cbr/\u003e     \"httpUrl\": \"https://learn.microsoft.com/api/mcp\" \u003cbr/\u003e   }\u003cbr/\u003e}\u003c/pre\u003e\u003c/details\u003e  | [Configure the MCP server in settings.json](https://qwenlm.github.io/qwen-code-docs/en/cli/tutorials/#configure-the-mcp-server-in-settingsjson)|\n| **GitHub** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Navigate to Settings → Coding agent\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"mslearn\": {\u003cbr/\u003e    \"type\": \"http\",\u003cbr/\u003e    \"url\": \"https://learn.microsoft.com/api/mcp\",\u003cbr/\u003e    \"tools\": [\u003cbr/\u003e      \"*\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003e\u003c/details\u003e\n| **ChatGPT** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Instructions\u003c/summary\u003e1. Open ChatGPT in the browser\u003cbr/\u003e2. Go to **Settings → Connectors → Advanced settings → Turn Developer mode on**\u003cbr/\u003e3. Go back to connectors and click **create**\u003cbr/\u003e4. Give the connector a **name**, enter **URL** `https://learn.microsoft.com/api/mcp`, set **authentication** to `No authentication` and **trust** the application\u003cbr/\u003e5. Click **create**\u003cbr/\u003e \u003c/details\u003e | [ChatGPT Official Guide](https://platform.openai.com/docs/guides/developer-mode)|\n\n### Alternative Installation (for legacy clients or local configuration)\n\nFor clients that don't support native remote MCP servers or if you prefer local configuration, you can use `mcp-remote` as a proxy:\n\n| Client | Manual Configuration | MCP Guide |\n|--------|----------------------|-----------| \n| **Claude Desktop (legacy config)** | \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Only use this if Settings → Integrations doesn't work\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"microsoft.docs.mcp\": {\u003cbr/\u003e    \"command\": \"npx\",\u003cbr/\u003e    \"args\": [\u003cbr/\u003e      \"-y\",\u003cbr/\u003e      \"mcp-remote\",\u003cbr/\u003e      \"https://learn.microsoft.com/api/mcp\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003eAdd to `claude_desktop_config.json`\u003c/details\u003e| [Claude Desktop MCP Guide](https://modelcontextprotocol.io/quickstart/user) |\n| **Windsurf** | \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e\u003cpre\u003e{\u003cbr/\u003e  \"microsoft.docs.mcp\": {\u003cbr/\u003e    \"command\": \"npx\",\u003cbr/\u003e    \"args\": [\u003cbr/\u003e      \"-y\",\u003cbr/\u003e      \"mcp-remote\",\u003cbr/\u003e      \"https://learn.microsoft.com/api/mcp\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003e \u003c/details\u003e| [Windsurf MCP Guide](https://docs.windsurf.com/windsurf/cascade/mcp) |\n| **Kiro** | \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e\u003cpre\u003e{\u003cbr/\u003e  \"microsoft.docs.mcp\": {\u003cbr/\u003e    \"command\": \"npx\",\u003cbr/\u003e    \"args\": [\u003cbr/\u003e      \"-y\",\u003cbr/\u003e      \"mcp-remote\",\u003cbr/\u003e      \"https://learn.microsoft.com/api/mcp\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003e \u003c/details\u003e| [Kiro MCP Guide](https://kiro.dev/docs/mcp/index) |\n\n### ▶️ Getting Started\n\n1. **For VS Code**: Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n2. **For Claude Desktop**: After adding the integration, you'll see the MCP tools icon in the chat interface\n3. You should see the Learn MCP Server in the list of available tools\n4. Try a prompt that tells the agent to use the MCP Server, such as \"what are the az cli commands to create an Azure container app according to official Microsoft Learn documentation?\"\n5. The agent should be able to use the MCP Server tools to complete your query\n\n\u003e ### ⚠️ Building a Custom Client\n\u003e\n\u003e If your use case requires a direct, programmatic integration, it is essential to understand that MCP is a **dynamic protocol, not a static API**. The available tools and their schemas will evolve.\n\u003e\n\u003e To build a resilient client that will not break as the service is updated, you should adhere to the following principles:\n\u003e\n\u003e 1.  **Discover Tools Dynamically:** Your client should fetch current tool definitions from the server at runtime (e.g., using `tools/list`). **Do not hard-code tool names or parameters.**\n\u003e 2.  **Refresh on Failure:** Your client should handle errors during `tool/invoke` calls. If a tool call fails with an error indicating it is missing or its schema has changed (e.g., an HTTP 404 or 400 error), your client should assume its cache is stale and automatically trigger a refresh by calling `tools/list`.\n\u003e 3.  **Handle Live Updates:** Your client should listen for server notifications (e.g., `listChanged`) and refresh its tool cache accordingly.\n\n## ❓ Troubleshooting\n\n### 💻 System Prompt\n\nEven tool-friendly models like Claude Sonnet 4 sometimes fail to call MCP tools by default; use system prompts to encourage usage.\n\nHere's an example of a Cursor rule (a system prompt) that will cause the LLM to utilize `microsoft.docs.mcp` more frequently:\n\n```md\n## Querying Microsoft Documentation\n\nYou have access to MCP tools called `microsoft_docs_search`, `microsoft_docs_fetch`, and `microsoft_code_sample_search` - these tools allow you to search through and fetch Microsoft's latest official documentation and code samples, and that information might be more detailed or newer than what's in your training data set.\n\nWhen handling questions around how to work with native Microsoft technologies, such as C#, F#, ASP.NET Core, Microsoft.Extensions, NuGet, Entity Framework, the `dotnet` runtime - please use these tools for research purposes when dealing with specific / narrowly defined questions that may occur.\n```\n\n### ⚠️ Common Issues\n\n| Issue | Possible Solution |\n|-------|-------------------|\n| Connection errors | Verify your network connection and that the server URL is correctly entered |\n| No results returned | Try rephrasing your query with more specific technical terms |\n| Tool not appearing in VS Code | Restart VS Code or check that the MCP extension is properly installed |\n| HTTP status 405  | Method not allowed happens when a browser tries to connect to the endpoint. Try using the MCP Server through VS Code GitHub Copilot or [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) instead. |\n\n### 🆘 Getting Support\n\n- [Ask questions, share ideas](https://github.com/MicrosoftDocs/mcp/discussions)\n- [Create an issue](https://github.com/MicrosoftDocs/mcp/issues)\n\n## 🔮 Future Enhancements\n\nThe Microsoft Learn MCP Server team is working on several enhancements:\n\n- Improved telemetry to help inform server enhancements\n- Expanding coverage to additional Microsoft documentation sources\n- Improved query understanding for more precise results\n\n## 📚 Additional Resources\n\n- [Microsoft Learn MCP Server product documentation](https://learn.microsoft.com/training/support/mcp)\n- [Microsoft MCP Servers](https://github.com/microsoft/mcp)\n- [Microsoft Learn](https://learn.microsoft.com)\n- [Model Context Protocol Specification](https://modelcontextprotocol.io)\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:32Z","updated_at":"2025-10-22T11:21:45Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://learn.microsoft.com/api/mcp"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"e0455530-d34a-4795-8dc6-b2888ae76c27","is_latest":true,"published_at":"2025-08-22T00:00:00Z","updated_at":"2025-08-22T00:00:00Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Microsoft Learn","homepage_url":"https://learn.microsoft.com","is_in_organization":true,"license":"Creative Commons Attribution 4.0 International","name":"mcp","name_with_owner":"MicrosoftDocs/mcp","opengraph_image_url":"https://opengraph.githubassets.com/c64357ffedd77b5f028035a71ed58f8acc0c65cc1b325a0981ec142d726e547f/MicrosoftDocs/mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/22479449?v=4","preferred_image":"https://avatars.githubusercontent.com/u/22479449?v=4","pushed_at":"2025-10-13T06:40:49Z","readme":"# 🌟 Microsoft Learn MCP Server\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Microsoft_Docs_MCP-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Microsoft_Docs_MCP-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D\u0026quality=insiders)\n\nThe Microsoft Learn MCP Server is a remote MCP Server that enables clients like GitHub Copilot and other AI agents to bring trusted and up-to-date information directly from Microsoft's official documentation. It supports streamable http transport, which is lightweight for clients to use.\n\n\u003e Please note that this project is in Public Preview and implementation may significantly change prior to our General Availability.\n\n## 📑 Table of contents\n1. [🎯 Overview](#-overview)\n2. [🌐 The Microsoft Learn MCP Server Endpoint](#-the-microsoft-learn-mcp-server-endpoint)\n3. [🛠️ Currently Supported Tools](#%EF%B8%8F-currently-supported-tools)\n4. [🔌 Installation \u0026 Getting Started](#-installation--getting-started)\n5. [❓ Troubleshooting](#-troubleshooting)\n6. [🔮 Future Enhancements](#-future-enhancements)\n7. [📚 Additional Resources](#-additional-resources)\n\n## 🎯 Overview\n\n### ✨ Example Prompts: Your Source of Truth\n\nYour AI assistant should automatically use these tools for Microsoft-related topics. With both search and fetch capabilities, you can get quick answers or comprehensive deep dives. To ensure that it always consults the official documentation, you can add phrases like `search Microsoft docs`, `deep dive`, `fetch full doc`.\n\n#### **Quick Search \u0026 Reference**\n\n\u003e \"Give me the Azure CLI commands to create an Azure Container App with a managed identity. **search Microsoft docs**\"\n\n\u003e \"Is gpt-4.1-mini available in EU regions? **fetch full doc**\"\n\n#### **Code Verification \u0026 Best Practices**\n\n\u003e \"Are you sure this is the right way to implement `IHttpClientFactory` in a .NET 8 minimal API? **search Microsoft docs and fetch full doc**\"\n\n\u003e \"Show me the complete guide for implementing authentication in ASP.NET Core. **fetch full doc**\"\n\n\u003e \"show me detailed, runnable python code sample to do harms eval using azure ai foundry evaluation sdk\"\n\n#### **Comprehensive Learning \u0026 Deep Dive**\n\n\u003e \"I need to understand Azure Functions end-to-end. **search Microsoft docs and deep dive**\"\n\n\u003e \"Get me the full step-by-step tutorial for deploying a .NET application to Azure App Service. **search Microsoft docs and deep dive**\"\n\n### 📊 Key Capabilities\n\n- **High-Quality Content Retrieval**: Search and retrieve relevant content from Microsoft's official documentation in markdown format.\n- **Code Sample Discovery**: Find official Microsoft/Azure code snippets and examples with language-specific filtering.\n- **Semantic Understanding**: Uses advanced vector search to find the most contextually relevant documentation for any query.\n- **Real-time Updates**: Access the latest Microsoft documentation as it's published.\n\n## 🌐 The Microsoft Learn MCP Server Endpoint\n\nThe Microsoft Learn MCP Server is accessible to any IDE, agent, or tool that supports the Model Context Protocol (MCP). Any compatible client can connect to the following **remote MCP endpoint**:\n\n```\nhttps://learn.microsoft.com/api/mcp\n```\n\u003e **Note:** This URL is intended for use **within a compliant MCP client** via Streamable HTTP, such as the recommended clients listed in our [Getting Started](#-installation--getting-started) section. It does not support direct access from a web browser and may return a `405 Method Not Allowed` error if accessed manually. For developers who need to build their own solution, please follow the mandatory guidelines in the [Building a Custom Client](#%EF%B8%8F-building-a-custom-client) section to ensure your implementation is resilient and supported.\n\n**Example JSON configuration:**\n```json\n{\n  \"microsoft.docs.mcp\": {\n    \"type\": \"http\",\n    \"url\": \"https://learn.microsoft.com/api/mcp\"\n  }\n}\n```\n\n## 🛠️ Currently Supported Tools\n\n| Tool Name | Description | Input Parameters |\n|-----------|-------------|------------------|\n| `microsoft_docs_search` | Performs semantic search against Microsoft official technical documentation | `query` (string): The search query for retrieval |\n| `microsoft_docs_fetch` | Fetch and convert a Microsoft documentation page into markdown format | `url` (string): URL of the documentation page to read |\n| `microsoft_code_sample_search` | Search for official Microsoft/Azure code snippets and examples | `query` (string): Search query for Microsoft/Azure code snippets\u003cbr/\u003e`language` (string, optional): Programming language filter.|\n\n\n## 🔌 Installation \u0026 Getting Started\n\nThe Microsoft Learn MCP Server supports quick installation across multiple development environments. Choose your preferred client below for streamlined setup:\n\n| Client | One-click Installation | MCP Guide |\n|--------|----------------------|-------------------|\n| **VS Code** | [![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Microsoft_Docs_MCP-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Microsoft_Docs_MCP-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp\u0026config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D\u0026quality=insiders) | [VS Code MCP Official Guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) |\n| **Claude Desktop** | \u003cdetails\u003e\u003csummary\u003eView Instructions\u003c/summary\u003e1. Open Claude Desktop\u003cbr/\u003e2. Go to **Settings → Integrations**\u003cbr/\u003e3. Click **Add Integration**\u003cbr/\u003e4. Enter URL: `https://learn.microsoft.com/api/mcp`\u003cbr/\u003e5. Click **Connect**\u003c/details\u003e | [Claude Desktop Remote MCP Guide](https://support.anthropic.com/en/articles/11503834-building-custom-integrations-via-remote-mcp-servers) |\n| **Claude Code** | \u003cdetails\u003e\u003csummary\u003eView Instructions\u003c/summary\u003e1. Open a CLI\u003cbr/\u003e2. Type `claude mcp add --transport http microsoft_docs_mcp https://learn.microsoft.com/api/mcp` and press enter\u003cbr/\u003e3. (optional) Type `--scope user` directly after `claude mcp add` to make this MCP server available in Claude Code for all of your projects\u003c/details\u003e | [Claude Code Remote MCP Guide](https://docs.anthropic.com/en/docs/claude-code/mcp) |\n| **Visual Studio** | Manual configuration required\u003cbr/\u003eUse `\"type\": \"http\"` | [Visual Studio MCP Official Guide](https://learn.microsoft.com/en-us/visualstudio/ide/mcp-servers?view=vs-2022) |\n| **Cursor IDE** | [![Install in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=microsoft.docs.mcp\u0026config=eyJ0eXBlIjoiaHR0cCIsInVybCI6Imh0dHBzOi8vbGVhcm4ubWljcm9zb2Z0LmNvbS9hcGkvbWNwIn0%3D) | [Cursor MCP Official Guide](https://docs.cursor.com/context/model-context-protocol) |\n| **Roo Code** | Manual configuration required\u003cbr/\u003eUse `\"type\": \"streamable-http\"` | [Roo Code MCP Official Guide](https://docs.roocode.com/features/mcp/using-mcp-in-roo) |\n| **Cline** | Manual configuration required\u003cbr/\u003eUse `\"type\": \"streamableHttp\"` | [Cline MCP Official Guide](https://docs.cline.bot/mcp/connecting-to-a-remote-server) |\n| **Gemini CLI** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Add an `mcpServer` object to `.gemini/settings.json` file\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"Microsoft Learn MCP Server\": {\u003cbr/\u003e     \"httpUrl\": \"https://learn.microsoft.com/api/mcp\" \u003cbr/\u003e   }\u003cbr/\u003e}\u003c/pre\u003e\u003c/details\u003e  | [How to set up your MCP server](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server)|\n| **Qwen Code** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Add an `mcpServer` object to `.qwen/settings.json` file\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"Microsoft Learn MCP Server\": {\u003cbr/\u003e     \"httpUrl\": \"https://learn.microsoft.com/api/mcp\" \u003cbr/\u003e   }\u003cbr/\u003e}\u003c/pre\u003e\u003c/details\u003e  | [Configure the MCP server in settings.json](https://qwenlm.github.io/qwen-code-docs/en/cli/tutorials/#configure-the-mcp-server-in-settingsjson)|\n| **GitHub** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Navigate to Settings → Coding agent\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"mslearn\": {\u003cbr/\u003e    \"type\": \"http\",\u003cbr/\u003e    \"url\": \"https://learn.microsoft.com/api/mcp\",\u003cbr/\u003e    \"tools\": [\u003cbr/\u003e      \"*\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003e\u003c/details\u003e\n| **ChatGPT** | Manual configuration required\u003cbr/\u003e \u003cdetails\u003e\u003csummary\u003eView Instructions\u003c/summary\u003e1. Open ChatGPT in the browser\u003cbr/\u003e2. Go to **Settings → Connectors → Advanced settings → Turn Developer mode on**\u003cbr/\u003e3. Go back to connectors and click **create**\u003cbr/\u003e4. Give the connector a **name**, enter **URL** `https://learn.microsoft.com/api/mcp`, set **authentication** to `No authentication` and **trust** the application\u003cbr/\u003e5. Click **create**\u003cbr/\u003e \u003c/details\u003e | [ChatGPT Official Guide](https://platform.openai.com/docs/guides/developer-mode)|\n\n### Alternative Installation (for legacy clients or local configuration)\n\nFor clients that don't support native remote MCP servers or if you prefer local configuration, you can use `mcp-remote` as a proxy:\n\n| Client | Manual Configuration | MCP Guide |\n|--------|----------------------|-----------| \n| **Claude Desktop (legacy config)** | \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e**Note**: Only use this if Settings → Integrations doesn't work\u003cbr/\u003e\u003cpre\u003e{\u003cbr/\u003e  \"microsoft.docs.mcp\": {\u003cbr/\u003e    \"command\": \"npx\",\u003cbr/\u003e    \"args\": [\u003cbr/\u003e      \"-y\",\u003cbr/\u003e      \"mcp-remote\",\u003cbr/\u003e      \"https://learn.microsoft.com/api/mcp\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003eAdd to `claude_desktop_config.json`\u003c/details\u003e| [Claude Desktop MCP Guide](https://modelcontextprotocol.io/quickstart/user) |\n| **Windsurf** | \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e\u003cpre\u003e{\u003cbr/\u003e  \"microsoft.docs.mcp\": {\u003cbr/\u003e    \"command\": \"npx\",\u003cbr/\u003e    \"args\": [\u003cbr/\u003e      \"-y\",\u003cbr/\u003e      \"mcp-remote\",\u003cbr/\u003e      \"https://learn.microsoft.com/api/mcp\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003e \u003c/details\u003e| [Windsurf MCP Guide](https://docs.windsurf.com/windsurf/cascade/mcp) |\n| **Kiro** | \u003cdetails\u003e\u003csummary\u003eView Config\u003c/summary\u003e\u003cpre\u003e{\u003cbr/\u003e  \"microsoft.docs.mcp\": {\u003cbr/\u003e    \"command\": \"npx\",\u003cbr/\u003e    \"args\": [\u003cbr/\u003e      \"-y\",\u003cbr/\u003e      \"mcp-remote\",\u003cbr/\u003e      \"https://learn.microsoft.com/api/mcp\"\u003cbr/\u003e    ]\u003cbr/\u003e  }\u003cbr/\u003e}\u003c/pre\u003e \u003c/details\u003e| [Kiro MCP Guide](https://kiro.dev/docs/mcp/index) |\n\n### ▶️ Getting Started\n\n1. **For VS Code**: Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n2. **For Claude Desktop**: After adding the integration, you'll see the MCP tools icon in the chat interface\n3. You should see the Learn MCP Server in the list of available tools\n4. Try a prompt that tells the agent to use the MCP Server, such as \"what are the az cli commands to create an Azure container app according to official Microsoft Learn documentation?\"\n5. The agent should be able to use the MCP Server tools to complete your query\n\n\u003e ### ⚠️ Building a Custom Client\n\u003e\n\u003e If your use case requires a direct, programmatic integration, it is essential to understand that MCP is a **dynamic protocol, not a static API**. The available tools and their schemas will evolve.\n\u003e\n\u003e To build a resilient client that will not break as the service is updated, you should adhere to the following principles:\n\u003e\n\u003e 1.  **Discover Tools Dynamically:** Your client should fetch current tool definitions from the server at runtime (e.g., using `tools/list`). **Do not hard-code tool names or parameters.**\n\u003e 2.  **Refresh on Failure:** Your client should handle errors during `tool/invoke` calls. If a tool call fails with an error indicating it is missing or its schema has changed (e.g., an HTTP 404 or 400 error), your client should assume its cache is stale and automatically trigger a refresh by calling `tools/list`.\n\u003e 3.  **Handle Live Updates:** Your client should listen for server notifications (e.g., `listChanged`) and refresh its tool cache accordingly.\n\n## ❓ Troubleshooting\n\n### 💻 System Prompt\n\nEven tool-friendly models like Claude Sonnet 4 sometimes fail to call MCP tools by default; use system prompts to encourage usage.\n\nHere's an example of a Cursor rule (a system prompt) that will cause the LLM to utilize `microsoft.docs.mcp` more frequently:\n\n```md\n## Querying Microsoft Documentation\n\nYou have access to MCP tools called `microsoft_docs_search`, `microsoft_docs_fetch`, and `microsoft_code_sample_search` - these tools allow you to search through and fetch Microsoft's latest official documentation and code samples, and that information might be more detailed or newer than what's in your training data set.\n\nWhen handling questions around how to work with native Microsoft technologies, such as C#, F#, ASP.NET Core, Microsoft.Extensions, NuGet, Entity Framework, the `dotnet` runtime - please use these tools for research purposes when dealing with specific / narrowly defined questions that may occur.\n```\n\n### ⚠️ Common Issues\n\n| Issue | Possible Solution |\n|-------|-------------------|\n| Connection errors | Verify your network connection and that the server URL is correctly entered |\n| No results returned | Try rephrasing your query with more specific technical terms |\n| Tool not appearing in VS Code | Restart VS Code or check that the MCP extension is properly installed |\n| HTTP status 405  | Method not allowed happens when a browser tries to connect to the endpoint. Try using the MCP Server through VS Code GitHub Copilot or [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) instead. |\n\n### 🆘 Getting Support\n\n- [Ask questions, share ideas](https://github.com/MicrosoftDocs/mcp/discussions)\n- [Create an issue](https://github.com/MicrosoftDocs/mcp/issues)\n\n## 🔮 Future Enhancements\n\nThe Microsoft Learn MCP Server team is working on several enhancements:\n\n- Improved telemetry to help inform server enhancements\n- Expanding coverage to additional Microsoft documentation sources\n- Improved query understanding for more precise results\n\n## 📚 Additional Resources\n\n- [Microsoft Learn MCP Server product documentation](https://learn.microsoft.com/training/support/mcp)\n- [Microsoft MCP Servers](https://github.com/microsoft/mcp)\n- [Microsoft Learn](https://learn.microsoft.com)\n- [Model Context Protocol Specification](https://modelcontextprotocol.io)\n","stargazer_count":1007,"topics":["ai","ai-agents","documentation","mcp","mcp-server","microsoft","microsoft-learn","rag","copilot","llm"],"uses_custom_opengraph_image":false}}}},{"name":"stripe/agent-toolkit","description":"Interact with Stripe API","status":"active","repository":{"url":"https://github.com/stripe/agent-toolkit","source":"github","id":"886826524","readme":"# Stripe Agent Toolkit\n\nThe Stripe Agent Toolkit enables popular agent frameworks including Model Context Protocol (MCP), OpenAI's Agent SDK, LangChain, CrewAI, and Vercel's AI SDK to integrate with Stripe APIs through function calling. The\nlibrary is not exhaustive of the entire Stripe API. It includes support for MCP, Python, and TypeScript and is built directly on top of the Stripe [Python][python-sdk] and [Node][node-sdk] SDKs.\n\nIncluded below are basic instructions, but refer to the [MCP](/modelcontextprotocol) [Python](/python), [TypeScript](/typescript) packages for more information.\n\n## Model Context Protocol\n\nStripe hosts a remote MCP server at `https://mcp.stripe.com`. This allows secure MCP client access via OAuth. View the docs [here](https://docs.stripe.com/mcp#remote).\n\nThe Stripe Agent Toolkit also exposes tools in the [Model Context Protocol (MCP)](https://modelcontextprotocol.com/) format.  Or, to run a local Stripe MCP server using npx, use the following command:\n\n```bash\nnpx -y @stripe/mcp --tools=all --api-key=YOUR_STRIPE_SECRET_KEY\n```\n\n## Python\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```sh\npip install stripe-agent-toolkit\n```\n\n#### Requirements\n\n- Python 3.11+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is\navailable in your [Stripe Dashboard][api-keys].\n\n```python\nfrom stripe_agent_toolkit.openai.toolkit import StripeAgentToolkit\n\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"actions\": {\n            \"payment_links\": {\n                \"create\": True,\n            },\n        }\n    },\n)\n```\n\nThe toolkit works with OpenAI's Agent SDK, LangChain, and CrewAI and can be passed as a list of tools. For example:\n\n```python\nfrom agents import Agent\n\nstripe_agent = Agent(\n    name=\"Stripe Agent\",\n    instructions=\"You are an expert at integrating with Stripe\",\n    tools=stripe_agent_toolkit.get_tools()\n)\n```\n\nExamples for OpenAI's Agent SDK,LangChain, and CrewAI are included in [/examples](/python/examples).\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```python\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"context\": {\n            \"account\": \"acct_123\"\n        }\n    }\n)\n```\n\n## TypeScript\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```\nnpm install @stripe/agent-toolkit\n```\n\n#### Requirements\n\n- Node 18+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is available in your [Stripe Dashboard][api-keys]. Additionally, `configuration` enables you to specify the types of actions that can be taken using the toolkit.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/langchain\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n```\n\n#### Tools\n\nThe toolkit works with LangChain and Vercel's AI SDK and can be passed as a list of tools. For example:\n\n```typescript\nimport { AgentExecutor, createStructuredChatAgent } from \"langchain/agents\";\n\nconst tools = stripeAgentToolkit.getTools();\n\nconst agent = await createStructuredChatAgent({\n  llm,\n  tools,\n  prompt,\n});\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n});\n```\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```typescript\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    context: {\n      account: \"acct_123\",\n    },\n  },\n});\n```\n\n#### Metered billing\n\nFor Vercel's AI SDK, you can use middleware to submit billing events for usage. All that is required is the customer ID and the input/output meters to bill.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/ai-sdk\";\nimport { openai } from \"@ai-sdk/openai\";\nimport {\n  generateText,\n  experimental_wrapLanguageModel as wrapLanguageModel,\n} from \"ai\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n\nconst model = wrapLanguageModel({\n  model: openai(\"gpt-4o\"),\n  middleware: stripeAgentToolkit.middleware({\n    billing: {\n      customer: \"cus_123\",\n      meters: {\n        input: \"input_tokens\",\n        output: \"output_tokens\",\n      },\n    },\n  }),\n});\n```\n\n\n\n## Supported API methods\n\n- [Cancel a subscription](https://docs.stripe.com/api/subscriptions/cancel)\n- [Create a coupon](https://docs.stripe.com/api/coupons/create)\n- [Create a customer](https://docs.stripe.com/api/customers/create)\n- [Create a payment link](https://docs.stripe.com/api/payment-link/create)\n- [Create a price](https://docs.stripe.com/api/prices/create)\n- [Create a product](https://docs.stripe.com/api/products/create)\n- [Create a refund](https://docs.stripe.com/api/refunds/create)\n- [Create an invoice item](https://docs.stripe.com/api/invoiceitems/create)\n- [Create an invoice](https://docs.stripe.com/api/invoices/create)\n- [Finalize an invoice](https://docs.stripe.com/api/invoices/finalize)\n- [List all coupons](https://docs.stripe.com/api/coupons/list)\n- [List all customers](https://docs.stripe.com/api/customers/list)\n- [List all disputes](https://docs.stripe.com/api/disputes/list)\n- [List all prices](https://docs.stripe.com/api/prices/list)\n- [List all products](https://docs.stripe.com/api/products/list)\n- [List all subscriptions](https://docs.stripe.com/api/subscriptions/list)\n- [Retrieve balance](https://docs.stripe.com/api/balance/balance_retrieve)\n- [Update a dispute](https://docs.stripe.com/api/disputes/update)\n- [Update a subscription](https://docs.stripe.com/api/subscriptions/update)\n\n[python-sdk]: https://github.com/stripe/stripe-python\n[node-sdk]: https://github.com/stripe/stripe-node\n[api-keys]: https://dashboard.stripe.com/account/apikeys\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:33Z","updated_at":"2025-10-22T11:20:49Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.stripe.com"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"f0b8ecdd-498c-4087-8ae2-b2677215b7ca","is_latest":true,"published_at":"2025-09-09T10:55:22.910285Z","updated_at":"2025-09-09T10:55:22.910285Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Stripe","homepage_url":"https://docs.stripe.com/agents","is_in_organization":true,"license":"MIT License","name":"agent-toolkit","name_with_owner":"stripe/agent-toolkit","opengraph_image_url":"https://opengraph.githubassets.com/7e8b2658e9c327267109b50cd7e750cdb0349d43a26243267c3bfedfbf641e6e/stripe/agent-toolkit","owner_avatar_url":"https://avatars.githubusercontent.com/u/856813?v=4","preferred_image":"https://avatars.githubusercontent.com/u/856813?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-21T21:02:06Z","readme":"# Stripe Agent Toolkit\n\nThe Stripe Agent Toolkit enables popular agent frameworks including Model Context Protocol (MCP), OpenAI's Agent SDK, LangChain, CrewAI, and Vercel's AI SDK to integrate with Stripe APIs through function calling. The\nlibrary is not exhaustive of the entire Stripe API. It includes support for MCP, Python, and TypeScript and is built directly on top of the Stripe [Python][python-sdk] and [Node][node-sdk] SDKs.\n\nIncluded below are basic instructions, but refer to the [MCP](/modelcontextprotocol) [Python](/python), [TypeScript](/typescript) packages for more information.\n\n## Model Context Protocol\n\nStripe hosts a remote MCP server at `https://mcp.stripe.com`. This allows secure MCP client access via OAuth. View the docs [here](https://docs.stripe.com/mcp#remote).\n\nThe Stripe Agent Toolkit also exposes tools in the [Model Context Protocol (MCP)](https://modelcontextprotocol.com/) format.  Or, to run a local Stripe MCP server using npx, use the following command:\n\n```bash\nnpx -y @stripe/mcp --tools=all --api-key=YOUR_STRIPE_SECRET_KEY\n```\n\n## Python\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```sh\npip install stripe-agent-toolkit\n```\n\n#### Requirements\n\n- Python 3.11+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is\navailable in your [Stripe Dashboard][api-keys].\n\n```python\nfrom stripe_agent_toolkit.openai.toolkit import StripeAgentToolkit\n\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"actions\": {\n            \"payment_links\": {\n                \"create\": True,\n            },\n        }\n    },\n)\n```\n\nThe toolkit works with OpenAI's Agent SDK, LangChain, and CrewAI and can be passed as a list of tools. For example:\n\n```python\nfrom agents import Agent\n\nstripe_agent = Agent(\n    name=\"Stripe Agent\",\n    instructions=\"You are an expert at integrating with Stripe\",\n    tools=stripe_agent_toolkit.get_tools()\n)\n```\n\nExamples for OpenAI's Agent SDK,LangChain, and CrewAI are included in [/examples](/python/examples).\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```python\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"context\": {\n            \"account\": \"acct_123\"\n        }\n    }\n)\n```\n\n## TypeScript\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```\nnpm install @stripe/agent-toolkit\n```\n\n#### Requirements\n\n- Node 18+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is available in your [Stripe Dashboard][api-keys]. Additionally, `configuration` enables you to specify the types of actions that can be taken using the toolkit.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/langchain\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n```\n\n#### Tools\n\nThe toolkit works with LangChain and Vercel's AI SDK and can be passed as a list of tools. For example:\n\n```typescript\nimport { AgentExecutor, createStructuredChatAgent } from \"langchain/agents\";\n\nconst tools = stripeAgentToolkit.getTools();\n\nconst agent = await createStructuredChatAgent({\n  llm,\n  tools,\n  prompt,\n});\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n});\n```\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```typescript\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    context: {\n      account: \"acct_123\",\n    },\n  },\n});\n```\n\n#### Metered billing\n\nFor Vercel's AI SDK, you can use middleware to submit billing events for usage. All that is required is the customer ID and the input/output meters to bill.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/ai-sdk\";\nimport { openai } from \"@ai-sdk/openai\";\nimport {\n  generateText,\n  experimental_wrapLanguageModel as wrapLanguageModel,\n} from \"ai\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n\nconst model = wrapLanguageModel({\n  model: openai(\"gpt-4o\"),\n  middleware: stripeAgentToolkit.middleware({\n    billing: {\n      customer: \"cus_123\",\n      meters: {\n        input: \"input_tokens\",\n        output: \"output_tokens\",\n      },\n    },\n  }),\n});\n```\n\n\n\n## Supported API methods\n\n- [Cancel a subscription](https://docs.stripe.com/api/subscriptions/cancel)\n- [Create a coupon](https://docs.stripe.com/api/coupons/create)\n- [Create a customer](https://docs.stripe.com/api/customers/create)\n- [Create a payment link](https://docs.stripe.com/api/payment-link/create)\n- [Create a price](https://docs.stripe.com/api/prices/create)\n- [Create a product](https://docs.stripe.com/api/products/create)\n- [Create a refund](https://docs.stripe.com/api/refunds/create)\n- [Create an invoice item](https://docs.stripe.com/api/invoiceitems/create)\n- [Create an invoice](https://docs.stripe.com/api/invoices/create)\n- [Finalize an invoice](https://docs.stripe.com/api/invoices/finalize)\n- [List all coupons](https://docs.stripe.com/api/coupons/list)\n- [List all customers](https://docs.stripe.com/api/customers/list)\n- [List all disputes](https://docs.stripe.com/api/disputes/list)\n- [List all prices](https://docs.stripe.com/api/prices/list)\n- [List all products](https://docs.stripe.com/api/products/list)\n- [List all subscriptions](https://docs.stripe.com/api/subscriptions/list)\n- [Retrieve balance](https://docs.stripe.com/api/balance/balance_retrieve)\n- [Update a dispute](https://docs.stripe.com/api/disputes/update)\n- [Update a subscription](https://docs.stripe.com/api/subscriptions/update)\n\n[python-sdk]: https://github.com/stripe/stripe-python\n[node-sdk]: https://github.com/stripe/stripe-node\n[api-keys]: https://dashboard.stripe.com/account/apikeys\n","stargazer_count":986,"topics":["llm","llm-agents","python","typescript","workflows","mcp"],"uses_custom_opengraph_image":false}}}},{"name":"microsoft/azure-devops-mcp","description":"Interact with Azure DevOps services like repositories, work items, builds, releases, test plans, and code search.","status":"active","repository":{"url":"https://github.com/microsoft/azure-devops-mcp","source":"github","id":"984142834","readme":"# ⭐ Azure DevOps MCP Server\n\nEasily install the Azure DevOps MCP Server for VS Code or VS Code Insiders:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-Install_AzureDevops_MCP_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n[![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_AzureDevops_MCP_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026quality=insiders\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n\nThis TypeScript project provides a **local** MCP server for Azure DevOps, enabling you to perform a wide range of Azure DevOps tasks directly from your code editor.\n\n## 📄 Table of Contents\n\n1. [📺 Overview](#-overview)\n2. [🏆 Expectations](#-expectations)\n3. [⚙️ Supported Tools](#️-supported-tools)\n4. [🔌 Installation \u0026 Getting Started](#-installation--getting-started)\n5. [🌏 Using Domains](#-using-domains)\n6. [📝 Troubleshooting](#-troubleshooting)\n7. [🎩 Examples \u0026 Best Practices](#-examples--best-practices)\n8. [🙋‍♀️ Frequently Asked Questions](#️-frequently-asked-questions)\n9. [📌 Contributing](#-contributing)\n\n## 📺 Overview\n\nThe Azure DevOps MCP Server brings Azure DevOps context to your agents. Try prompts like:\n\n- \"List my ADO projects\"\n- \"List ADO Builds for 'Contoso'\"\n- \"List ADO Repos for 'Contoso'\"\n- \"List test plans for 'Contoso'\"\n- \"List teams for project 'Contoso'\"\n- \"List iterations for project 'Contoso'\"\n- \"List my work items for project 'Contoso'\"\n- \"List work items in current iteration for 'Contoso' project and 'Contoso Team'\"\n- \"List all wikis in the 'Contoso' project\"\n- \"Create a wiki page '/Architecture/Overview' with content about system design\"\n- \"Update the wiki page '/Getting Started' with new onboarding instructions\"\n- \"Get the content of the wiki page '/API/Authentication' from the Documentation wiki\"\n\n## 🏆 Expectations\n\nThe Azure DevOps MCP Server is built from tools that are concise, simple, focused, and easy to use—each designed for a specific scenario. We intentionally avoid complex tools that try to do too much. The goal is to provide a thin abstraction layer over the REST APIs, making data access straightforward and letting the language model handle complex reasoning.\n\n## ⚙️ Supported Tools\n\nInteract with these Azure DevOps services:\n\n### 🧿 Core\n\n- **core_list_project_teams**: Retrieve a list of teams for the specified Azure DevOps project.\n- **core_list_projects**: Retrieve a list of projects in your Azure DevOps organization.\n- **core_get_identity_ids**: Retrieve Azure DevOps identity IDs for a list of unique names.\n\n### ⚒️ Work\n\n- **work_list_team_iterations**: Retrieve a list of iterations for a specific team in a project.\n- **work_create_iterations**: Create new iterations in a specified Azure DevOps project.\n- **work_assign_iterations**: Assign existing iterations to a specific team in a project.\n\n### 📅 Work Items\n\n- **wit_my_work_items**: Retrieve a list of work items relevant to the authenticated user.\n- **wit_list_backlogs**: Retrieve a list of backlogs for a given project and team.\n- **wit_list_backlog_work_items**: Retrieve a list of backlogs for a given project, team, and backlog category.\n- **wit_get_work_item**: Get a single work item by ID.\n- **wit_get_work_items_batch_by_ids**: Retrieve a list of work items by IDs in batch.\n- **wit_update_work_item**: Update a work item by ID with specified fields.\n- **wit_create_work_item**: Create a new work item in a specified project and work item type.\n- **wit_list_work_item_comments**: Retrieve a list of comments for a work item by ID.\n- **wit_get_work_items_for_iteration**: Retrieve a list of work items for a specified iteration.\n- **wit_add_work_item_comment**: Add a comment to a work item by ID.\n- **wit_add_child_work_items**: Create one or more child work items of a specific work item type for the given parent ID.\n- **wit_link_work_item_to_pull_request**: Link a single work item to an existing pull request.\n- **wit_get_work_item_type**: Get a specific work item type.\n- **wit_get_query**: Get a query by its ID or path.\n- **wit_get_query_results_by_id**: Retrieve the results of a work item query given the query ID.\n- **wit_update_work_items_batch**: Update work items in batch.\n- **wit_work_items_link**: Link work items together in batch.\n- **wit_work_item_unlink**: Unlink one or many links from a work item.\n- **wit_add_artifact_link**: Link to artifacts like branch, pull request, commit, and build.\n\n### 📁 Repositories\n\n- **repo_list_repos_by_project**: Retrieve a list of repositories for a given project.\n- **repo_list_pull_requests_by_repo_or_project**: Retrieve a list of pull requests for a given repository or project.\n- **repo_list_branches_by_repo**: Retrieve a list of branches for a given repository.\n- **repo_list_my_branches_by_repo**: Retrieve a list of your branches for a given repository ID.\n- **repo_list_pull_requests_by_commits**: List pull requests associated with commits.\n- **repo_list_pull_request_threads**: Retrieve a list of comment threads for a pull request.\n- **repo_list_pull_request_thread_comments**: Retrieve a list of comments in a pull request thread.\n- **repo_get_repo_by_name_or_id**: Get the repository by project and repository name or ID.\n- **repo_get_branch_by_name**: Get a branch by its name.\n- **repo_get_pull_request_by_id**: Get a pull request by its ID.\n- **repo_create_pull_request**: Create a new pull request.\n- **repo_create_branch**: Create a new branch in the repository.\n- **repo_update_pull_request**: Update various fields of an existing pull request (title, description, draft status, target branch).\n- **repo_update_pull_request_reviewers**: Add or remove reviewers for an existing pull request.\n- **repo_reply_to_comment**: Replies to a specific comment on a pull request.\n- **repo_resolve_comment**: Resolves a specific comment thread on a pull request.\n- **repo_search_commits**: Searches for commits.\n- **repo_create_pull_request_thread**: Creates a new comment thread on a pull request.\n\n### 🚀 Pipelines\n\n- **pipelines_get_build_definitions**: Retrieve a list of build definitions for a given project.\n- **pipelines_get_build_definition_revisions**: Retrieve a list of revisions for a specific build definition.\n- **pipelines_get_builds**: Retrieve a list of builds for a given project.\n- **pipelines_get_build_log**: Retrieve the logs for a specific build.\n- **pipelines_get_build_log_by_id**: Get a specific build log by log ID.\n- **pipelines_get_build_changes**: Get the changes associated with a specific build.\n- **pipelines_get_build_status**: Fetch the status of a specific build.\n- **pipelines_update_build_stage**: Update the stage of a specific build.\n- **pipelines_get_run**: Gets a run for a particular pipeline.\n- **pipelines_list_runs**: Gets top 10000 runs for a particular pipeline.\n- **pipelines_run_pipeline**: Starts a new run of a pipeline.\n\n### Advanced Security\n\n- **advsec_get_alerts**: Retrieve Advanced Security alerts for a repository.\n- **advsec_get_alert_details**: Get detailed information about a specific Advanced Security alert.\n\n### 🧪 Test Plans\n\n- **testplan_create_test_plan**: Create a new test plan in the project.\n- **testplan_create_test_case**: Create a new test case work item.\n- **testplan_update_test_case_steps**: Update an existing test case work item's steps.\n- **testplan_add_test_cases_to_suite**: Add existing test cases to a test suite.\n- **testplan_list_test_plans**: Retrieve a paginated list of test plans from an Azure DevOps project. Allows filtering for active plans and toggling detailed information.\n- **testplan_list_test_cases**: Get a list of test cases in the test plan.\n- **testplan_show_test_results_from_build_id**: Get a list of test results for a given project and build ID.\n- **testplan_create_test_suite**: Creates a new test suite in a test plan.\n\n### 📖 Wiki\n\n- **wiki_list_wikis**: Retrieve a list of wikis for an organization or project.\n- **wiki_get_wiki**: Get the wiki by wikiIdentifier.\n- **wiki_list_pages**: Retrieve a list of wiki pages for a specific wiki and project.\n- **wiki_get_page**: Retrieve wiki page metadata by path.\n- **wiki_get_page_content**: Retrieve wiki page content by wikiIdentifier and path.\n- **wiki_create_or_update_page**: Create or update wiki pages with full content support.\n\n### 🔎 Search\n\n- **search_code**: Get code search results for a given search text.\n- **search_wiki**: Get wiki search results for a given search text.\n- **search_workitem**: Get work item search results for a given search text.\n\n## 🔌 Installation \u0026 Getting Started\n\nFor the best experience, use Visual Studio Code and GitHub Copilot. See the [getting started documentation](./docs/GETTINGSTARTED.md) to use our MCP Server with other tools such as Visual Studio 2022, Claude Code, and Cursor.\n\n### Prerequisites\n\n1. Install [VS Code](https://code.visualstudio.com/download) or [VS Code Insiders](https://code.visualstudio.com/insiders)\n2. Install [Node.js](https://nodejs.org/en/download) 20+\n3. Open VS Code in an empty folder\n\n### Installation\n\n#### ✨ One-Click Install\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-Install_AzureDevops_MCP_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n[![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_AzureDevops_MCP_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026quality=insiders\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n\nAfter installation, select GitHub Copilot Agent Mode and refresh the tools list. Learn more about Agent Mode in the [VS Code Documentation](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode).\n\n#### 🧨 Install from Public Feed (Recommended)\n\nThis installation method is the easiest for all users of Visual Studio Code.\n\n🎥 [Watch this quick start video to get up and running in under two minutes!](https://youtu.be/EUmFM6qXoYk)\n\n##### Steps\n\nIn your project, add a `.vscode\\mcp.json` file with the following content:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp\", \"${input:ado_org}\"]\n    }\n  }\n}\n```\n\n🔥 To stay up to date with the latest features, you can use our nightly builds. Simply update your `mcp.json` configuration to use `@azure-devops/mcp@next`. Here is an updated example:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp@next\", \"${input:ado_org}\"]\n    }\n  }\n}\n```\n\nSave the file, then click 'Start'.\n\n![start mcp server](./docs/media/start-mcp-server.gif)\n\nIn chat, switch to [Agent Mode](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode).\n\nClick \"Select Tools\" and choose the available tools.\n\n![configure mcp server tools](./docs/media/configure-mcp-server-tools.gif)\n\nOpen GitHub Copilot Chat and try a prompt like `List ADO projects`. The first time an ADO tool is executed browser will open prompting to login with your Microsoft account. Please ensure you are using credentials matching selected Azure DevOps organization.\n\n\u003e 💥 We strongly recommend creating a `.github\\copilot-instructions.md` in your project. This will enhance your experience using the Azure DevOps MCP Server with GitHub Copilot Chat.\n\u003e To start, just include \"`This project uses Azure DevOps. Always check to see if the Azure DevOps MCP server has a tool relevant to the user's request`\" in your copilot instructions file.\n\nSee the [getting started documentation](./docs/GETTINGSTARTED.md) to use our MCP Server with other tools such as Visual Studio 2022, Claude Code, and Cursor.\n\n## 🌏 Using Domains\n\nAzure DevOps exposes a large surface area. As a result, our Azure DevOps MCP Server includes many tools. To keep the toolset manageable, avoid confusing the model, and respect client limits on loaded tools, use Domains to load only the areas you need. Domains are named groups of related tools (for example: core, work, work-items, repositories, wiki). Add the `-d` argument and the domain names to the server args in your `mcp.json` to list the domains to enable.\n\nFor example, use `\"-d\", \"core\", \"work\", \"work-items\"` to load only Work Item related tools (see the example below).\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado_with_filtered_domains\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp\", \"${input:ado_org}\", \"-d\", \"core\", \"work\", \"work-items\"]\n    }\n  }\n}\n```\n\nDomains that are available are: `core`, `work`, `work-items`, `search`, `test-plans`, `repositories`, `wiki`, `pipelines`, `advanced-security`\n\nWe recommend that you always enable `core` tools so that you can fetch project level information.\n\n\u003e By default all domains are loaded\n\n## 📝 Troubleshooting\n\nSee the [Troubleshooting guide](./docs/TROUBLESHOOTING.md) for help with common issues and logging.\n\n## 🎩 Examples \u0026 Best Practices\n\nExplore example prompts in our [Examples documentation](./docs/EXAMPLES.md).\n\nFor best practices and tips to enhance your experience with the MCP Server, refer to the [How-To guide](./docs/HOWTO.md).\n\n## 🙋‍♀️ Frequently Asked Questions\n\nFor answers to common questions about the Azure DevOps MCP Server, see the [Frequently Asked Questions](./docs/FAQ.md).\n\n## 📌 Contributing\n\nWe welcome contributions! During preview, please file issues for bugs, enhancements, or documentation improvements.\n\nSee our [Contributions Guide](./CONTRIBUTING.md) for:\n\n- 🛠️ Development setup\n- ✨ Adding new tools\n- 📝 Code style \u0026 testing\n- 🔄 Pull request process\n\n## 🤝 Code of Conduct\n\nThis project follows the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor questions, see the [FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [open@microsoft.com](mailto:open@microsoft.com).\n\n## 📈 Project Stats\n\n[![Star History Chart](https://api.star-history.com/svg?repos=microsoft/azure-devops-mcp\u0026type=Date)](https://star-history.com/#microsoft/azure-devops-mcp)\n\n## 🏆 Hall of Fame\n\nThanks to all contributors who make this project awesome! ❤️\n\n[![Contributors](https://contrib.rocks/image?repo=microsoft/azure-devops-mcp)](https://github.com/microsoft/azure-devops-mcp/graphs/contributors)\n\n\u003e Generated with [contrib.rocks](https://contrib.rocks)\n\n## License\n\nLicensed under the [MIT License](./LICENSE.md).\n\n---\n\n_Trademarks: This project may include trademarks or logos for Microsoft or third parties. Use of Microsoft trademarks or logos must follow [Microsoft’s Trademark \u0026 Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Third-party trademarks are subject to their respective policies._\n\n\u003c!-- version: 2023-04-07 [Do not delete this line, it is used for analytics that drive template improvements] --\u003e\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:07Z","updated_at":"2025-10-22T11:21:22Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"@azure-devops/mcp","version":"latest","runtime_hint":"npx","runtime_arguments":[{"is_required":true,"format":"string","value":"-y","type":"positional","name":"-y","value_hint":"noninteractive_mode"}],"package_arguments":[{"is_required":true,"format":"string","value":"{ado_org}","variables":{"ado_org":{"description":"Azure DevOps organization name (e.g., contoso).","is_required":true}},"type":"positional","name":"ado_org","value_hint":"organization"},{"format":"string","type":"named","name":"-d","value_hint":"optional_flag"},{"format":"string","value":"{ado_domain}","variables":{"ado_domain":{"description":"Repeat to enable specific domains: core, work, work-items, search, test-plans, repositories, wiki, pipelines, advanced-security."}},"type":"positional","name":"ado_domain","is_repeated":true}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"0348a3b3-6f52-4664-9eb7-bf732f71c144","is_latest":true,"published_at":"2025-09-09T10:55:22.907776Z","updated_at":"2025-09-09T10:55:22.907776Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Azure DevOps","is_in_organization":true,"license":"MIT License","name":"azure-devops-mcp","name_with_owner":"microsoft/azure-devops-mcp","opengraph_image_url":"https://repository-images.githubusercontent.com/984142834/d0a14641-bb08-45c9-a6f6-d0096a355a3b","owner_avatar_url":"https://avatars.githubusercontent.com/u/6154722?v=4","preferred_image":"https://avatars.githubusercontent.com/u/6154722?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-21T10:24:48Z","readme":"# ⭐ Azure DevOps MCP Server\n\nEasily install the Azure DevOps MCP Server for VS Code or VS Code Insiders:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-Install_AzureDevops_MCP_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n[![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_AzureDevops_MCP_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026quality=insiders\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n\nThis TypeScript project provides a **local** MCP server for Azure DevOps, enabling you to perform a wide range of Azure DevOps tasks directly from your code editor.\n\n## 📄 Table of Contents\n\n1. [📺 Overview](#-overview)\n2. [🏆 Expectations](#-expectations)\n3. [⚙️ Supported Tools](#️-supported-tools)\n4. [🔌 Installation \u0026 Getting Started](#-installation--getting-started)\n5. [🌏 Using Domains](#-using-domains)\n6. [📝 Troubleshooting](#-troubleshooting)\n7. [🎩 Examples \u0026 Best Practices](#-examples--best-practices)\n8. [🙋‍♀️ Frequently Asked Questions](#️-frequently-asked-questions)\n9. [📌 Contributing](#-contributing)\n\n## 📺 Overview\n\nThe Azure DevOps MCP Server brings Azure DevOps context to your agents. Try prompts like:\n\n- \"List my ADO projects\"\n- \"List ADO Builds for 'Contoso'\"\n- \"List ADO Repos for 'Contoso'\"\n- \"List test plans for 'Contoso'\"\n- \"List teams for project 'Contoso'\"\n- \"List iterations for project 'Contoso'\"\n- \"List my work items for project 'Contoso'\"\n- \"List work items in current iteration for 'Contoso' project and 'Contoso Team'\"\n- \"List all wikis in the 'Contoso' project\"\n- \"Create a wiki page '/Architecture/Overview' with content about system design\"\n- \"Update the wiki page '/Getting Started' with new onboarding instructions\"\n- \"Get the content of the wiki page '/API/Authentication' from the Documentation wiki\"\n\n## 🏆 Expectations\n\nThe Azure DevOps MCP Server is built from tools that are concise, simple, focused, and easy to use—each designed for a specific scenario. We intentionally avoid complex tools that try to do too much. The goal is to provide a thin abstraction layer over the REST APIs, making data access straightforward and letting the language model handle complex reasoning.\n\n## ⚙️ Supported Tools\n\nInteract with these Azure DevOps services:\n\n### 🧿 Core\n\n- **core_list_project_teams**: Retrieve a list of teams for the specified Azure DevOps project.\n- **core_list_projects**: Retrieve a list of projects in your Azure DevOps organization.\n- **core_get_identity_ids**: Retrieve Azure DevOps identity IDs for a list of unique names.\n\n### ⚒️ Work\n\n- **work_list_team_iterations**: Retrieve a list of iterations for a specific team in a project.\n- **work_create_iterations**: Create new iterations in a specified Azure DevOps project.\n- **work_assign_iterations**: Assign existing iterations to a specific team in a project.\n\n### 📅 Work Items\n\n- **wit_my_work_items**: Retrieve a list of work items relevant to the authenticated user.\n- **wit_list_backlogs**: Retrieve a list of backlogs for a given project and team.\n- **wit_list_backlog_work_items**: Retrieve a list of backlogs for a given project, team, and backlog category.\n- **wit_get_work_item**: Get a single work item by ID.\n- **wit_get_work_items_batch_by_ids**: Retrieve a list of work items by IDs in batch.\n- **wit_update_work_item**: Update a work item by ID with specified fields.\n- **wit_create_work_item**: Create a new work item in a specified project and work item type.\n- **wit_list_work_item_comments**: Retrieve a list of comments for a work item by ID.\n- **wit_get_work_items_for_iteration**: Retrieve a list of work items for a specified iteration.\n- **wit_add_work_item_comment**: Add a comment to a work item by ID.\n- **wit_add_child_work_items**: Create one or more child work items of a specific work item type for the given parent ID.\n- **wit_link_work_item_to_pull_request**: Link a single work item to an existing pull request.\n- **wit_get_work_item_type**: Get a specific work item type.\n- **wit_get_query**: Get a query by its ID or path.\n- **wit_get_query_results_by_id**: Retrieve the results of a work item query given the query ID.\n- **wit_update_work_items_batch**: Update work items in batch.\n- **wit_work_items_link**: Link work items together in batch.\n- **wit_work_item_unlink**: Unlink one or many links from a work item.\n- **wit_add_artifact_link**: Link to artifacts like branch, pull request, commit, and build.\n\n### 📁 Repositories\n\n- **repo_list_repos_by_project**: Retrieve a list of repositories for a given project.\n- **repo_list_pull_requests_by_repo_or_project**: Retrieve a list of pull requests for a given repository or project.\n- **repo_list_branches_by_repo**: Retrieve a list of branches for a given repository.\n- **repo_list_my_branches_by_repo**: Retrieve a list of your branches for a given repository ID.\n- **repo_list_pull_requests_by_commits**: List pull requests associated with commits.\n- **repo_list_pull_request_threads**: Retrieve a list of comment threads for a pull request.\n- **repo_list_pull_request_thread_comments**: Retrieve a list of comments in a pull request thread.\n- **repo_get_repo_by_name_or_id**: Get the repository by project and repository name or ID.\n- **repo_get_branch_by_name**: Get a branch by its name.\n- **repo_get_pull_request_by_id**: Get a pull request by its ID.\n- **repo_create_pull_request**: Create a new pull request.\n- **repo_create_branch**: Create a new branch in the repository.\n- **repo_update_pull_request**: Update various fields of an existing pull request (title, description, draft status, target branch).\n- **repo_update_pull_request_reviewers**: Add or remove reviewers for an existing pull request.\n- **repo_reply_to_comment**: Replies to a specific comment on a pull request.\n- **repo_resolve_comment**: Resolves a specific comment thread on a pull request.\n- **repo_search_commits**: Searches for commits.\n- **repo_create_pull_request_thread**: Creates a new comment thread on a pull request.\n\n### 🚀 Pipelines\n\n- **pipelines_get_build_definitions**: Retrieve a list of build definitions for a given project.\n- **pipelines_get_build_definition_revisions**: Retrieve a list of revisions for a specific build definition.\n- **pipelines_get_builds**: Retrieve a list of builds for a given project.\n- **pipelines_get_build_log**: Retrieve the logs for a specific build.\n- **pipelines_get_build_log_by_id**: Get a specific build log by log ID.\n- **pipelines_get_build_changes**: Get the changes associated with a specific build.\n- **pipelines_get_build_status**: Fetch the status of a specific build.\n- **pipelines_update_build_stage**: Update the stage of a specific build.\n- **pipelines_get_run**: Gets a run for a particular pipeline.\n- **pipelines_list_runs**: Gets top 10000 runs for a particular pipeline.\n- **pipelines_run_pipeline**: Starts a new run of a pipeline.\n\n### Advanced Security\n\n- **advsec_get_alerts**: Retrieve Advanced Security alerts for a repository.\n- **advsec_get_alert_details**: Get detailed information about a specific Advanced Security alert.\n\n### 🧪 Test Plans\n\n- **testplan_create_test_plan**: Create a new test plan in the project.\n- **testplan_create_test_case**: Create a new test case work item.\n- **testplan_update_test_case_steps**: Update an existing test case work item's steps.\n- **testplan_add_test_cases_to_suite**: Add existing test cases to a test suite.\n- **testplan_list_test_plans**: Retrieve a paginated list of test plans from an Azure DevOps project. Allows filtering for active plans and toggling detailed information.\n- **testplan_list_test_cases**: Get a list of test cases in the test plan.\n- **testplan_show_test_results_from_build_id**: Get a list of test results for a given project and build ID.\n- **testplan_create_test_suite**: Creates a new test suite in a test plan.\n\n### 📖 Wiki\n\n- **wiki_list_wikis**: Retrieve a list of wikis for an organization or project.\n- **wiki_get_wiki**: Get the wiki by wikiIdentifier.\n- **wiki_list_pages**: Retrieve a list of wiki pages for a specific wiki and project.\n- **wiki_get_page**: Retrieve wiki page metadata by path.\n- **wiki_get_page_content**: Retrieve wiki page content by wikiIdentifier and path.\n- **wiki_create_or_update_page**: Create or update wiki pages with full content support.\n\n### 🔎 Search\n\n- **search_code**: Get code search results for a given search text.\n- **search_wiki**: Get wiki search results for a given search text.\n- **search_workitem**: Get work item search results for a given search text.\n\n## 🔌 Installation \u0026 Getting Started\n\nFor the best experience, use Visual Studio Code and GitHub Copilot. See the [getting started documentation](./docs/GETTINGSTARTED.md) to use our MCP Server with other tools such as Visual Studio 2022, Claude Code, and Cursor.\n\n### Prerequisites\n\n1. Install [VS Code](https://code.visualstudio.com/download) or [VS Code Insiders](https://code.visualstudio.com/insiders)\n2. Install [Node.js](https://nodejs.org/en/download) 20+\n3. Open VS Code in an empty folder\n\n### Installation\n\n#### ✨ One-Click Install\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-Install_AzureDevops_MCP_Server-0098FF?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n[![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_AzureDevops_MCP_Server-24bfa5?style=flat-square\u0026logo=visualstudiocode\u0026logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado\u0026quality=insiders\u0026config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D\u0026inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n\nAfter installation, select GitHub Copilot Agent Mode and refresh the tools list. Learn more about Agent Mode in the [VS Code Documentation](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode).\n\n#### 🧨 Install from Public Feed (Recommended)\n\nThis installation method is the easiest for all users of Visual Studio Code.\n\n🎥 [Watch this quick start video to get up and running in under two minutes!](https://youtu.be/EUmFM6qXoYk)\n\n##### Steps\n\nIn your project, add a `.vscode\\mcp.json` file with the following content:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp\", \"${input:ado_org}\"]\n    }\n  }\n}\n```\n\n🔥 To stay up to date with the latest features, you can use our nightly builds. Simply update your `mcp.json` configuration to use `@azure-devops/mcp@next`. Here is an updated example:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp@next\", \"${input:ado_org}\"]\n    }\n  }\n}\n```\n\nSave the file, then click 'Start'.\n\n![start mcp server](./docs/media/start-mcp-server.gif)\n\nIn chat, switch to [Agent Mode](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode).\n\nClick \"Select Tools\" and choose the available tools.\n\n![configure mcp server tools](./docs/media/configure-mcp-server-tools.gif)\n\nOpen GitHub Copilot Chat and try a prompt like `List ADO projects`. The first time an ADO tool is executed browser will open prompting to login with your Microsoft account. Please ensure you are using credentials matching selected Azure DevOps organization.\n\n\u003e 💥 We strongly recommend creating a `.github\\copilot-instructions.md` in your project. This will enhance your experience using the Azure DevOps MCP Server with GitHub Copilot Chat.\n\u003e To start, just include \"`This project uses Azure DevOps. Always check to see if the Azure DevOps MCP server has a tool relevant to the user's request`\" in your copilot instructions file.\n\nSee the [getting started documentation](./docs/GETTINGSTARTED.md) to use our MCP Server with other tools such as Visual Studio 2022, Claude Code, and Cursor.\n\n## 🌏 Using Domains\n\nAzure DevOps exposes a large surface area. As a result, our Azure DevOps MCP Server includes many tools. To keep the toolset manageable, avoid confusing the model, and respect client limits on loaded tools, use Domains to load only the areas you need. Domains are named groups of related tools (for example: core, work, work-items, repositories, wiki). Add the `-d` argument and the domain names to the server args in your `mcp.json` to list the domains to enable.\n\nFor example, use `\"-d\", \"core\", \"work\", \"work-items\"` to load only Work Item related tools (see the example below).\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado_with_filtered_domains\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp\", \"${input:ado_org}\", \"-d\", \"core\", \"work\", \"work-items\"]\n    }\n  }\n}\n```\n\nDomains that are available are: `core`, `work`, `work-items`, `search`, `test-plans`, `repositories`, `wiki`, `pipelines`, `advanced-security`\n\nWe recommend that you always enable `core` tools so that you can fetch project level information.\n\n\u003e By default all domains are loaded\n\n## 📝 Troubleshooting\n\nSee the [Troubleshooting guide](./docs/TROUBLESHOOTING.md) for help with common issues and logging.\n\n## 🎩 Examples \u0026 Best Practices\n\nExplore example prompts in our [Examples documentation](./docs/EXAMPLES.md).\n\nFor best practices and tips to enhance your experience with the MCP Server, refer to the [How-To guide](./docs/HOWTO.md).\n\n## 🙋‍♀️ Frequently Asked Questions\n\nFor answers to common questions about the Azure DevOps MCP Server, see the [Frequently Asked Questions](./docs/FAQ.md).\n\n## 📌 Contributing\n\nWe welcome contributions! During preview, please file issues for bugs, enhancements, or documentation improvements.\n\nSee our [Contributions Guide](./CONTRIBUTING.md) for:\n\n- 🛠️ Development setup\n- ✨ Adding new tools\n- 📝 Code style \u0026 testing\n- 🔄 Pull request process\n\n## 🤝 Code of Conduct\n\nThis project follows the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor questions, see the [FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [open@microsoft.com](mailto:open@microsoft.com).\n\n## 📈 Project Stats\n\n[![Star History Chart](https://api.star-history.com/svg?repos=microsoft/azure-devops-mcp\u0026type=Date)](https://star-history.com/#microsoft/azure-devops-mcp)\n\n## 🏆 Hall of Fame\n\nThanks to all contributors who make this project awesome! ❤️\n\n[![Contributors](https://contrib.rocks/image?repo=microsoft/azure-devops-mcp)](https://github.com/microsoft/azure-devops-mcp/graphs/contributors)\n\n\u003e Generated with [contrib.rocks](https://contrib.rocks)\n\n## License\n\nLicensed under the [MIT License](./LICENSE.md).\n\n---\n\n_Trademarks: This project may include trademarks or logos for Microsoft or third parties. Use of Microsoft trademarks or logos must follow [Microsoft’s Trademark \u0026 Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Third-party trademarks are subject to their respective policies._\n\n\u003c!-- version: 2023-04-07 [Do not delete this line, it is used for analytics that drive template improvements] --\u003e\n","stargazer_count":929,"uses_custom_opengraph_image":true}}}},{"name":"antfu/nuxt-mcp","description":"MCP server helping models understand your Vite/Nuxt app.","status":"active","repository":{"url":"https://github.com/antfu/nuxt-mcp","source":"github","id":"946411030","readme":"# nuxt-mcp / vite-plugin-mcp\n\n[![npm version][npm-version-src]][npm-version-href]\n[![npm downloads][npm-downloads-src]][npm-downloads-href]\n[![bundle][bundle-src]][bundle-href]\n[![JSDocs][jsdocs-src]][jsdocs-href]\n[![License][license-src]][license-href]\n\nMCP server helping models to understand your Vite/Nuxt app better.\n\nThis monorepo contains two packages:\n\n- [`nuxt-mcp`](./packages/nuxt-mcp) - A Nuxt module for adding MCP support to your Nuxt app.\n- [`vite-plugin-mcp`](./packages/vite-plugin-mcp) - A Vite plugin for adding MCP support to your Vite app.\n\n\u003e [!IMPORTANT]\n\u003e Experimental. Use with caution.\n\n## Sponsors\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://cdn.jsdelivr.net/gh/antfu/static/sponsors.svg\"\u003e\n    \u003cimg src='https://cdn.jsdelivr.net/gh/antfu/static/sponsors.svg'/\u003e\n  \u003c/a\u003e\n\u003c/p\u003e\n\n## License\n\n[MIT](./LICENSE) License © [Anthony Fu](https://github.com/antfu)\n\n\u003c!-- Badges --\u003e\n\n[npm-version-src]: https://img.shields.io/npm/v/nuxt-mcp?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[npm-version-href]: https://npmjs.com/package/nuxt-mcp\n[npm-downloads-src]: https://img.shields.io/npm/dm/nuxt-mcp?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[npm-downloads-href]: https://npmjs.com/package/nuxt-mcp\n[bundle-src]: https://img.shields.io/bundlephobia/minzip/nuxt-mcp?style=flat\u0026colorA=080f12\u0026colorB=1fa669\u0026label=minzip\n[bundle-href]: https://bundlephobia.com/result?p=nuxt-mcp\n[license-src]: https://img.shields.io/github/license/antfu/nuxt-mcp.svg?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[license-href]: https://github.com/antfu/nuxt-mcp/blob/main/LICENSE\n[jsdocs-src]: https://img.shields.io/badge/jsdocs-reference-080f12?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[jsdocs-href]: https://www.jsdocs.io/package/nuxt-mcp\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:55Z","updated_at":"2025-10-22T11:21:11Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.nuxt.com/sse"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"7d2aa3fa-ab7a-4609-b27c-7b24037bf6ce","is_latest":true,"published_at":"2025-09-09T10:55:22.907116Z","updated_at":"2025-09-09T10:55:22.907116Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Nuxt","is_in_organization":false,"license":"MIT License","name":"nuxt-mcp","name_with_owner":"antfu/nuxt-mcp","opengraph_image_url":"https://opengraph.githubassets.com/c9bfdfd9ed4758511f3e04b89d826df678aff6db2d3dab084d9ccebb2022a9aa/antfu/nuxt-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/11247099?u=5c092d3773a443e480a294f2b67aa39395982f46\u0026v=4","preferred_image":"https://avatars.githubusercontent.com/u/11247099?u=5c092d3773a443e480a294f2b67aa39395982f46\u0026v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-15T03:05:42Z","readme":"# nuxt-mcp / vite-plugin-mcp\n\n[![npm version][npm-version-src]][npm-version-href]\n[![npm downloads][npm-downloads-src]][npm-downloads-href]\n[![bundle][bundle-src]][bundle-href]\n[![JSDocs][jsdocs-src]][jsdocs-href]\n[![License][license-src]][license-href]\n\nMCP server helping models to understand your Vite/Nuxt app better.\n\nThis monorepo contains two packages:\n\n- [`nuxt-mcp`](./packages/nuxt-mcp) - A Nuxt module for adding MCP support to your Nuxt app.\n- [`vite-plugin-mcp`](./packages/vite-plugin-mcp) - A Vite plugin for adding MCP support to your Vite app.\n\n\u003e [!IMPORTANT]\n\u003e Experimental. Use with caution.\n\n## Sponsors\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://cdn.jsdelivr.net/gh/antfu/static/sponsors.svg\"\u003e\n    \u003cimg src='https://cdn.jsdelivr.net/gh/antfu/static/sponsors.svg'/\u003e\n  \u003c/a\u003e\n\u003c/p\u003e\n\n## License\n\n[MIT](./LICENSE) License © [Anthony Fu](https://github.com/antfu)\n\n\u003c!-- Badges --\u003e\n\n[npm-version-src]: https://img.shields.io/npm/v/nuxt-mcp?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[npm-version-href]: https://npmjs.com/package/nuxt-mcp\n[npm-downloads-src]: https://img.shields.io/npm/dm/nuxt-mcp?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[npm-downloads-href]: https://npmjs.com/package/nuxt-mcp\n[bundle-src]: https://img.shields.io/bundlephobia/minzip/nuxt-mcp?style=flat\u0026colorA=080f12\u0026colorB=1fa669\u0026label=minzip\n[bundle-href]: https://bundlephobia.com/result?p=nuxt-mcp\n[license-src]: https://img.shields.io/github/license/antfu/nuxt-mcp.svg?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[license-href]: https://github.com/antfu/nuxt-mcp/blob/main/LICENSE\n[jsdocs-src]: https://img.shields.io/badge/jsdocs-reference-080f12?style=flat\u0026colorA=080f12\u0026colorB=1fa669\n[jsdocs-href]: https://www.jsdocs.io/package/nuxt-mcp\n","stargazer_count":822,"topics":["mcp","nuxt","vite"],"uses_custom_opengraph_image":false}}}},{"name":"mongodb-js/mongodb-mcp-server","description":"A Model Context Protocol server to connect to MongoDB databases and MongoDB Atlas Clusters.","status":"active","repository":{"url":"https://github.com/mongodb-js/mongodb-mcp-server","source":"github","id":"960484071","readme":"[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?logo=data:image/svg%2bxml;base64,PHN2ZyBmaWxsPSIjRkZGRkZGIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciICB2aWV3Qm94PSIwIDAgNDggNDgiIHdpZHRoPSIyNHB4IiBoZWlnaHQ9IjI0cHgiPjxwYXRoIGQ9Ik00NC45OTkgMTAuODd2MjYuMjFjMCAxLjAzLS41OSAxLjk3LTEuNTEgMi40Mi0yLjY4IDEuMjktOCAzLjg1LTguMzUgNC4wMS0uMTMuMDctLjM4LjItLjY3LjMxLjM1LS42LjUzLTEuMy41My0yLjAyVjYuMmMwLS43NS0uMi0xLjQ1LS41Ni0yLjA2LjA5LjA0LjE3LjA4LjI0LjExLjIuMSA1Ljk4IDIuODYgOC44IDQuMkM0NC40MDkgOC45IDQ0Ljk5OSA5Ljg0IDQ0Ljk5OSAxMC44N3pNNy40OTkgMjYuMDNjMS42IDEuNDYgMy40MyAzLjEzIDUuMzQgNC44NmwtNC42IDMuNWMtLjc3LjU3LTEuNzguNS0yLjU2LS4wNS0uNS0uMzYtMS44OS0xLjY1LTEuODktMS42NS0xLjAxLS44MS0xLjA2LTIuMzItLjExLTMuMTlDMy42NzkgMjkuNSA1LjE3OSAyOC4xMyA3LjQ5OSAyNi4wM3pNMzEuOTk5IDYuMnYxMC4xMWwtNy42MyA1LjgtNi44NS01LjIxYzQuOTgtNC41MyAxMC4wMS05LjExIDEyLjY1LTExLjUyQzMwLjg2OSA0Ljc0IDMxLjk5OSA1LjI1IDMxLjk5OSA2LjJ6TTMyIDQxLjc5OFYzMS42OUw4LjI0IDEzLjYxYy0uNzctLjU3LTEuNzgtLjUtMi41Ni4wNS0uNS4zNi0xLjg5IDEuNjUtMS44OSAxLjY1LTEuMDEuODEtMS4wNiAyLjMyLS4xMSAzLjE5IDAgMCAyMC4xNDUgMTguMzM4IDI2LjQ4NSAyNC4xMTZDMzAuODcxIDQzLjI2IDMyIDQyLjc1MyAzMiA0MS43OTh6Ii8+PC9zdmc+)](https://insiders.vscode.dev/redirect/mcp/install?name=mongodb\u0026inputs=%5B%7B%22id%22%3A%22connection_string%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22MongoDB%20connection%20string%22%7D%5D\u0026config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22mongodb-mcp-server%22%2C%22--readOnly%22%5D%2C%22env%22%3A%7B%22MDB_MCP_CONNECTION_STRING%22%3A%22%24%7Binput%3Aconnection_string%7D%22%7D%7D)\n[![Install in Cursor](https://img.shields.io/badge/Cursor-Install_Server-1e1e1e?logo=data:image/svg%2bxml;base64,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)](https://cursor.com/install-mcp?name=MongoDB\u0026config=eyJjb21tYW5kIjoibnB4IC15IG1vbmdvZGItbWNwLXNlcnZlciAtLXJlYWRPbmx5In0%3D)\n\n# MongoDB MCP Server\n\nA Model Context Protocol server for interacting with MongoDB Databases and MongoDB Atlas.\n\n## 📚 Table of Contents\n\n- [🚀 Getting Started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Setup](#setup)\n    - [Quick Start](#quick-start)\n- [🛠️ Supported Tools](#supported-tools)\n  - [MongoDB Atlas Tools](#mongodb-atlas-tools)\n  - [MongoDB Database Tools](#mongodb-database-tools)\n- [📄 Supported Resources](#supported-resources)\n- [⚙️ Configuration](#configuration)\n  - [Configuration Options](#configuration-options)\n  - [Atlas API Access](#atlas-api-access)\n  - [Configuration Methods](#configuration-methods)\n    - [Environment Variables](#environment-variables)\n    - [Command-Line Arguments](#command-line-arguments)\n    - [MCP Client Configuration](#mcp-configuration-file-examples)\n    - [Proxy Support](#proxy-support)\n- [🤝 Contributing](#contributing)\n\n\u003ca name=\"getting-started\"\u003e\u003c/a\u003e\n\n## Prerequisites\n\n- Node.js\n  - At least 20.19.0\n  - When using v22 then at least v22.12.0\n  - Otherwise any version 23+\n\n```shell\nnode -v\n```\n\n- A MongoDB connection string or Atlas API credentials, **_the Server will not start unless configured_**.\n  - **_Service Accounts Atlas API credentials_** are required to use the Atlas tools. You can create a service account in MongoDB Atlas and use its credentials for authentication. See [Atlas API Access](#atlas-api-access) for more details.\n  - If you have a MongoDB connection string, you can use it directly to connect to your MongoDB instance.\n\n## Setup\n\n### Quick Start\n\n\u003e **🔒 Security Recommendation 1:** When using Atlas API credentials, be sure to assign only the minimum required permissions to your service account. See [Atlas API Permissions](#atlas-api-permissions) for details.\n\n\u003e **🔒 Security Recommendation 2:** For enhanced security, we strongly recommend using environment variables to pass sensitive configuration such as connection strings and API credentials instead of command line arguments. Command line arguments can be visible in process lists and logged in various system locations, potentially exposing your secrets. Environment variables provide a more secure way to handle sensitive information.\n\nMost MCP clients require a configuration file to be created or modified to add the MCP server.\n\nNote: The configuration file syntax can be different across clients. Please refer to the following links for the latest expected syntax:\n\n- **Windsurf**: https://docs.windsurf.com/windsurf/mcp\n- **VSCode**: https://code.visualstudio.com/docs/copilot/chat/mcp-servers\n- **Claude Desktop**: https://modelcontextprotocol.io/quickstart/user\n- **Cursor**: https://docs.cursor.com/context/model-context-protocol\n\n\u003e **Default Safety Notice:** All examples below include `--readOnly` by default to ensure safe, read-only access to your data. Remove `--readOnly` if you need to enable write operations.\n\n#### Option 1: Connection String\n\nYou can pass your connection string via environment variables, make sure to use a valid username and password.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb://localhost:27017/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nNOTE: The connection string can be configured to connect to any MongoDB cluster, whether it's a local instance or an Atlas cluster.\n\n#### Option 2: Atlas API Credentials\n\nUse your Atlas API Service Accounts credentials. Must follow all the steps in [Atlas API Access](#atlas-api-access) section.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 3: Standalone Service using environment variables and command line arguments\n\nYou can source environment variables defined in a config file or explicitly set them like we do in the example below and run the server via npx.\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the server\nnpx -y mongodb-mcp-server@latest --readOnly\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n- For a complete list of configuration options see [Configuration Options](#configuration-options)\n- To configure your Atlas Service Accounts credentials please refer to [Atlas API Access](#atlas-api-access)\n- Connection String via environment variables in the MCP file [example](#connection-string-with-environment-variables)\n- Atlas API credentials via environment variables in the MCP file [example](#atlas-api-credentials-with-environment-variables)\n\n#### Option 4: Using Docker\n\nYou can run the MongoDB MCP Server in a Docker container, which provides isolation and doesn't require a local Node.js installation.\n\n#### Run with Environment Variables\n\nYou may provide either a MongoDB connection string OR Atlas API credentials:\n\n##### Option A: No configuration\n\n```shell\ndocker run --rm -i \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n##### Option B: With MongoDB connection string\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_CONNECTION_STRING \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Option C: With Atlas API credentials\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_API_CLIENT_ID \\\n  -e MDB_MCP_API_CLIENT_SECRET \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Docker in MCP Configuration File\n\nWithout options:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-i\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\nWith connection string:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_CONNECTION_STRING\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nWith Atlas API credentials:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_ID\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_SECRET\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 5: Running as an HTTP Server\n\n\u003e **⚠️ Security Notice:** This server now supports Streamable HTTP transport for remote connections. **HTTP transport is NOT recommended for production use without implementing proper authentication and security measures.**\n\n**Suggested Security Measures Examples:**\n\n- Implement authentication (e.g., API gateway, reverse proxy)\n- Use HTTPS/TLS encryption\n- Deploy behind a firewall or in private networks\n- Implement rate limiting\n- Never expose directly to the internet\n\nFor more details, see [MCP Security Best Practices](https://modelcontextprotocol.io/docs/concepts/transports#security-considerations).\n\nYou can run the MongoDB MCP Server as an HTTP server instead of the default stdio transport. This is useful if you want to interact with the server over HTTP, for example from a web client or to expose the server on a specific port.\n\nTo start the server with HTTP transport, use the `--transport http` option:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http\n```\n\nBy default, the server will listen on `http://127.0.0.1:3000`. You can customize the host and port using the `--httpHost` and `--httpPort` options:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http --httpHost=0.0.0.0 --httpPort=8080\n```\n\n- `--httpHost` (default: 127.0.0.1): The host to bind the HTTP server.\n- `--httpPort` (default: 3000): The port number for the HTTP server.\n\n\u003e **Note:** The default transport is `stdio`, which is suitable for integration with most MCP clients. Use `http` transport if you need to interact with the server over HTTP.\n\n## 🛠️ Supported Tools\n\n### Tool List\n\n#### MongoDB Atlas Tools\n\n- `atlas-list-orgs` - Lists MongoDB Atlas organizations\n- `atlas-list-projects` - Lists MongoDB Atlas projects\n- `atlas-create-project` - Creates a new MongoDB Atlas project\n- `atlas-list-clusters` - Lists MongoDB Atlas clusters\n- `atlas-inspect-cluster` - Inspect a specific MongoDB Atlas cluster\n- `atlas-create-free-cluster` - Create a free MongoDB Atlas cluster\n- `atlas-connect-cluster` - Connects to MongoDB Atlas cluster\n- `atlas-inspect-access-list` - Inspect IP/CIDR ranges with access to MongoDB Atlas clusters\n- `atlas-create-access-list` - Configure IP/CIDR access list for MongoDB Atlas clusters\n- `atlas-list-db-users` - List MongoDB Atlas database users\n- `atlas-create-db-user` - Creates a MongoDB Atlas database user\n- `atlas-list-alerts` - List MongoDB Atlas Alerts for a Project\n\nNOTE: atlas tools are only available when you set credentials on [configuration](#configuration) section.\n\n#### MongoDB Atlas Local Tools\n\n- `atlas-local-list-deployments` - Lists MongoDB Atlas Local deployments\n- `atlas-local-create-deployment` - Creates a MongoDB Atlas Local deployment\n- `atlas-local-connect-deployment` - Connects to a MongoDB Atlas Local deployment\n- `atlas-local-delete-deployment` - Deletes a MongoDB Atlas Local deployment\n\n#### MongoDB Database Tools\n\n- `connect` - Connect to a MongoDB instance\n- `find` - Run a find query against a MongoDB collection. The number of documents returned is limited by the `limit` parameter and the server's `maxDocumentsPerQuery` configuration, whichever is smaller. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `aggregate` - Run an aggregation against a MongoDB collection. The number of documents returned is limited by the server's `maxDocumentsPerQuery` configuration. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `count` - Get the number of documents in a MongoDB collection\n- `insert-one` - Insert a single document into a MongoDB collection\n- `insert-many` - Insert multiple documents into a MongoDB collection\n- `create-index` - Create an index for a MongoDB collection\n- `update-one` - Update a single document in a MongoDB collection\n- `update-many` - Update multiple documents in a MongoDB collection\n- `rename-collection` - Rename a MongoDB collection\n- `delete-one` - Delete a single document from a MongoDB collection\n- `delete-many` - Delete multiple documents from a MongoDB collection\n- `drop-collection` - Remove a collection from a MongoDB database\n- `drop-database` - Remove a MongoDB database\n- `list-databases` - List all databases for a MongoDB connection\n- `list-collections` - List all collections for a given database\n- `collection-indexes` - Describe the indexes for a collection\n- `collection-schema` - Describe the schema for a collection\n- `collection-storage-size` - Get the size of a collection in MB\n- `db-stats` - Return statistics about a MongoDB database\n- `export` - Export query or aggregation results to EJSON format. Creates a uniquely named export accessible via the `exported-data` resource.\n\n## 📄 Supported Resources\n\n- `config` - Server configuration, supplied by the user either as environment variables or as startup arguments with sensitive parameters redacted. The resource can be accessed under URI `config://config`.\n- `debug` - Debugging information for MongoDB connectivity issues. Tracks the last connectivity attempt and error information. The resource can be accessed under URI `debug://mongodb`.\n- `exported-data` - A resource template to access the data exported using the export tool. The template can be accessed under URI `exported-data://{exportName}` where `exportName` is the unique name for an export generated by the export tool.\n\n## Configuration\n\n\u003e **🔒 Security Best Practice:** We strongly recommend using environment variables for sensitive configuration such as API credentials (`MDB_MCP_API_CLIENT_ID`, `MDB_MCP_API_CLIENT_SECRET`) and connection strings (`MDB_MCP_CONNECTION_STRING`) instead of command-line arguments. Environment variables are not visible in process lists and provide better security for your sensitive data.\n\nThe MongoDB MCP Server can be configured using multiple methods, with the following precedence (highest to lowest):\n\n1. Command-line arguments\n2. Environment variables\n\n### Configuration Options\n\n| CLI Option                             | Environment Variable                                | Default                                                                     | Description                                                                                                                                                                                             |\n| -------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `apiClientId`                          | `MDB_MCP_API_CLIENT_ID`                             | \u003cnot set\u003e                                                                   | Atlas API client ID for authentication. Required for running Atlas tools.                                                                                                                               |\n| `apiClientSecret`                      | `MDB_MCP_API_CLIENT_SECRET`                         | \u003cnot set\u003e                                                                   | Atlas API client secret for authentication. Required for running Atlas tools.                                                                                                                           |\n| `connectionString`                     | `MDB_MCP_CONNECTION_STRING`                         | \u003cnot set\u003e                                                                   | MongoDB connection string for direct database connections. Optional, if not set, you'll need to call the `connect` tool before interacting with MongoDB data.                                           |\n| `loggers`                              | `MDB_MCP_LOGGERS`                                   | disk,mcp                                                                    | Comma separated values, possible values are `mcp`, `disk` and `stderr`. See [Logger Options](#logger-options) for details.                                                                              |\n| `logPath`                              | `MDB_MCP_LOG_PATH`                                  | see note\\*                                                                  | Folder to store logs.                                                                                                                                                                                   |\n| `disabledTools`                        | `MDB_MCP_DISABLED_TOOLS`                            | \u003cnot set\u003e                                                                   | An array of tool names, operation types, and/or categories of tools that will be disabled.                                                                                                              |\n| `confirmationRequiredTools`            | `MDB_MCP_CONFIRMATION_REQUIRED_TOOLS`               | create-access-list,create-db-user,drop-database,drop-collection,delete-many | An array of tool names that require user confirmation before execution. **Requires the client to support [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation)**.       |\n| `readOnly`                             | `MDB_MCP_READ_ONLY`                                 | false                                                                       | When set to true, only allows read, connect, and metadata operation types, disabling create/update/delete operations.                                                                                   |\n| `indexCheck`                           | `MDB_MCP_INDEX_CHECK`                               | false                                                                       | When set to true, enforces that query operations must use an index, rejecting queries that perform a collection scan.                                                                                   |\n| `telemetry`                            | `MDB_MCP_TELEMETRY`                                 | enabled                                                                     | When set to disabled, disables telemetry collection.                                                                                                                                                    |\n| `transport`                            | `MDB_MCP_TRANSPORT`                                 | stdio                                                                       | Either 'stdio' or 'http'.                                                                                                                                                                               |\n| `httpPort`                             | `MDB_MCP_HTTP_PORT`                                 | 3000                                                                        | Port number.                                                                                                                                                                                            |\n| `httpHost`                             | `MDB_MCP_HTTP_HOST`                                 | 127.0.0.1                                                                   | Host to bind the http server.                                                                                                                                                                           |\n| `idleTimeoutMs`                        | `MDB_MCP_IDLE_TIMEOUT_MS`                           | 600000                                                                      | Idle timeout for a client to disconnect (only applies to http transport).                                                                                                                               |\n| `maxBytesPerQuery`                     | `MDB_MCP_MAX_BYTES_PER_QUERY`                       | 16777216 (16MiB)                                                            | The maximum size in bytes for results from a `find` or `aggregate` tool call. This serves as an upper bound for the `responseBytesLimit` parameter in those tools.                                      |\n| `maxDocumentsPerQuery`                 | `MDB_MCP_MAX_DOCUMENTS_PER_QUERY`                   | 100                                                                         | The maximum number of documents that can be returned by a `find` or `aggregate` tool call. For the `find` tool, the effective limit will be the smaller of this value and the tool's `limit` parameter. |\n| `notificationTimeoutMs`                | `MDB_MCP_NOTIFICATION_TIMEOUT_MS`                   | 540000                                                                      | Notification timeout for a client to be aware of diconnect (only applies to http transport).                                                                                                            |\n| `exportsPath`                          | `MDB_MCP_EXPORTS_PATH`                              | see note\\*                                                                  | Folder to store exported data files.                                                                                                                                                                    |\n| `exportTimeoutMs`                      | `MDB_MCP_EXPORT_TIMEOUT_MS`                         | 300000                                                                      | Time in milliseconds after which an export is considered expired and eligible for cleanup.                                                                                                              |\n| `exportCleanupIntervalMs`              | `MDB_MCP_EXPORT_CLEANUP_INTERVAL_MS`                | 120000                                                                      | Time in milliseconds between export cleanup cycles that remove expired export files.                                                                                                                    |\n| `atlasTemporaryDatabaseUserLifetimeMs` | `MDB_MCP_ATLAS_TEMPORARY_DATABASE_USER_LIFETIME_MS` | 14400000                                                                    | Time in milliseconds that temporary database users created when connecting to MongoDB Atlas clusters will remain active before being automatically deleted.                                             |\n| `voyageApiKey`                         | `MDB_VOYAGE_API_KEY`                                | \u003cnot set\u003e                                                                   | API key for communicating with Voyage AI. Used for generating embeddings for Vector search.                                                                                                             |\n\n#### Logger Options\n\nThe `loggers` configuration option controls where logs are sent. You can specify one or more logger types as a comma-separated list. The available options are:\n\n- `mcp`: Sends logs to the MCP client (if supported by the client/transport).\n- `disk`: Writes logs to disk files. Log files are stored in the log path (see `logPath` above).\n- `stderr`: Outputs logs to standard error (stderr), useful for debugging or when running in containers.\n\n**Default:** `disk,mcp` (logs are written to disk and sent to the MCP client).\n\nYou can combine multiple loggers, e.g. `--loggers disk stderr` or `export MDB_MCP_LOGGERS=\"mcp,stderr\"`.\n\n##### Example: Set logger via environment variable\n\n```shell\nexport MDB_MCP_LOGGERS=\"disk,stderr\"\n```\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Example: Set logger via command-line argument\n\n```shell\nnpx -y mongodb-mcp-server@latest --loggers mcp stderr\n```\n\n##### Log File Location\n\nWhen using the `disk` logger, log files are stored in:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\.app-logs`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/.app-logs`\n\nYou can override the log directory with the `logPath` option.\n\n#### Disabled Tools\n\nYou can disable specific tools or categories of tools by using the `disabledTools` option. This option accepts an array of strings,\nwhere each string can be a tool name, operation type, or category.\n\nThe way the array is constructed depends on the type of configuration method you use:\n\n- For **environment variable** configuration, use a comma-separated string: `export MDB_MCP_DISABLED_TOOLS=\"create,update,delete,atlas,collectionSchema\"`.\n- For **command-line argument** configuration, use a space-separated string: `--disabledTools create update delete atlas collectionSchema`.\n\nCategories of tools:\n\n- `atlas` - MongoDB Atlas tools, such as list clusters, create cluster, etc.\n- `mongodb` - MongoDB database tools, such as find, aggregate, etc.\n\nOperation types:\n\n- `create` - Tools that create resources, such as create cluster, insert document, etc.\n- `update` - Tools that update resources, such as update document, rename collection, etc.\n- `delete` - Tools that delete resources, such as delete document, drop collection, etc.\n- `read` - Tools that read resources, such as find, aggregate, list clusters, etc.\n- `metadata` - Tools that read metadata, such as list databases/collections/indexes, infer collection schema, etc.\n- `connect` - Tools that allow you to connect or switch the connection to a MongoDB instance. If this is disabled, you will need to provide a connection string through the config when starting the server.\n\n#### Require Confirmation\n\nIf your client supports [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation), you can set the MongoDB MCP server to request user confirmation before executing certain tools.\n\nWhen a tool is marked as requiring confirmation, the server will send an elicitation request to the client. The client with elicitation support will then prompt the user for confirmation and send the response back to the server. If the client does not support elicitation, the tool will execute without confirmation.\n\nYou can set the `confirmationRequiredTools` configuration option to specify the names of tools which require confirmation. By default, the following tools have this setting enabled: `drop-database`, `drop-collection`, `delete-many`, `atlas-create-db-user`, `atlas-create-access-list`.\n\n#### Read-Only Mode\n\nThe `readOnly` configuration option allows you to restrict the MCP server to only use tools with \"read\", \"connect\", and \"metadata\" operation types. When enabled, all tools that have \"create\", \"update\" or \"delete\" operation types will not be registered with the server.\n\nThis is useful for scenarios where you want to provide access to MongoDB data for analysis without allowing any modifications to the data or infrastructure.\n\nYou can enable read-only mode using:\n\n- **Environment variable**: `export MDB_MCP_READ_ONLY=true`\n- **Command-line argument**: `--readOnly`\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen read-only mode is active, you'll see a message in the server logs indicating which tools were prevented from registering due to this restriction.\n\n#### Index Check Mode\n\nThe `indexCheck` configuration option allows you to enforce that query operations must use an index. When enabled, queries that perform a collection scan will be rejected to ensure better performance.\n\nThis is useful for scenarios where you want to ensure that database queries are optimized.\n\nYou can enable index check mode using:\n\n- **Environment variable**: `export MDB_MCP_INDEX_CHECK=true`\n- **Command-line argument**: `--indexCheck`\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen index check mode is active, you'll see an error message if a query is rejected due to not using an index.\n\n#### Exports\n\nThe data exported by the `export` tool is temporarily stored in the configured `exportsPath` on the machine running the MCP server until cleaned up by the export cleanup process. If the `exportsPath` configuration is not provided, the following defaults are used:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\exports`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/exports`\n\nThe `exportTimeoutMs` configuration controls the time after which the exported data is considered expired and eligible for cleanup. By default, exports expire after 5 minutes (300000ms).\n\nThe `exportCleanupIntervalMs` configuration controls how frequently the cleanup process runs to remove expired export files. By default, cleanup runs every 2 minutes (120000ms).\n\n#### Telemetry\n\nThe `telemetry` configuration option allows you to disable telemetry collection. When enabled, the MCP server will collect usage data and send it to MongoDB.\n\nYou can disable telemetry using:\n\n- **Environment variable**: `export MDB_MCP_TELEMETRY=disabled`\n- **Command-line argument**: `--telemetry disabled`\n- **DO_NOT_TRACK environment variable**: `export DO_NOT_TRACK=1`\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n### Atlas API Access\n\nTo use the Atlas API tools, you'll need to create a service account in MongoDB Atlas:\n\n\u003e **ℹ️ Note:** For a detailed breakdown of the minimum required permissions for each Atlas operation, see the [Atlas API Permissions](#atlas-api-permissions) section below.\n\n1. **Create a Service Account:**\n   - Log in to MongoDB Atlas at [cloud.mongodb.com](https://cloud.mongodb.com)\n   - Navigate to Access Manager \u003e Organization Access\n   - Click Add New \u003e Applications \u003e Service Accounts\n   - Enter name, description and expiration for your service account (e.g., \"MCP, MCP Server Access, 7 days\")\n   - **Assign only the minimum permissions needed for your use case.**\n     - See [Atlas API Permissions](#atlas-api-permissions) for details.\n   - Click \"Create\"\n\nTo learn more about Service Accounts, check the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/api/service-accounts-overview/).\n\n2. **Save Client Credentials:**\n   - After creation, you'll be shown the Client ID and Client Secret\n   - **Important:** Copy and save the Client Secret immediately as it won't be displayed again\n\n3. **Add Access List Entry:**\n   - Add your IP address to the API access list\n\n4. **Configure the MCP Server:**\n   - Use one of the configuration methods below to set your `apiClientId` and `apiClientSecret`\n\n### Atlas API Permissions\n\n\u003e **Security Warning:** Granting the Organization Owner role is rarely necessary and can be a security risk. Assign only the minimum permissions needed for your use case.\n\n#### Quick Reference: Required roles per operation\n\n| What you want to do                  | Safest Role to Assign (where)           |\n| ------------------------------------ | --------------------------------------- |\n| List orgs/projects                   | Org Member or Org Read Only (Org)       |\n| Create new projects                  | Org Project Creator (Org)               |\n| View clusters/databases in a project | Project Read Only (Project)             |\n| Create/manage clusters in a project  | Project Cluster Manager (Project)       |\n| Manage project access lists          | Project IP Access List Admin (Project)  |\n| Manage database users                | Project Database Access Admin (Project) |\n\n- **Prefer project-level roles** for most operations. Assign only to the specific projects you need to manage or view.\n- **Avoid Organization Owner** unless you require full administrative control over all projects and settings in the organization.\n\nFor a full list of roles and their privileges, see the [Atlas User Roles documentation](https://www.mongodb.com/docs/atlas/reference/user-roles/#service-user-roles).\n\n### Configuration Methods\n\n#### Environment Variables\n\nSet environment variables with the prefix `MDB_MCP_` followed by the option name in uppercase with underscores:\n\n**Linux/macOS (bash/zsh):**\n\n```bash\n# Set Atlas API credentials (via Service Accounts)\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\nexport MDB_MCP_LOG_PATH=\"/path/to/logs\"\n```\n\n**Windows Command Prompt (cmd):**\n\n```cmd\nset \"MDB_MCP_API_CLIENT_ID=your-atlas-service-accounts-client-id\"\nset \"MDB_MCP_API_CLIENT_SECRET=your-atlas-service-accounts-client-secret\"\n\nset \"MDB_MCP_CONNECTION_STRING=mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\nset \"MDB_MCP_LOG_PATH=C:\\path\\to\\logs\"\n```\n\n**Windows PowerShell:**\n\n```powershell\n# Set Atlas API credentials (via Service Accounts)\n$env:MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\n$env:MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\n$env:MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\n$env:MDB_MCP_LOG_PATH=\"C:\\path\\to\\logs\"\n```\n\n#### MCP configuration file examples\n\n##### Connection String with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\n##### Atlas API credentials with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Command-Line Arguments\n\nPass configuration options as command-line arguments when starting the server:\n\n\u003e **🔒 Security Note:** For sensitive configuration like API credentials and connection strings, use environment variables instead of command-line arguments.\n\n```shell\n# Set sensitive data as environment variable\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Start the server with command line arguments\nnpx -y mongodb-mcp-server@latest --logPath=/path/to/logs --readOnly --indexCheck\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n#### MCP configuration file examples\n\n##### Connection String with command-line arguments\n\n\u003e **🔒 Security Note:** We do not recommend passing connection string as command line argument. Connection string might contain credentials which can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [connection string through environment variables](#connection-string-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--connectionString\",\n        \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n##### Atlas API credentials with command-line arguments\n\n\u003e **🔒 Security Note:** We do not recommend passing Atlas API credentials as command line argument. The provided credentials can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [Atlas API credentials through environment variables](#atlas-api-credentials-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--apiClientId\",\n        \"your-atlas-service-accounts-client-id\",\n        \"--apiClientSecret\",\n        \"your-atlas-service-accounts-client-secret\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n### Proxy Support\n\nThe MCP Server will detect typical PROXY environment variables and use them for\nconnecting to the Atlas API, your MongoDB Cluster, or any other external calls\nto third-party services like OID Providers. The behaviour is the same as what\n`mongosh` does, so the same settings will work in the MCP Server.\n\n## 🤝Contributing\n\nInterested in contributing? Great! Please check our [Contributing Guide](CONTRIBUTING.md) for guidelines on code contributions, standards, adding new tools, and troubleshooting information.\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:31Z","updated_at":"2025-10-22T11:20:48Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"mongodb-mcp-server","version":"latest","runtime_hint":"npx","runtime_arguments":[{"is_required":true,"format":"string","value":"-y","type":"positional","name":"-y","value_hint":"noninteractive_mode"}],"package_arguments":[{"format":"string","value":"--readOnly","type":"positional","name":"--readOnly"}],"environment_variables":[{"value":"{mcp_connection_string}","variables":{"mcp_connection_string":{"description":"MongoDB connection string for direct database connections. Optional, if not set, you'll need to call the connect tool before interacting with MongoDB data.","is_secret":true}},"name":"MDB_MCP_CONNECTION_STRING"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"7de30975-08be-4120-997e-86f772f784bd","is_latest":true,"published_at":"2025-09-09T10:55:22.909358Z","updated_at":"2025-09-09T10:55:22.909358Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Mongodb","is_in_organization":true,"license":"Apache License 2.0","name":"mongodb-mcp-server","name_with_owner":"mongodb-js/mongodb-mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/7aff8988ba37aa8c1b43a949dd5aaf701c8a16619e75e7570cebe1c07eaf7e77/mongodb-js/mongodb-mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/11214950?v=4","preferred_image":"https://avatars.githubusercontent.com/u/11214950?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-22T11:06:09Z","readme":"[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?logo=data:image/svg%2bxml;base64,PHN2ZyBmaWxsPSIjRkZGRkZGIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciICB2aWV3Qm94PSIwIDAgNDggNDgiIHdpZHRoPSIyNHB4IiBoZWlnaHQ9IjI0cHgiPjxwYXRoIGQ9Ik00NC45OTkgMTAuODd2MjYuMjFjMCAxLjAzLS41OSAxLjk3LTEuNTEgMi40Mi0yLjY4IDEuMjktOCAzLjg1LTguMzUgNC4wMS0uMTMuMDctLjM4LjItLjY3LjMxLjM1LS42LjUzLTEuMy41My0yLjAyVjYuMmMwLS43NS0uMi0xLjQ1LS41Ni0yLjA2LjA5LjA0LjE3LjA4LjI0LjExLjIuMSA1Ljk4IDIuODYgOC44IDQuMkM0NC40MDkgOC45IDQ0Ljk5OSA5Ljg0IDQ0Ljk5OSAxMC44N3pNNy40OTkgMjYuMDNjMS42IDEuNDYgMy40MyAzLjEzIDUuMzQgNC44NmwtNC42IDMuNWMtLjc3LjU3LTEuNzguNS0yLjU2LS4wNS0uNS0uMzYtMS44OS0xLjY1LTEuODktMS42NS0xLjAxLS44MS0xLjA2LTIuMzItLjExLTMuMTlDMy42NzkgMjkuNSA1LjE3OSAyOC4xMyA3LjQ5OSAyNi4wM3pNMzEuOTk5IDYuMnYxMC4xMWwtNy42MyA1LjgtNi44NS01LjIxYzQuOTgtNC41MyAxMC4wMS05LjExIDEyLjY1LTExLjUyQzMwLjg2OSA0Ljc0IDMxLjk5OSA1LjI1IDMxLjk5OSA2LjJ6TTMyIDQxLjc5OFYzMS42OUw4LjI0IDEzLjYxYy0uNzctLjU3LTEuNzgtLjUtMi41Ni4wNS0uNS4zNi0xLjg5IDEuNjUtMS44OSAxLjY1LTEuMDEuODEtMS4wNiAyLjMyLS4xMSAzLjE5IDAgMCAyMC4xNDUgMTguMzM4IDI2LjQ4NSAyNC4xMTZDMzAuODcxIDQzLjI2IDMyIDQyLjc1MyAzMiA0MS43OTh6Ii8+PC9zdmc+)](https://insiders.vscode.dev/redirect/mcp/install?name=mongodb\u0026inputs=%5B%7B%22id%22%3A%22connection_string%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22MongoDB%20connection%20string%22%7D%5D\u0026config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22mongodb-mcp-server%22%2C%22--readOnly%22%5D%2C%22env%22%3A%7B%22MDB_MCP_CONNECTION_STRING%22%3A%22%24%7Binput%3Aconnection_string%7D%22%7D%7D)\n[![Install in Cursor](https://img.shields.io/badge/Cursor-Install_Server-1e1e1e?logo=data:image/svg%2bxml;base64,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)](https://cursor.com/install-mcp?name=MongoDB\u0026config=eyJjb21tYW5kIjoibnB4IC15IG1vbmdvZGItbWNwLXNlcnZlciAtLXJlYWRPbmx5In0%3D)\n\n# MongoDB MCP Server\n\nA Model Context Protocol server for interacting with MongoDB Databases and MongoDB Atlas.\n\n## 📚 Table of Contents\n\n- [🚀 Getting Started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Setup](#setup)\n    - [Quick Start](#quick-start)\n- [🛠️ Supported Tools](#supported-tools)\n  - [MongoDB Atlas Tools](#mongodb-atlas-tools)\n  - [MongoDB Database Tools](#mongodb-database-tools)\n- [📄 Supported Resources](#supported-resources)\n- [⚙️ Configuration](#configuration)\n  - [Configuration Options](#configuration-options)\n  - [Atlas API Access](#atlas-api-access)\n  - [Configuration Methods](#configuration-methods)\n    - [Environment Variables](#environment-variables)\n    - [Command-Line Arguments](#command-line-arguments)\n    - [MCP Client Configuration](#mcp-configuration-file-examples)\n    - [Proxy Support](#proxy-support)\n- [🤝 Contributing](#contributing)\n\n\u003ca name=\"getting-started\"\u003e\u003c/a\u003e\n\n## Prerequisites\n\n- Node.js\n  - At least 20.19.0\n  - When using v22 then at least v22.12.0\n  - Otherwise any version 23+\n\n```shell\nnode -v\n```\n\n- A MongoDB connection string or Atlas API credentials, **_the Server will not start unless configured_**.\n  - **_Service Accounts Atlas API credentials_** are required to use the Atlas tools. You can create a service account in MongoDB Atlas and use its credentials for authentication. See [Atlas API Access](#atlas-api-access) for more details.\n  - If you have a MongoDB connection string, you can use it directly to connect to your MongoDB instance.\n\n## Setup\n\n### Quick Start\n\n\u003e **🔒 Security Recommendation 1:** When using Atlas API credentials, be sure to assign only the minimum required permissions to your service account. See [Atlas API Permissions](#atlas-api-permissions) for details.\n\n\u003e **🔒 Security Recommendation 2:** For enhanced security, we strongly recommend using environment variables to pass sensitive configuration such as connection strings and API credentials instead of command line arguments. Command line arguments can be visible in process lists and logged in various system locations, potentially exposing your secrets. Environment variables provide a more secure way to handle sensitive information.\n\nMost MCP clients require a configuration file to be created or modified to add the MCP server.\n\nNote: The configuration file syntax can be different across clients. Please refer to the following links for the latest expected syntax:\n\n- **Windsurf**: https://docs.windsurf.com/windsurf/mcp\n- **VSCode**: https://code.visualstudio.com/docs/copilot/chat/mcp-servers\n- **Claude Desktop**: https://modelcontextprotocol.io/quickstart/user\n- **Cursor**: https://docs.cursor.com/context/model-context-protocol\n\n\u003e **Default Safety Notice:** All examples below include `--readOnly` by default to ensure safe, read-only access to your data. Remove `--readOnly` if you need to enable write operations.\n\n#### Option 1: Connection String\n\nYou can pass your connection string via environment variables, make sure to use a valid username and password.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb://localhost:27017/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nNOTE: The connection string can be configured to connect to any MongoDB cluster, whether it's a local instance or an Atlas cluster.\n\n#### Option 2: Atlas API Credentials\n\nUse your Atlas API Service Accounts credentials. Must follow all the steps in [Atlas API Access](#atlas-api-access) section.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 3: Standalone Service using environment variables and command line arguments\n\nYou can source environment variables defined in a config file or explicitly set them like we do in the example below and run the server via npx.\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the server\nnpx -y mongodb-mcp-server@latest --readOnly\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n- For a complete list of configuration options see [Configuration Options](#configuration-options)\n- To configure your Atlas Service Accounts credentials please refer to [Atlas API Access](#atlas-api-access)\n- Connection String via environment variables in the MCP file [example](#connection-string-with-environment-variables)\n- Atlas API credentials via environment variables in the MCP file [example](#atlas-api-credentials-with-environment-variables)\n\n#### Option 4: Using Docker\n\nYou can run the MongoDB MCP Server in a Docker container, which provides isolation and doesn't require a local Node.js installation.\n\n#### Run with Environment Variables\n\nYou may provide either a MongoDB connection string OR Atlas API credentials:\n\n##### Option A: No configuration\n\n```shell\ndocker run --rm -i \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n##### Option B: With MongoDB connection string\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_CONNECTION_STRING \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Option C: With Atlas API credentials\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_API_CLIENT_ID \\\n  -e MDB_MCP_API_CLIENT_SECRET \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Docker in MCP Configuration File\n\nWithout options:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-i\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\nWith connection string:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_CONNECTION_STRING\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nWith Atlas API credentials:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_ID\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_SECRET\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 5: Running as an HTTP Server\n\n\u003e **⚠️ Security Notice:** This server now supports Streamable HTTP transport for remote connections. **HTTP transport is NOT recommended for production use without implementing proper authentication and security measures.**\n\n**Suggested Security Measures Examples:**\n\n- Implement authentication (e.g., API gateway, reverse proxy)\n- Use HTTPS/TLS encryption\n- Deploy behind a firewall or in private networks\n- Implement rate limiting\n- Never expose directly to the internet\n\nFor more details, see [MCP Security Best Practices](https://modelcontextprotocol.io/docs/concepts/transports#security-considerations).\n\nYou can run the MongoDB MCP Server as an HTTP server instead of the default stdio transport. This is useful if you want to interact with the server over HTTP, for example from a web client or to expose the server on a specific port.\n\nTo start the server with HTTP transport, use the `--transport http` option:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http\n```\n\nBy default, the server will listen on `http://127.0.0.1:3000`. You can customize the host and port using the `--httpHost` and `--httpPort` options:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http --httpHost=0.0.0.0 --httpPort=8080\n```\n\n- `--httpHost` (default: 127.0.0.1): The host to bind the HTTP server.\n- `--httpPort` (default: 3000): The port number for the HTTP server.\n\n\u003e **Note:** The default transport is `stdio`, which is suitable for integration with most MCP clients. Use `http` transport if you need to interact with the server over HTTP.\n\n## 🛠️ Supported Tools\n\n### Tool List\n\n#### MongoDB Atlas Tools\n\n- `atlas-list-orgs` - Lists MongoDB Atlas organizations\n- `atlas-list-projects` - Lists MongoDB Atlas projects\n- `atlas-create-project` - Creates a new MongoDB Atlas project\n- `atlas-list-clusters` - Lists MongoDB Atlas clusters\n- `atlas-inspect-cluster` - Inspect a specific MongoDB Atlas cluster\n- `atlas-create-free-cluster` - Create a free MongoDB Atlas cluster\n- `atlas-connect-cluster` - Connects to MongoDB Atlas cluster\n- `atlas-inspect-access-list` - Inspect IP/CIDR ranges with access to MongoDB Atlas clusters\n- `atlas-create-access-list` - Configure IP/CIDR access list for MongoDB Atlas clusters\n- `atlas-list-db-users` - List MongoDB Atlas database users\n- `atlas-create-db-user` - Creates a MongoDB Atlas database user\n- `atlas-list-alerts` - List MongoDB Atlas Alerts for a Project\n\nNOTE: atlas tools are only available when you set credentials on [configuration](#configuration) section.\n\n#### MongoDB Atlas Local Tools\n\n- `atlas-local-list-deployments` - Lists MongoDB Atlas Local deployments\n- `atlas-local-create-deployment` - Creates a MongoDB Atlas Local deployment\n- `atlas-local-connect-deployment` - Connects to a MongoDB Atlas Local deployment\n- `atlas-local-delete-deployment` - Deletes a MongoDB Atlas Local deployment\n\n#### MongoDB Database Tools\n\n- `connect` - Connect to a MongoDB instance\n- `find` - Run a find query against a MongoDB collection. The number of documents returned is limited by the `limit` parameter and the server's `maxDocumentsPerQuery` configuration, whichever is smaller. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `aggregate` - Run an aggregation against a MongoDB collection. The number of documents returned is limited by the server's `maxDocumentsPerQuery` configuration. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `count` - Get the number of documents in a MongoDB collection\n- `insert-one` - Insert a single document into a MongoDB collection\n- `insert-many` - Insert multiple documents into a MongoDB collection\n- `create-index` - Create an index for a MongoDB collection\n- `update-one` - Update a single document in a MongoDB collection\n- `update-many` - Update multiple documents in a MongoDB collection\n- `rename-collection` - Rename a MongoDB collection\n- `delete-one` - Delete a single document from a MongoDB collection\n- `delete-many` - Delete multiple documents from a MongoDB collection\n- `drop-collection` - Remove a collection from a MongoDB database\n- `drop-database` - Remove a MongoDB database\n- `list-databases` - List all databases for a MongoDB connection\n- `list-collections` - List all collections for a given database\n- `collection-indexes` - Describe the indexes for a collection\n- `collection-schema` - Describe the schema for a collection\n- `collection-storage-size` - Get the size of a collection in MB\n- `db-stats` - Return statistics about a MongoDB database\n- `export` - Export query or aggregation results to EJSON format. Creates a uniquely named export accessible via the `exported-data` resource.\n\n## 📄 Supported Resources\n\n- `config` - Server configuration, supplied by the user either as environment variables or as startup arguments with sensitive parameters redacted. The resource can be accessed under URI `config://config`.\n- `debug` - Debugging information for MongoDB connectivity issues. Tracks the last connectivity attempt and error information. The resource can be accessed under URI `debug://mongodb`.\n- `exported-data` - A resource template to access the data exported using the export tool. The template can be accessed under URI `exported-data://{exportName}` where `exportName` is the unique name for an export generated by the export tool.\n\n## Configuration\n\n\u003e **🔒 Security Best Practice:** We strongly recommend using environment variables for sensitive configuration such as API credentials (`MDB_MCP_API_CLIENT_ID`, `MDB_MCP_API_CLIENT_SECRET`) and connection strings (`MDB_MCP_CONNECTION_STRING`) instead of command-line arguments. Environment variables are not visible in process lists and provide better security for your sensitive data.\n\nThe MongoDB MCP Server can be configured using multiple methods, with the following precedence (highest to lowest):\n\n1. Command-line arguments\n2. Environment variables\n\n### Configuration Options\n\n| CLI Option                             | Environment Variable                                | Default                                                                     | Description                                                                                                                                                                                             |\n| -------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `apiClientId`                          | `MDB_MCP_API_CLIENT_ID`                             | \u003cnot set\u003e                                                                   | Atlas API client ID for authentication. Required for running Atlas tools.                                                                                                                               |\n| `apiClientSecret`                      | `MDB_MCP_API_CLIENT_SECRET`                         | \u003cnot set\u003e                                                                   | Atlas API client secret for authentication. Required for running Atlas tools.                                                                                                                           |\n| `connectionString`                     | `MDB_MCP_CONNECTION_STRING`                         | \u003cnot set\u003e                                                                   | MongoDB connection string for direct database connections. Optional, if not set, you'll need to call the `connect` tool before interacting with MongoDB data.                                           |\n| `loggers`                              | `MDB_MCP_LOGGERS`                                   | disk,mcp                                                                    | Comma separated values, possible values are `mcp`, `disk` and `stderr`. See [Logger Options](#logger-options) for details.                                                                              |\n| `logPath`                              | `MDB_MCP_LOG_PATH`                                  | see note\\*                                                                  | Folder to store logs.                                                                                                                                                                                   |\n| `disabledTools`                        | `MDB_MCP_DISABLED_TOOLS`                            | \u003cnot set\u003e                                                                   | An array of tool names, operation types, and/or categories of tools that will be disabled.                                                                                                              |\n| `confirmationRequiredTools`            | `MDB_MCP_CONFIRMATION_REQUIRED_TOOLS`               | create-access-list,create-db-user,drop-database,drop-collection,delete-many | An array of tool names that require user confirmation before execution. **Requires the client to support [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation)**.       |\n| `readOnly`                             | `MDB_MCP_READ_ONLY`                                 | false                                                                       | When set to true, only allows read, connect, and metadata operation types, disabling create/update/delete operations.                                                                                   |\n| `indexCheck`                           | `MDB_MCP_INDEX_CHECK`                               | false                                                                       | When set to true, enforces that query operations must use an index, rejecting queries that perform a collection scan.                                                                                   |\n| `telemetry`                            | `MDB_MCP_TELEMETRY`                                 | enabled                                                                     | When set to disabled, disables telemetry collection.                                                                                                                                                    |\n| `transport`                            | `MDB_MCP_TRANSPORT`                                 | stdio                                                                       | Either 'stdio' or 'http'.                                                                                                                                                                               |\n| `httpPort`                             | `MDB_MCP_HTTP_PORT`                                 | 3000                                                                        | Port number.                                                                                                                                                                                            |\n| `httpHost`                             | `MDB_MCP_HTTP_HOST`                                 | 127.0.0.1                                                                   | Host to bind the http server.                                                                                                                                                                           |\n| `idleTimeoutMs`                        | `MDB_MCP_IDLE_TIMEOUT_MS`                           | 600000                                                                      | Idle timeout for a client to disconnect (only applies to http transport).                                                                                                                               |\n| `maxBytesPerQuery`                     | `MDB_MCP_MAX_BYTES_PER_QUERY`                       | 16777216 (16MiB)                                                            | The maximum size in bytes for results from a `find` or `aggregate` tool call. This serves as an upper bound for the `responseBytesLimit` parameter in those tools.                                      |\n| `maxDocumentsPerQuery`                 | `MDB_MCP_MAX_DOCUMENTS_PER_QUERY`                   | 100                                                                         | The maximum number of documents that can be returned by a `find` or `aggregate` tool call. For the `find` tool, the effective limit will be the smaller of this value and the tool's `limit` parameter. |\n| `notificationTimeoutMs`                | `MDB_MCP_NOTIFICATION_TIMEOUT_MS`                   | 540000                                                                      | Notification timeout for a client to be aware of diconnect (only applies to http transport).                                                                                                            |\n| `exportsPath`                          | `MDB_MCP_EXPORTS_PATH`                              | see note\\*                                                                  | Folder to store exported data files.                                                                                                                                                                    |\n| `exportTimeoutMs`                      | `MDB_MCP_EXPORT_TIMEOUT_MS`                         | 300000                                                                      | Time in milliseconds after which an export is considered expired and eligible for cleanup.                                                                                                              |\n| `exportCleanupIntervalMs`              | `MDB_MCP_EXPORT_CLEANUP_INTERVAL_MS`                | 120000                                                                      | Time in milliseconds between export cleanup cycles that remove expired export files.                                                                                                                    |\n| `atlasTemporaryDatabaseUserLifetimeMs` | `MDB_MCP_ATLAS_TEMPORARY_DATABASE_USER_LIFETIME_MS` | 14400000                                                                    | Time in milliseconds that temporary database users created when connecting to MongoDB Atlas clusters will remain active before being automatically deleted.                                             |\n| `voyageApiKey`                         | `MDB_VOYAGE_API_KEY`                                | \u003cnot set\u003e                                                                   | API key for communicating with Voyage AI. Used for generating embeddings for Vector search.                                                                                                             |\n\n#### Logger Options\n\nThe `loggers` configuration option controls where logs are sent. You can specify one or more logger types as a comma-separated list. The available options are:\n\n- `mcp`: Sends logs to the MCP client (if supported by the client/transport).\n- `disk`: Writes logs to disk files. Log files are stored in the log path (see `logPath` above).\n- `stderr`: Outputs logs to standard error (stderr), useful for debugging or when running in containers.\n\n**Default:** `disk,mcp` (logs are written to disk and sent to the MCP client).\n\nYou can combine multiple loggers, e.g. `--loggers disk stderr` or `export MDB_MCP_LOGGERS=\"mcp,stderr\"`.\n\n##### Example: Set logger via environment variable\n\n```shell\nexport MDB_MCP_LOGGERS=\"disk,stderr\"\n```\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Example: Set logger via command-line argument\n\n```shell\nnpx -y mongodb-mcp-server@latest --loggers mcp stderr\n```\n\n##### Log File Location\n\nWhen using the `disk` logger, log files are stored in:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\.app-logs`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/.app-logs`\n\nYou can override the log directory with the `logPath` option.\n\n#### Disabled Tools\n\nYou can disable specific tools or categories of tools by using the `disabledTools` option. This option accepts an array of strings,\nwhere each string can be a tool name, operation type, or category.\n\nThe way the array is constructed depends on the type of configuration method you use:\n\n- For **environment variable** configuration, use a comma-separated string: `export MDB_MCP_DISABLED_TOOLS=\"create,update,delete,atlas,collectionSchema\"`.\n- For **command-line argument** configuration, use a space-separated string: `--disabledTools create update delete atlas collectionSchema`.\n\nCategories of tools:\n\n- `atlas` - MongoDB Atlas tools, such as list clusters, create cluster, etc.\n- `mongodb` - MongoDB database tools, such as find, aggregate, etc.\n\nOperation types:\n\n- `create` - Tools that create resources, such as create cluster, insert document, etc.\n- `update` - Tools that update resources, such as update document, rename collection, etc.\n- `delete` - Tools that delete resources, such as delete document, drop collection, etc.\n- `read` - Tools that read resources, such as find, aggregate, list clusters, etc.\n- `metadata` - Tools that read metadata, such as list databases/collections/indexes, infer collection schema, etc.\n- `connect` - Tools that allow you to connect or switch the connection to a MongoDB instance. If this is disabled, you will need to provide a connection string through the config when starting the server.\n\n#### Require Confirmation\n\nIf your client supports [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation), you can set the MongoDB MCP server to request user confirmation before executing certain tools.\n\nWhen a tool is marked as requiring confirmation, the server will send an elicitation request to the client. The client with elicitation support will then prompt the user for confirmation and send the response back to the server. If the client does not support elicitation, the tool will execute without confirmation.\n\nYou can set the `confirmationRequiredTools` configuration option to specify the names of tools which require confirmation. By default, the following tools have this setting enabled: `drop-database`, `drop-collection`, `delete-many`, `atlas-create-db-user`, `atlas-create-access-list`.\n\n#### Read-Only Mode\n\nThe `readOnly` configuration option allows you to restrict the MCP server to only use tools with \"read\", \"connect\", and \"metadata\" operation types. When enabled, all tools that have \"create\", \"update\" or \"delete\" operation types will not be registered with the server.\n\nThis is useful for scenarios where you want to provide access to MongoDB data for analysis without allowing any modifications to the data or infrastructure.\n\nYou can enable read-only mode using:\n\n- **Environment variable**: `export MDB_MCP_READ_ONLY=true`\n- **Command-line argument**: `--readOnly`\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen read-only mode is active, you'll see a message in the server logs indicating which tools were prevented from registering due to this restriction.\n\n#### Index Check Mode\n\nThe `indexCheck` configuration option allows you to enforce that query operations must use an index. When enabled, queries that perform a collection scan will be rejected to ensure better performance.\n\nThis is useful for scenarios where you want to ensure that database queries are optimized.\n\nYou can enable index check mode using:\n\n- **Environment variable**: `export MDB_MCP_INDEX_CHECK=true`\n- **Command-line argument**: `--indexCheck`\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen index check mode is active, you'll see an error message if a query is rejected due to not using an index.\n\n#### Exports\n\nThe data exported by the `export` tool is temporarily stored in the configured `exportsPath` on the machine running the MCP server until cleaned up by the export cleanup process. If the `exportsPath` configuration is not provided, the following defaults are used:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\exports`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/exports`\n\nThe `exportTimeoutMs` configuration controls the time after which the exported data is considered expired and eligible for cleanup. By default, exports expire after 5 minutes (300000ms).\n\nThe `exportCleanupIntervalMs` configuration controls how frequently the cleanup process runs to remove expired export files. By default, cleanup runs every 2 minutes (120000ms).\n\n#### Telemetry\n\nThe `telemetry` configuration option allows you to disable telemetry collection. When enabled, the MCP server will collect usage data and send it to MongoDB.\n\nYou can disable telemetry using:\n\n- **Environment variable**: `export MDB_MCP_TELEMETRY=disabled`\n- **Command-line argument**: `--telemetry disabled`\n- **DO_NOT_TRACK environment variable**: `export DO_NOT_TRACK=1`\n\n\u003e **💡 Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n### Atlas API Access\n\nTo use the Atlas API tools, you'll need to create a service account in MongoDB Atlas:\n\n\u003e **ℹ️ Note:** For a detailed breakdown of the minimum required permissions for each Atlas operation, see the [Atlas API Permissions](#atlas-api-permissions) section below.\n\n1. **Create a Service Account:**\n   - Log in to MongoDB Atlas at [cloud.mongodb.com](https://cloud.mongodb.com)\n   - Navigate to Access Manager \u003e Organization Access\n   - Click Add New \u003e Applications \u003e Service Accounts\n   - Enter name, description and expiration for your service account (e.g., \"MCP, MCP Server Access, 7 days\")\n   - **Assign only the minimum permissions needed for your use case.**\n     - See [Atlas API Permissions](#atlas-api-permissions) for details.\n   - Click \"Create\"\n\nTo learn more about Service Accounts, check the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/api/service-accounts-overview/).\n\n2. **Save Client Credentials:**\n   - After creation, you'll be shown the Client ID and Client Secret\n   - **Important:** Copy and save the Client Secret immediately as it won't be displayed again\n\n3. **Add Access List Entry:**\n   - Add your IP address to the API access list\n\n4. **Configure the MCP Server:**\n   - Use one of the configuration methods below to set your `apiClientId` and `apiClientSecret`\n\n### Atlas API Permissions\n\n\u003e **Security Warning:** Granting the Organization Owner role is rarely necessary and can be a security risk. Assign only the minimum permissions needed for your use case.\n\n#### Quick Reference: Required roles per operation\n\n| What you want to do                  | Safest Role to Assign (where)           |\n| ------------------------------------ | --------------------------------------- |\n| List orgs/projects                   | Org Member or Org Read Only (Org)       |\n| Create new projects                  | Org Project Creator (Org)               |\n| View clusters/databases in a project | Project Read Only (Project)             |\n| Create/manage clusters in a project  | Project Cluster Manager (Project)       |\n| Manage project access lists          | Project IP Access List Admin (Project)  |\n| Manage database users                | Project Database Access Admin (Project) |\n\n- **Prefer project-level roles** for most operations. Assign only to the specific projects you need to manage or view.\n- **Avoid Organization Owner** unless you require full administrative control over all projects and settings in the organization.\n\nFor a full list of roles and their privileges, see the [Atlas User Roles documentation](https://www.mongodb.com/docs/atlas/reference/user-roles/#service-user-roles).\n\n### Configuration Methods\n\n#### Environment Variables\n\nSet environment variables with the prefix `MDB_MCP_` followed by the option name in uppercase with underscores:\n\n**Linux/macOS (bash/zsh):**\n\n```bash\n# Set Atlas API credentials (via Service Accounts)\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\nexport MDB_MCP_LOG_PATH=\"/path/to/logs\"\n```\n\n**Windows Command Prompt (cmd):**\n\n```cmd\nset \"MDB_MCP_API_CLIENT_ID=your-atlas-service-accounts-client-id\"\nset \"MDB_MCP_API_CLIENT_SECRET=your-atlas-service-accounts-client-secret\"\n\nset \"MDB_MCP_CONNECTION_STRING=mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\nset \"MDB_MCP_LOG_PATH=C:\\path\\to\\logs\"\n```\n\n**Windows PowerShell:**\n\n```powershell\n# Set Atlas API credentials (via Service Accounts)\n$env:MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\n$env:MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\n$env:MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\n$env:MDB_MCP_LOG_PATH=\"C:\\path\\to\\logs\"\n```\n\n#### MCP configuration file examples\n\n##### Connection String with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\n##### Atlas API credentials with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Command-Line Arguments\n\nPass configuration options as command-line arguments when starting the server:\n\n\u003e **🔒 Security Note:** For sensitive configuration like API credentials and connection strings, use environment variables instead of command-line arguments.\n\n```shell\n# Set sensitive data as environment variable\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Start the server with command line arguments\nnpx -y mongodb-mcp-server@latest --logPath=/path/to/logs --readOnly --indexCheck\n```\n\n\u003e **💡 Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n#### MCP configuration file examples\n\n##### Connection String with command-line arguments\n\n\u003e **🔒 Security Note:** We do not recommend passing connection string as command line argument. Connection string might contain credentials which can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [connection string through environment variables](#connection-string-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--connectionString\",\n        \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n##### Atlas API credentials with command-line arguments\n\n\u003e **🔒 Security Note:** We do not recommend passing Atlas API credentials as command line argument. The provided credentials can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [Atlas API credentials through environment variables](#atlas-api-credentials-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--apiClientId\",\n        \"your-atlas-service-accounts-client-id\",\n        \"--apiClientSecret\",\n        \"your-atlas-service-accounts-client-secret\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n### Proxy Support\n\nThe MCP Server will detect typical PROXY environment variables and use them for\nconnecting to the Atlas API, your MongoDB Cluster, or any other external calls\nto third-party services like OID Providers. The behaviour is the same as what\n`mongosh` does, so the same settings will work in the MCP Server.\n\n## 🤝Contributing\n\nInterested in contributing? Great! Please check our [Contributing Guide](CONTRIBUTING.md) for guidelines on code contributions, standards, adding new tools, and troubleshooting information.\n","stargazer_count":709,"topics":["mcp","mcp-server","mongodb","mongodb-atlas","mongodb-database"],"uses_custom_opengraph_image":false}}}},{"name":"elastic/mcp-server-elasticsearch","description":"MCP server for connecting to Elasticsearch data and indices. Supports search queries, mappings, ES|QL, and shard information through natural language interactions.","status":"active","repository":{"url":"https://github.com/elastic/mcp-server-elasticsearch","source":"github","id":"953992846","readme":"# Elasticsearch MCP Server\n\n\u003e [!CAUTION]\n\u003e\n\u003e **WARNING: this MCP server is EXPERIMENTAL.**\n\nConnect to your Elasticsearch data directly from any MCP Client using the Model Context Protocol (MCP).\n\nThis server connects agents to your Elasticsearch data using the Model Context Protocol. It allows you to interact with your Elasticsearch indices through natural language conversations.\n\n## Available Tools\n\n* `list_indices`: List all available Elasticsearch indices\n* `get_mappings`: Get field mappings for a specific Elasticsearch index\n* `search`: Perform an Elasticsearch search with the provided query DSL\n* `esql`: Perform an ES|QL query\n* `get_shards`: Get shard information for all or specific indices\n\n## Prerequisites\n\n* An Elasticsearch instance\n* Elasticsearch authentication credentials (API key or username/password)\n* An MCP Client (e.g. [Claude Desktop](https://claude.ai/download), [Goose](https://block.github.io/goose/))\n\n**Supported Elasticsearch versions**\n\nThis works with Elasticsearch versions `8.x` and `9.x`.\n\n## Installation \u0026 Setup\n\n\u003e [!NOTE]\n\u003e\n\u003e Versions 0.3.1 and earlier were installed via `npm`. These versions are deprecated and no longer supported. The following instructions only apply to 0.4.0 and later.\n\u003e\n\u003e To view instructions for versions 0.3.1 and earlier, see the [README for v0.3.1](https://github.com/elastic/mcp-server-elasticsearch/tree/v0.3.1).\n\nThis MCP server is provided as a Docker image at `docker.elastic.co/mcp/elasticsearch`\nthat supports MCP's stdio, SSE and streamable-HTTP protocols.\n\nRunning this container without any argument will output a usage message:\n\n```\ndocker run docker.elastic.co/mcp/elasticsearch\n```\n\n```\nUsage: elasticsearch-mcp-server \u003cCOMMAND\u003e\n\nCommands:\n  stdio  Start a stdio server\n  http   Start a streamable-HTTP server with optional SSE support\n  help   Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version\n```\n\n### Using the stdio protocol\n\nThe MCP server needs environment variables to be set:\n\n* `ES_URL`: the URL of your Elasticsearch cluster\n* For authentication use either an API key or basic authentication:\n  * API key: `ES_API_KEY`\n  * Basic auth: `ES_USERNAME` and `ES_PASSWORD`\n* Optionally, `ES_SSL_SKIP_VERIFY` set to `true` skips SSL/TLS certificate verification when connecting\n  to Elasticsearch. The ability to provide a custom certificate will be added in a later version.\n\nThe MCP server is started in stdio mode with this command:\n\n```bash\ndocker run -i --rm -e ES_URL -e ES_API_KEY docker.elastic.co/mcp/elasticsearch stdio\n```\n\nThe configuration for Claude Desktop is as follows:\n\n```json\n{\n \"mcpServers\": {\n   \"elasticsearch-mcp-server\": {\n    \"command\": \"docker\",\n    \"args\": [\n     \"run\", \"-i\", \"--rm\",\n     \"-e\", \"ES_URL\", \"-e\", \"ES_API_KEY\",\n     \"docker.elastic.co/mcp/elasticsearch\",\n     \"stdio\"\n    ],\n    \"env\": {\n      \"ES_URL\": \"\u003celasticsearch-cluster-url\u003e\",\n      \"ES_API_KEY\": \"\u003celasticsearch-API-key\u003e\"\n    }\n   }\n }\n}\n```\n\n### Using the streamable-HTTP and SSE protocols\n\nNote: streamable-HTTP is recommended, as [SSE is deprecated](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse-deprecated).\n\nThe MCP server needs environment variables to be set:\n\n* `ES_URL`, the URL of your Elasticsearch cluster\n* For authentication use either an API key or basic authentication:\n  * API key: `ES_API_KEY`\n  * Basic auth: `ES_USERNAME` and `ES_PASSWORD`\n* Optionally, `ES_SSL_SKIP_VERIFY` set to `true` skips SSL/TLS certificate verification when connecting\n  to Elasticsearch. The ability to provide a custom certificate will be added in a later version.\n\nThe MCP server is started in http mode with this command:\n\n```bash\ndocker run --rm -e ES_URL -e ES_API_KEY -p 8080:8080 docker.elastic.co/mcp/elasticsearch http\n```\n\nIf for some reason your execution environment doesn't allow passing parameters to the container, they can be passed\nusing the `CLI_ARGS` environment variable: `docker run --rm -e ES_URL -e ES_API_KEY -e CLI_ARGS=http -p 8080:8080...`\n\nThe streamable-HTTP endpoint is at `http:\u003chost\u003e:8080/mcp`. There's also a health check at `http:\u003chost\u003e:8080/ping`\n\nConfiguration for Claude Desktop (free edition that only supports the stdio protocol).\n\n1. Install `mcp-proxy` (or an equivalent), that will bridge stdio to streamable-http. The executable\n   will be installed in `~/.local/bin`:\n\n    ```bash\n    uv tool install mcp-proxy\n    ```\n\n2. Add this configuration to Claude Desktop:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"elasticsearch-mcp-server\": {\n          \"command\": \"/\u003chome-directory\u003e/.local/bin/mcp-proxy\",\n          \"args\": [\n            \"--transport=streamablehttp\",\n            \"--header\", \"Authorization\", \"ApiKey \u003celasticsearch-API-key\u003e\",\n            \"http://\u003cmcp-server-host\u003e:\u003cmcp-server-port\u003e/mcp\"\n          ]\n        }\n      }\n    }\n    ```\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:28Z","updated_at":"2025-10-22T11:20:44Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"docker","identifier":"docker.elastic.co/mcp/elasticsearch","version":"latest","runtime_hint":"docker","runtime_arguments":[{"is_required":true,"format":"string","value":"run","type":"positional","name":"run","value_hint":"docker_cmd"},{"is_required":true,"format":"string","type":"named","name":"-i","value_hint":"interactive_flag"},{"is_required":true,"format":"string","type":"named","name":"--rm","value_hint":"remove_flag"},{"is_required":true,"format":"string","type":"named","name":"-e","value_hint":"env_flag"},{"is_required":true,"format":"string","value":"$ES_URL","type":"positional","name":"ES_URL","value_hint":"env_var_name"},{"is_required":true,"format":"string","type":"named","name":"-e","value_hint":"env_flag"},{"is_required":true,"format":"string","value":"$ES_API_KEY","type":"positional","name":"ES_API_KEY","value_hint":"env_var_name"},{"is_required":true,"format":"string","value":"docker.elastic.co/mcp/elasticsearch","type":"positional","name":"docker.elastic.co/mcp/elasticsearch","value_hint":"image"},{"format":"string","value":"stdio","type":"positional","name":"stdio","value_hint":"transport_mode"}],"environment_variables":[{"description":"your elasticsearch url","value":"{es_url}","variables":{"es_url":{"description":"your elasticsearch url","is_required":true}},"name":"ES_URL"},{"description":"your api key","value":"{es_api_key}","variables":{"es_api_key":{"description":"your api key","is_required":true,"is_secret":true}},"name":"ES_API_KEY"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"1b8a16d9-0648-4790-b853-d919a6be16e9","is_latest":true,"published_at":"2025-05-16T18:58:37Z","updated_at":"2025-05-16T18:58:37Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Elasticsearch","homepage_url":"https://www.elastic.co/elasticsearch","is_in_organization":true,"license":"Apache License 2.0","name":"mcp-server-elasticsearch","name_with_owner":"elastic/mcp-server-elasticsearch","opengraph_image_url":"https://opengraph.githubassets.com/8353a3896cd1f6ea68838480669df39e942c90195dd99fb240535879dcf0e2f4/elastic/mcp-server-elasticsearch","owner_avatar_url":"https://avatars.githubusercontent.com/u/6764390?v=4","preferred_image":"https://avatars.githubusercontent.com/u/6764390?v=4","primary_language":"Rust","primary_language_color":"#dea584","pushed_at":"2025-10-21T18:31:32Z","readme":"# Elasticsearch MCP Server\n\n\u003e [!CAUTION]\n\u003e\n\u003e **WARNING: this MCP server is EXPERIMENTAL.**\n\nConnect to your Elasticsearch data directly from any MCP Client using the Model Context Protocol (MCP).\n\nThis server connects agents to your Elasticsearch data using the Model Context Protocol. It allows you to interact with your Elasticsearch indices through natural language conversations.\n\n## Available Tools\n\n* `list_indices`: List all available Elasticsearch indices\n* `get_mappings`: Get field mappings for a specific Elasticsearch index\n* `search`: Perform an Elasticsearch search with the provided query DSL\n* `esql`: Perform an ES|QL query\n* `get_shards`: Get shard information for all or specific indices\n\n## Prerequisites\n\n* An Elasticsearch instance\n* Elasticsearch authentication credentials (API key or username/password)\n* An MCP Client (e.g. [Claude Desktop](https://claude.ai/download), [Goose](https://block.github.io/goose/))\n\n**Supported Elasticsearch versions**\n\nThis works with Elasticsearch versions `8.x` and `9.x`.\n\n## Installation \u0026 Setup\n\n\u003e [!NOTE]\n\u003e\n\u003e Versions 0.3.1 and earlier were installed via `npm`. These versions are deprecated and no longer supported. The following instructions only apply to 0.4.0 and later.\n\u003e\n\u003e To view instructions for versions 0.3.1 and earlier, see the [README for v0.3.1](https://github.com/elastic/mcp-server-elasticsearch/tree/v0.3.1).\n\nThis MCP server is provided as a Docker image at `docker.elastic.co/mcp/elasticsearch`\nthat supports MCP's stdio, SSE and streamable-HTTP protocols.\n\nRunning this container without any argument will output a usage message:\n\n```\ndocker run docker.elastic.co/mcp/elasticsearch\n```\n\n```\nUsage: elasticsearch-mcp-server \u003cCOMMAND\u003e\n\nCommands:\n  stdio  Start a stdio server\n  http   Start a streamable-HTTP server with optional SSE support\n  help   Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version\n```\n\n### Using the stdio protocol\n\nThe MCP server needs environment variables to be set:\n\n* `ES_URL`: the URL of your Elasticsearch cluster\n* For authentication use either an API key or basic authentication:\n  * API key: `ES_API_KEY`\n  * Basic auth: `ES_USERNAME` and `ES_PASSWORD`\n* Optionally, `ES_SSL_SKIP_VERIFY` set to `true` skips SSL/TLS certificate verification when connecting\n  to Elasticsearch. The ability to provide a custom certificate will be added in a later version.\n\nThe MCP server is started in stdio mode with this command:\n\n```bash\ndocker run -i --rm -e ES_URL -e ES_API_KEY docker.elastic.co/mcp/elasticsearch stdio\n```\n\nThe configuration for Claude Desktop is as follows:\n\n```json\n{\n \"mcpServers\": {\n   \"elasticsearch-mcp-server\": {\n    \"command\": \"docker\",\n    \"args\": [\n     \"run\", \"-i\", \"--rm\",\n     \"-e\", \"ES_URL\", \"-e\", \"ES_API_KEY\",\n     \"docker.elastic.co/mcp/elasticsearch\",\n     \"stdio\"\n    ],\n    \"env\": {\n      \"ES_URL\": \"\u003celasticsearch-cluster-url\u003e\",\n      \"ES_API_KEY\": \"\u003celasticsearch-API-key\u003e\"\n    }\n   }\n }\n}\n```\n\n### Using the streamable-HTTP and SSE protocols\n\nNote: streamable-HTTP is recommended, as [SSE is deprecated](https://modelcontextprotocol.io/docs/concepts/transports#server-sent-events-sse-deprecated).\n\nThe MCP server needs environment variables to be set:\n\n* `ES_URL`, the URL of your Elasticsearch cluster\n* For authentication use either an API key or basic authentication:\n  * API key: `ES_API_KEY`\n  * Basic auth: `ES_USERNAME` and `ES_PASSWORD`\n* Optionally, `ES_SSL_SKIP_VERIFY` set to `true` skips SSL/TLS certificate verification when connecting\n  to Elasticsearch. The ability to provide a custom certificate will be added in a later version.\n\nThe MCP server is started in http mode with this command:\n\n```bash\ndocker run --rm -e ES_URL -e ES_API_KEY -p 8080:8080 docker.elastic.co/mcp/elasticsearch http\n```\n\nIf for some reason your execution environment doesn't allow passing parameters to the container, they can be passed\nusing the `CLI_ARGS` environment variable: `docker run --rm -e ES_URL -e ES_API_KEY -e CLI_ARGS=http -p 8080:8080...`\n\nThe streamable-HTTP endpoint is at `http:\u003chost\u003e:8080/mcp`. There's also a health check at `http:\u003chost\u003e:8080/ping`\n\nConfiguration for Claude Desktop (free edition that only supports the stdio protocol).\n\n1. Install `mcp-proxy` (or an equivalent), that will bridge stdio to streamable-http. The executable\n   will be installed in `~/.local/bin`:\n\n    ```bash\n    uv tool install mcp-proxy\n    ```\n\n2. Add this configuration to Claude Desktop:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"elasticsearch-mcp-server\": {\n          \"command\": \"/\u003chome-directory\u003e/.local/bin/mcp-proxy\",\n          \"args\": [\n            \"--transport=streamablehttp\",\n            \"--header\", \"Authorization\", \"ApiKey \u003celasticsearch-API-key\u003e\",\n            \"http://\u003cmcp-server-host\u003e:\u003cmcp-server-port\u003e/mcp\"\n          ]\n        }\n      }\n    }\n    ```\n","stargazer_count":524,"topics":["elasticsearch","mcp","mcp-server","vector-database"],"uses_custom_opengraph_image":false}}}},{"name":"neondatabase/mcp-server-neon","description":"MCP server for interacting with Neon Management API and databases","status":"active","repository":{"url":"https://github.com/neondatabase/mcp-server-neon","source":"github","id":"896203400","readme":"\u003cpicture\u003e\n  \u003csource media=\"(prefers-color-scheme: dark)\" srcset=\"https://neon.com/brand/neon-logo-dark-color.svg\"\u003e\n  \u003csource media=\"(prefers-color-scheme: light)\" srcset=\"https://neon.com/brand/neon-logo-light-color.svg\"\u003e\n  \u003cimg width=\"250px\" alt=\"Neon Logo fallback\" src=\"https://neon.com/brand/neon-logo-dark-color.svg\"\u003e\n\u003c/picture\u003e\n\n# Neon MCP Server\n\n[![Install MCP Server in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Neon\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5uZW9uLnRlY2gvbWNwIn0%3D)\n\n**Neon MCP Server** is an open-source tool that lets you interact with your Neon Postgres databases in **natural language**.\n\n[![npm version](https://img.shields.io/npm/v/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![npm downloads](https://img.shields.io/npm/dt/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThe Model Context Protocol (MCP) is a [new, standardized protocol](https://modelcontextprotocol.io/introduction) designed to manage context between large language models (LLMs) and external systems. This repository offers an installer and an MCP Server for [Neon](https://neon.tech).\n\nNeon's MCP server acts as a bridge between natural language requests and the [Neon API](https://api-docs.neon.tech/reference/getting-started-with-neon-api). Built upon MCP, it translates your requests into the necessary API calls, enabling you to manage tasks such as creating projects and branches, running queries, and performing database migrations seamlessly.\n\nSome of the key features of the Neon MCP server include:\n\n- **Natural language interaction:** Manage Neon databases using intuitive, conversational commands.\n- **Simplified database management:** Perform complex actions without writing SQL or directly using the Neon API.\n- **Accessibility for non-developers:** Empower users with varying technical backgrounds to interact with Neon databases.\n- **Database migration support:** Leverage Neon's branching capabilities for database schema changes initiated via natural language.\n\nFor example, in Claude Desktop, or any MCP Client, you can use natural language to accomplish things with Neon, such as:\n\n- `Let's create a new Postgres database, and call it \"my-database\". Let's then create a table called users with the following columns: id, name, email, and password.`\n- `I want to run a migration on my project called \"my-project\" that alters the users table to add a new column called \"created_at\".`\n- `Can you give me a summary of all of my Neon projects and what data is in each one?`\n\n\u003e [!WARNING]  \n\u003e **Neon MCP Server Security Considerations**  \n\u003e The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.\n\u003e\n\u003e The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.\n\u003e\n\u003e For more information, see [MCP security guidance →](https://neon.tech/docs/ai/neon-mcp-server#mcp-security-guidance).\n\n## Setting up Neon MCP Server\n\nYou have two options for connecting your MCP client to Neon:\n\n1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys. Additionally, you will automatically receive the latest features and improvements as soon as they are released.\n\n2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.\n\n## Prerequisites\n\n- An MCP Client application.\n- A [Neon account](https://console.neon.tech/signup).\n- **Node.js (\u003e= v18.0.0) and npm:** Download from [nodejs.org](https://nodejs.org).\n\nFor Local MCP Server setup, you also need a Neon API key. See [Neon API Keys documentation](https://neon.tech/docs/manage/api-keys) for instructions on generating one.\n\n### Option 1. Remote Hosted MCP Server (Preview)\n\nConnect to Neon's managed MCP server using OAuth for authentication. This is the easiest setup, requires no local installation of this server, and doesn't need a Neon API key configured in the client.\n\n- Add the following \"Neon\" entry to your client's MCP server configuration file (e.g., `mcp.json`, `mcp_config.json`):\n\n  ```json\n  {\n    \"mcpServers\": {\n      \"Neon\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.neon.tech/mcp\"]\n      }\n    }\n  }\n  ```\n\n- Save the configuration file.\n- Restart or refresh your MCP client.\n- An OAuth window will open in your browser. Follow the prompts to authorize your MCP client to access your Neon account.\n\n\u003e With OAuth base authentication, the MCP server will, by default operate on projects under your personal Neon account. To access or manage projects under organization, you must explicitly provide either the `org_id` or the `project_id` in your prompt to MCP client.\n\nRemote MCP Server also supports authentication using API key in the `Authorization` header if your client supports it\n\n```json\n{\n  \"mcpServers\": {\n    \"Neon\": {\n      \"url\": \"https://mcp.neon.tech/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer \u003c$NEON_API_KEY\u003e\"\n      }\n    }\n  }\n}\n```\n\n\u003e Provider organization's API key to limit access to projects under the organization only.\n\nMCP supports two remote server transports: the deprecated Server-Sent Events (SSE) and the newer, recommended Streamable HTTP. If your LLM client doesn't support Streamable HTTP yet, you can switch the endpoint from `https://mcp.neon.tech/mcp` to `https://mcp.neon.tech/sse` to use SSE instead.\n\n### Option 2. Local MCP Server\n\nRun the Neon MCP server on your local machine with your Neon API key. This method allows you to manage your Neon projects and databases without relying on a remote MCP server.\n\nAdd the following JSON configuration within the `mcpServers` section of your client's `mcp_config` file, replacing `\u003cYOUR_NEON_API_KEY\u003e` with your actual Neon API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003cYOUR_NEON_API_KEY\u003e\"\n      ]\n    }\n  }\n}\n```\n\n### Troubleshooting\n\nIf your client does not use `JSON` for configuration of MCP servers (such as older versions of Cursor), you can use the following command when prompted:\n\n```bash\nnpx -y @neondatabase/mcp-server-neon start \u003cYOUR_NEON_API_KEY\u003e\n```\n\n#### Troubleshooting on Windows\n\nIf you are using Windows and encounter issues while adding the MCP server, you might need to use the Command Prompt (`cmd`) or Windows Subsystem for Linux (`wsl`) to run the necessary commands. Your configuration setup may resemble the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003cYOUR_NEON_API_KEY\u003e\"\n      ]\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"wsl\",\n      \"args\": [\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003cYOUR_NEON_API_KEY\u003e\"\n      ]\n    }\n  }\n}\n```\n\n## Guides\n\n- [Neon MCP Server Guide](https://neon.tech/docs/ai/neon-mcp-server)\n- [Connect MCP Clients to Neon](https://neon.tech/docs/ai/connect-mcp-clients-to-neon)\n- [Cursor with Neon MCP Server](https://neon.tech/guides/cursor-mcp-neon)\n- [Claude Desktop with Neon MCP Server](https://neon.tech/guides/neon-mcp-server)\n- [Cline with Neon MCP Server](https://neon.tech/guides/cline-mcp-neon)\n- [Windsurf with Neon MCP Server](https://neon.tech/guides/windsurf-mcp-neon)\n- [Zed with Neon MCP Server](https://neon.tech/guides/zed-mcp-neon)\n\n# Features\n\n## Supported Tools\n\nThe Neon MCP Server provides the following actions, which are exposed as \"tools\" to MCP Clients. You can use these tools to interact with your Neon projects and databases using natural language commands.\n\n**Project Management:**\n\n- **`list_projects`**: Lists the first 10 Neon projects in your account, providing a summary of each project. If you can't find a specific project, increase the limit by passing a higher value to the `limit` parameter.\n- **`list_shared_projects`**: Lists Neon projects shared with the current user. Supports a search parameter and limiting the number of projects returned (default: 10).\n- **`describe_project`**: Fetches detailed information about a specific Neon project, including its ID, name, and associated branches and databases.\n- **`create_project`**: Creates a new Neon project in your Neon account. A project acts as a container for branches, databases, roles, and computes.\n- **`delete_project`**: Deletes an existing Neon project and all its associated resources.\n\n**Branch Management:**\n\n- **`create_branch`**: Creates a new branch within a specified Neon project. Leverages [Neon's branching](/docs/introduction/branching) feature for development, testing, or migrations.\n- **`delete_branch`**: Deletes an existing branch from a Neon project.\n- **`describe_branch`**: Retrieves details about a specific branch, such as its name, ID, and parent branch.\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, including compute ID, type, size, and autoscaling information.\n- **`list_organizations`**: Lists all organizations that the current user has access to. Optionally filter by organization name or ID using the search parameter.\n- **`reset_from_parent`**: Resets the current branch to its parent's state, discarding local changes. Automatically preserves to backup if branch has children, or optionally preserve on request with a custom name.\n\n**SQL Query Execution:**\n\n- **`get_connection_string`**: Returns your database connection string.\n- **`run_sql`**: Executes a single SQL query against a specified Neon database. Supports both read and write operations.\n- **`run_sql_transaction`**: Executes a series of SQL queries within a single transaction against a Neon database.\n- **`get_database_tables`**: Lists all tables within a specified Neon database.\n- **`describe_table_schema`**: Retrieves the schema definition of a specific table, detailing columns, data types, and constraints.\n- **`list_slow_queries`**: Identifies performance bottlenecks by finding the slowest queries in a database. Requires the pg_stat_statements extension.\n\n**Database Migrations (Schema Changes):**\n\n- **`prepare_database_migration`**: Initiates a database migration process. Critically, it creates a temporary branch to apply and test the migration safely before affecting the main branch.\n- **`complete_database_migration`**: Finalizes and applies a prepared database migration to the main branch. This action merges changes from the temporary migration branch and cleans up temporary resources.\n\n**Query Performance Optimization:**\n\n- **`explain_sql_statement`**: Provides detailed execution plans for SQL queries to help identify performance bottlenecks.\n- **`prepare_query_tuning`**: Analyzes query performance and suggests optimizations like index creation. Creates a temporary branch for safely testing these optimizations.\n- **`complete_query_tuning`**: Applies or discards query optimizations after testing. Can merge changes from the temporary branch to the main branch.\n- **`list_slow_queries`**: Identifies and analyzes slow-performing queries in your database. Requires the `pg_stat_statements` extension.\n\n**Compute Management:**\n\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, showing details like compute ID, type, size, and last active time.\n\n**Neon Auth:**\n\n- **`provision_neon_auth`**: Provisions Neon Auth for a Neon project. It allows developers to easily set up authentication infrastructure by creating an integration with Stack Auth (`@stackframe/stack`).\n\n**Query Performance Tuning:**\n\n- **`explain_sql_statement`**: Analyzes a SQL query and returns detailed execution plan information to help understand query performance.\n- **`prepare_query_tuning`**: Identifies potential performance issues in a SQL query and suggests optimizations. Creates a temporary branch for testing improvements.\n- **`complete_query_tuning`**: Finalizes and applies query optimizations after testing. Merges changes from the temporary tuning branch to the main branch.\n\n## Migrations\n\nMigrations are a way to manage changes to your database schema over time. With the Neon MCP server, LLMs are empowered to do migrations safely with separate \"Start\" (`prepare_database_migration`) and \"Commit\" (`complete_database_migration`) commands.\n\nThe \"Start\" command accepts a migration and runs it in a new temporary branch. Upon returning, this command hints to the LLM that it should test the migration on this branch. The LLM can then run the \"Commit\" command to apply the migration to the original branch.\n\n# Development\n\n## Development with MCP CLI Client\n\nThe easiest way to iterate on the MCP Server is using the `mcp-client/`. Learn more in `mcp-client/README.md`.\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\ncd mcp-client/ \u0026\u0026 NEON_API_KEY=... npm run start:mcp-server-neon\n```\n\n## Development with Claude Desktop (Local MCP Server)\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\nnode dist/index.js init $NEON_API_KEY\n```\n\nThen, **restart Claude** each time you want to test changes.\n\n# Testing\n\nTo run the tests you need to setup the `.env` file according to the `.env.example` file.\n\n```bash\nnpm run test\n```\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:22Z","updated_at":"2025-10-22T11:20:39Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.neon.tech/sse"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"80cf5adf-774d-45c6-b97a-d527509aa505","is_latest":true,"published_at":"2025-09-09T10:55:22.909438Z","updated_at":"2025-09-09T10:55:22.909438Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Neon","is_in_organization":true,"license":"MIT License","name":"mcp-server-neon","name_with_owner":"neondatabase/mcp-server-neon","opengraph_image_url":"https://opengraph.githubassets.com/353e781bf34d3edf9268852cb6344f394d18d215e5327191f5b4e0bd7db97b35/neondatabase/mcp-server-neon","owner_avatar_url":"https://avatars.githubusercontent.com/u/77690634?v=4","preferred_image":"https://avatars.githubusercontent.com/u/77690634?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-08T21:08:12Z","readme":"\u003cpicture\u003e\n  \u003csource media=\"(prefers-color-scheme: dark)\" srcset=\"https://neon.com/brand/neon-logo-dark-color.svg\"\u003e\n  \u003csource media=\"(prefers-color-scheme: light)\" srcset=\"https://neon.com/brand/neon-logo-light-color.svg\"\u003e\n  \u003cimg width=\"250px\" alt=\"Neon Logo fallback\" src=\"https://neon.com/brand/neon-logo-dark-color.svg\"\u003e\n\u003c/picture\u003e\n\n# Neon MCP Server\n\n[![Install MCP Server in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Neon\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5uZW9uLnRlY2gvbWNwIn0%3D)\n\n**Neon MCP Server** is an open-source tool that lets you interact with your Neon Postgres databases in **natural language**.\n\n[![npm version](https://img.shields.io/npm/v/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![npm downloads](https://img.shields.io/npm/dt/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThe Model Context Protocol (MCP) is a [new, standardized protocol](https://modelcontextprotocol.io/introduction) designed to manage context between large language models (LLMs) and external systems. This repository offers an installer and an MCP Server for [Neon](https://neon.tech).\n\nNeon's MCP server acts as a bridge between natural language requests and the [Neon API](https://api-docs.neon.tech/reference/getting-started-with-neon-api). Built upon MCP, it translates your requests into the necessary API calls, enabling you to manage tasks such as creating projects and branches, running queries, and performing database migrations seamlessly.\n\nSome of the key features of the Neon MCP server include:\n\n- **Natural language interaction:** Manage Neon databases using intuitive, conversational commands.\n- **Simplified database management:** Perform complex actions without writing SQL or directly using the Neon API.\n- **Accessibility for non-developers:** Empower users with varying technical backgrounds to interact with Neon databases.\n- **Database migration support:** Leverage Neon's branching capabilities for database schema changes initiated via natural language.\n\nFor example, in Claude Desktop, or any MCP Client, you can use natural language to accomplish things with Neon, such as:\n\n- `Let's create a new Postgres database, and call it \"my-database\". Let's then create a table called users with the following columns: id, name, email, and password.`\n- `I want to run a migration on my project called \"my-project\" that alters the users table to add a new column called \"created_at\".`\n- `Can you give me a summary of all of my Neon projects and what data is in each one?`\n\n\u003e [!WARNING]  \n\u003e **Neon MCP Server Security Considerations**  \n\u003e The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.\n\u003e\n\u003e The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.\n\u003e\n\u003e For more information, see [MCP security guidance →](https://neon.tech/docs/ai/neon-mcp-server#mcp-security-guidance).\n\n## Setting up Neon MCP Server\n\nYou have two options for connecting your MCP client to Neon:\n\n1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys. Additionally, you will automatically receive the latest features and improvements as soon as they are released.\n\n2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.\n\n## Prerequisites\n\n- An MCP Client application.\n- A [Neon account](https://console.neon.tech/signup).\n- **Node.js (\u003e= v18.0.0) and npm:** Download from [nodejs.org](https://nodejs.org).\n\nFor Local MCP Server setup, you also need a Neon API key. See [Neon API Keys documentation](https://neon.tech/docs/manage/api-keys) for instructions on generating one.\n\n### Option 1. Remote Hosted MCP Server (Preview)\n\nConnect to Neon's managed MCP server using OAuth for authentication. This is the easiest setup, requires no local installation of this server, and doesn't need a Neon API key configured in the client.\n\n- Add the following \"Neon\" entry to your client's MCP server configuration file (e.g., `mcp.json`, `mcp_config.json`):\n\n  ```json\n  {\n    \"mcpServers\": {\n      \"Neon\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.neon.tech/mcp\"]\n      }\n    }\n  }\n  ```\n\n- Save the configuration file.\n- Restart or refresh your MCP client.\n- An OAuth window will open in your browser. Follow the prompts to authorize your MCP client to access your Neon account.\n\n\u003e With OAuth base authentication, the MCP server will, by default operate on projects under your personal Neon account. To access or manage projects under organization, you must explicitly provide either the `org_id` or the `project_id` in your prompt to MCP client.\n\nRemote MCP Server also supports authentication using API key in the `Authorization` header if your client supports it\n\n```json\n{\n  \"mcpServers\": {\n    \"Neon\": {\n      \"url\": \"https://mcp.neon.tech/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer \u003c$NEON_API_KEY\u003e\"\n      }\n    }\n  }\n}\n```\n\n\u003e Provider organization's API key to limit access to projects under the organization only.\n\nMCP supports two remote server transports: the deprecated Server-Sent Events (SSE) and the newer, recommended Streamable HTTP. If your LLM client doesn't support Streamable HTTP yet, you can switch the endpoint from `https://mcp.neon.tech/mcp` to `https://mcp.neon.tech/sse` to use SSE instead.\n\n### Option 2. Local MCP Server\n\nRun the Neon MCP server on your local machine with your Neon API key. This method allows you to manage your Neon projects and databases without relying on a remote MCP server.\n\nAdd the following JSON configuration within the `mcpServers` section of your client's `mcp_config` file, replacing `\u003cYOUR_NEON_API_KEY\u003e` with your actual Neon API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003cYOUR_NEON_API_KEY\u003e\"\n      ]\n    }\n  }\n}\n```\n\n### Troubleshooting\n\nIf your client does not use `JSON` for configuration of MCP servers (such as older versions of Cursor), you can use the following command when prompted:\n\n```bash\nnpx -y @neondatabase/mcp-server-neon start \u003cYOUR_NEON_API_KEY\u003e\n```\n\n#### Troubleshooting on Windows\n\nIf you are using Windows and encounter issues while adding the MCP server, you might need to use the Command Prompt (`cmd`) or Windows Subsystem for Linux (`wsl`) to run the necessary commands. Your configuration setup may resemble the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003cYOUR_NEON_API_KEY\u003e\"\n      ]\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"wsl\",\n      \"args\": [\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003cYOUR_NEON_API_KEY\u003e\"\n      ]\n    }\n  }\n}\n```\n\n## Guides\n\n- [Neon MCP Server Guide](https://neon.tech/docs/ai/neon-mcp-server)\n- [Connect MCP Clients to Neon](https://neon.tech/docs/ai/connect-mcp-clients-to-neon)\n- [Cursor with Neon MCP Server](https://neon.tech/guides/cursor-mcp-neon)\n- [Claude Desktop with Neon MCP Server](https://neon.tech/guides/neon-mcp-server)\n- [Cline with Neon MCP Server](https://neon.tech/guides/cline-mcp-neon)\n- [Windsurf with Neon MCP Server](https://neon.tech/guides/windsurf-mcp-neon)\n- [Zed with Neon MCP Server](https://neon.tech/guides/zed-mcp-neon)\n\n# Features\n\n## Supported Tools\n\nThe Neon MCP Server provides the following actions, which are exposed as \"tools\" to MCP Clients. You can use these tools to interact with your Neon projects and databases using natural language commands.\n\n**Project Management:**\n\n- **`list_projects`**: Lists the first 10 Neon projects in your account, providing a summary of each project. If you can't find a specific project, increase the limit by passing a higher value to the `limit` parameter.\n- **`list_shared_projects`**: Lists Neon projects shared with the current user. Supports a search parameter and limiting the number of projects returned (default: 10).\n- **`describe_project`**: Fetches detailed information about a specific Neon project, including its ID, name, and associated branches and databases.\n- **`create_project`**: Creates a new Neon project in your Neon account. A project acts as a container for branches, databases, roles, and computes.\n- **`delete_project`**: Deletes an existing Neon project and all its associated resources.\n\n**Branch Management:**\n\n- **`create_branch`**: Creates a new branch within a specified Neon project. Leverages [Neon's branching](/docs/introduction/branching) feature for development, testing, or migrations.\n- **`delete_branch`**: Deletes an existing branch from a Neon project.\n- **`describe_branch`**: Retrieves details about a specific branch, such as its name, ID, and parent branch.\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, including compute ID, type, size, and autoscaling information.\n- **`list_organizations`**: Lists all organizations that the current user has access to. Optionally filter by organization name or ID using the search parameter.\n- **`reset_from_parent`**: Resets the current branch to its parent's state, discarding local changes. Automatically preserves to backup if branch has children, or optionally preserve on request with a custom name.\n\n**SQL Query Execution:**\n\n- **`get_connection_string`**: Returns your database connection string.\n- **`run_sql`**: Executes a single SQL query against a specified Neon database. Supports both read and write operations.\n- **`run_sql_transaction`**: Executes a series of SQL queries within a single transaction against a Neon database.\n- **`get_database_tables`**: Lists all tables within a specified Neon database.\n- **`describe_table_schema`**: Retrieves the schema definition of a specific table, detailing columns, data types, and constraints.\n- **`list_slow_queries`**: Identifies performance bottlenecks by finding the slowest queries in a database. Requires the pg_stat_statements extension.\n\n**Database Migrations (Schema Changes):**\n\n- **`prepare_database_migration`**: Initiates a database migration process. Critically, it creates a temporary branch to apply and test the migration safely before affecting the main branch.\n- **`complete_database_migration`**: Finalizes and applies a prepared database migration to the main branch. This action merges changes from the temporary migration branch and cleans up temporary resources.\n\n**Query Performance Optimization:**\n\n- **`explain_sql_statement`**: Provides detailed execution plans for SQL queries to help identify performance bottlenecks.\n- **`prepare_query_tuning`**: Analyzes query performance and suggests optimizations like index creation. Creates a temporary branch for safely testing these optimizations.\n- **`complete_query_tuning`**: Applies or discards query optimizations after testing. Can merge changes from the temporary branch to the main branch.\n- **`list_slow_queries`**: Identifies and analyzes slow-performing queries in your database. Requires the `pg_stat_statements` extension.\n\n**Compute Management:**\n\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, showing details like compute ID, type, size, and last active time.\n\n**Neon Auth:**\n\n- **`provision_neon_auth`**: Provisions Neon Auth for a Neon project. It allows developers to easily set up authentication infrastructure by creating an integration with Stack Auth (`@stackframe/stack`).\n\n**Query Performance Tuning:**\n\n- **`explain_sql_statement`**: Analyzes a SQL query and returns detailed execution plan information to help understand query performance.\n- **`prepare_query_tuning`**: Identifies potential performance issues in a SQL query and suggests optimizations. Creates a temporary branch for testing improvements.\n- **`complete_query_tuning`**: Finalizes and applies query optimizations after testing. Merges changes from the temporary tuning branch to the main branch.\n\n## Migrations\n\nMigrations are a way to manage changes to your database schema over time. With the Neon MCP server, LLMs are empowered to do migrations safely with separate \"Start\" (`prepare_database_migration`) and \"Commit\" (`complete_database_migration`) commands.\n\nThe \"Start\" command accepts a migration and runs it in a new temporary branch. Upon returning, this command hints to the LLM that it should test the migration on this branch. The LLM can then run the \"Commit\" command to apply the migration to the original branch.\n\n# Development\n\n## Development with MCP CLI Client\n\nThe easiest way to iterate on the MCP Server is using the `mcp-client/`. Learn more in `mcp-client/README.md`.\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\ncd mcp-client/ \u0026\u0026 NEON_API_KEY=... npm run start:mcp-server-neon\n```\n\n## Development with Claude Desktop (Local MCP Server)\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\nnode dist/index.js init $NEON_API_KEY\n```\n\nThen, **restart Claude** each time you want to test changes.\n\n# Testing\n\nTo run the tests you need to setup the `.env` file according to the `.env.example` file.\n\n```bash\nnpm run test\n```\n","stargazer_count":490,"uses_custom_opengraph_image":false}}}},{"name":"apify/apify-mcp-server","description":"Extract data from any website with thousands of scrapers, crawlers, and automations on Apify Store ⚡","status":"active","repository":{"url":"https://github.com/apify/apify-mcp-server","source":"github","id":"911256711","readme":"\u003ch1 align=\"center\"\u003e\n    \u003ca href=\"https://mcp.apify.com\"\u003e\n        \u003cpicture\u003e\n            \u003csource media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_dark_background.png\"\u003e\n            \u003cimg alt=\"Apify MCP Server\" src=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_white_background.png\" width=\"500\"\u003e\n        \u003c/picture\u003e\n    \u003c/a\u003e\n    \u003cbr\u003e\n    \u003csmall\u003e\u003ca href=\"https://mcp.apify.com\"\u003emcp.apify.com\u003c/a\u003e\u003c/small\u003e\n\u003c/h1\u003e\n\n\u003cp align=center\u003e\n    \u003ca href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"\u003e\u003cimg src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" alt=\"NPM latest version\" data-canonical-src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"\u003e\u003cimg src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml\"\u003e\u003cimg src=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://apify.com/apify/actors-mcp-server\"\u003e\u003cimg src=\"https://apify.com/actor-badge?actor=apify/actors-mcp-server\" alt=\"Actor runs\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\nThe Apify Model Context Protocol (MCP) server at [**mcp.apify.com**](https://mcp.apify.com) enables your AI agents to extract data from social media, search engines, maps, e-commerce sites, or any other website using thousands of ready-made scrapers, crawlers, and automation tools available on the [Apify Store](https://apify.com/store).\n\n\u003e **🚀 Try the hosted Apify MCP Server!**\n\u003e\n\u003e For the easiest setup and most powerful features, including the ability to find and use any Actor from Apify Store, connect your AI assistant to our hosted server:\n\u003e\n\u003e **[`https://mcp.apify.com`](https://mcp.apify.com)**\n\u003e\n\u003e It supports OAuth, so you can connect from clients like Claude.ai or Visual Studio Code with just the URL.\n\n![Apify-MCP-server](https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify-mcp-server.png)\n\n## Table of Contents\n- [🌐 Introducing the Apify MCP server](#-introducing-the-apify-mcp-server)\n- [🚀 Quickstart](#-quickstart)\n- [🤖 MCP clients and examples](#-mcp-clients-and-examples)\n- [🪄 Try Apify MCP instantly](#-try-apify-mcp-instantly)\n- [🛠️ Tools, resources, and prompts](#-tools-resources-and-prompts)\n- [🐛 Troubleshooting (local MCP server)](#-troubleshooting-local-mcp-server)\n- [⚙️ Development](#-development)\n- [🤝 Contributing](#-contributing)\n- [📚 Learn more](#-learn-more)\n\n# 🌐 Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.\nFor example, it can:\n- Use [Facebook Posts Scraper](https://apify.com/apify/facebook-posts-scraper) to extract data from Facebook posts from multiple pages/profiles.\n- Use [Google Maps Email Extractor](https://apify.com/lukaskrivka/google-maps-with-contact-details) to extract contact details from Google Maps.\n- Use [Google Search Results Scraper](https://apify.com/apify/google-search-scraper) to scrape Google Search Engine Results Pages (SERPs).\n- Use [Instagram Scraper](https://apify.com/apify/instagram-scraper) to scrape Instagram posts, profiles, places, photos, and comments.\n- Use [RAG Web Browser](https://apify.com/apify/web-scraper) to search the web, scrape the top N URLs, and return their content.\n\n**Video tutorial: Integrate 5,000+ Apify Actors and Agents with Claude**\n\n[![Apify MCP Server Tutorial: Integrate 5,000+ Apify Actors and Agents with Claude](https://img.youtube.com/vi/BKu8H91uCTg/hqdefault.jpg)](https://www.youtube.com/watch?v=BKu8H91uCTg)\n\n# 🚀 Quickstart\n\nYou can use the Apify MCP Server in two ways:\n\n**HTTPS Endpoint (mcp.apify.com)**: Connect from your MCP client via OAuth or by including the `Authorization: Bearer \u003cAPIFY_TOKEN\u003e` header in your requests. This is the recommended method for most use cases. Because it supports OAuth, you can connect from clients like [Claude.ai](https://claude.ai) or [Visual Studio Code](https://code.visualstudio.com/) using just the URL: `https://mcp.apify.com`.\n- `https://mcp.apify.com` streamable transport\n\n**Standard Input/Output (stdio)**: Ideal for local integrations and command-line tools like the Claude for Desktop client.\n- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.\n\nYou can find detailed instructions for setting up the MCP server in the [Apify documentation](https://docs.apify.com/platform/integrations/mcp).\n\n# 🤖 MCP clients and examples\n\nTo interact with the Apify MCP server, you can use various MCP clients, such as:\n- [Claude Desktop](https://claude.ai/download)\n- [Visual Studio Code](https://code.visualstudio.com/)\n- [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- Other clients at [https://modelcontextprotocol.io/clients](https://modelcontextprotocol.io/clients)\n- More clients at [https://glama.ai/mcp/clients](https://glama.ai/mcp/clients)\n\nWith MCP server integrated, you can ask your AI assistant things like:\n- \"Search the web and summarize recent trends in AI Agents.\"\n- \"Find the top 10 Italian restaurants in San Francisco.\"\n- \"Find and analyze the Instagram profile of The Rock.\"\n- \"Provide a step-by-step guide on using the Model Context Protocol, including source URLs.\"\n- \"What Apify Actors can I use?\"\n\n### Supported clients matrix\n\nThe following table outlines the tested MCP clients and their level of support for key features.\n\n| Client | Dynamic Tool Discovery | Notes |\n| --- | --- | --- |\n| **Claude.ai (web)** | ✅ Full | |\n| **Claude Desktop** | 🟡 Partial | Tools may need to be reloaded manually in the client. |\n| **VS Code (Genie)** | ✅ Full | |\n| **Apify Tester MCP Client** | ✅ Full | Designed for testing Apify MCP servers. |\n\nApify MCP Server is compatible with any MCP client that adheres to the [Model Context Protocol](https://modelcontextprotocol.org/), but the level of support for dynamic tool discovery and other features may vary between clients. Therefore, the server uses [mcp-client-capabilities](https://github.com/apify/mcp-client-capabilities) to detect client capabilities and adjust its behavior accordingly.\n\n**Smart tool selection based on client capabilities:**\n\nWhen the `actors` tool category is requested, the server intelligently selects the most appropriate Actor-related tools based on the client's capabilities:\n\n- **Clients with dynamic tool support** (e.g., Claude.ai web, VS Code Genie): The server provides the `add-actor` tool instead of `call-actor`. This allows for a better user experience where users can dynamically discover and add new Actors as tools during their conversation.\n\n- **Clients with limited dynamic tool support** (e.g., Claude Desktop): The server provides the standard `call-actor` tool along with other Actor category tools, ensuring compatibility while maintaining functionality.\n\n# 🪄 Try Apify MCP instantly\n\nWant to try Apify MCP without any setup?\n\nCheck out [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n\nThis interactive, chat-like interface provides an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!\n\nOr use the MCP bundle file (formerly known as Anthropic Desktop extension file, or DXT) for one-click installation: [Apify MCP server MCPB file](https://github.com/apify/apify-mcp-server/releases/latest/download/apify-mcp-server.mcpb)\n\n# 🛠️ Tools, resources, and prompts\n\nThe MCP server provides a set of tools for interacting with Apify Actors.\nSince the Apify Store is large and growing rapidly, the MCP server provides a way to dynamically discover and use new Actors.\n\n### Actors\n\nAny [Apify Actor](https://apify.com/store) can be used as a tool.\nBy default, the server is pre-configured with one Actor, `apify/rag-web-browser`, and several helper tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.\n\n\nFor example, for the `apify/rag-web-browser` Actor, the input parameters are:\n\n```json\n{\n  \"query\": \"restaurants in San Francisco\",\n  \"maxResults\": 3\n}\n```\nYou don't need to manually specify which Actor to call or its input parameters; the LLM handles this automatically.\nWhen a tool is called, the arguments are automatically passed to the Actor by the LLM.\nYou can refer to the specific Actor's documentation for a list of available arguments.\n\n### Helper tools\n\nOne of the most powerful features of using MCP with Apify is dynamic tool discovery.\nIt gives an AI agent the ability to find new tools (Actors) as needed and incorporate them.\nHere are some special MCP operations and how the Apify MCP Server supports them:\n\n- **Apify Actors**: Search for Actors, view their details, and use them as tools for the AI.\n- **Apify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs**: Get lists of your Actor runs, inspect their details, and retrieve logs.\n- **Apify storage**: Access data from your datasets and key-value stores.\n\n### Overview of available tools\n\nHere is an overview list of all the tools provided by the Apify MCP Server.\n\n| Tool name | Category | Description | Enabled by default |\n| :--- | :--- | :--- | :---: |\n| `search-actors` | actors | Search for Actors in the Apify Store. | ✅ |\n| `fetch-actor-details` | actors | Retrieve detailed information about a specific Actor. | ✅ |\n| `call-actor`* | actors | Call an Actor and get its run results. | ❔ |\n| [`apify-slash-rag-web-browser`](https://apify.com/apify/rag-web-browser) | Actor (see [tool configuration](#tools-configuration)) | An Actor tool to browse the web. | ✅ |\n| `search-apify-docs` | docs | Search the Apify documentation for relevant pages. | ✅ |\n| `fetch-apify-docs` | docs | Fetch the full content of an Apify documentation page by its URL. | ✅ |\n| `get-actor-run` | runs | Get detailed information about a specific Actor run. |  |\n| `get-actor-run-list` | runs | Get a list of an Actor's runs, filterable by status. |  |\n| `get-actor-log` | runs | Retrieve the logs for a specific Actor run. |  |\n| `get-dataset` | storage | Get metadata about a specific dataset. |  |\n| `get-dataset-items` | storage | Retrieve items from a dataset with support for filtering and pagination. |  |\n| `get-dataset-schema` | storage | Generate a JSON schema from dataset items. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user. |  |\n| `get-key-value-store-list`| storage | List all available key-value stores for the user. |  |\n| `add-actor`* | experimental | Add an Actor as a new tool for the user to call. | ❔ |\n| `get-actor-output`* | - | Retrieve the output from an Actor call which is not included in the output preview of the Actor tool. | ✅ |\n\n\u003e **Note:**\n\u003e\n\u003e When using the `actors` tool category, clients that support dynamic tool discovery (like Claude.ai web and VS Code) automatically receive the `add-actor` tool instead of `call-actor` for enhanced Actor discovery capabilities.\n\n\u003e The `get-actor-output` tool is automatically included with any Actor-related tool, such as `call-actor`, `add-actor`, or any specific Actor tool like `apify-slash-rag-web-browser`. When you call an Actor - either through the `call-actor` tool or directly via an Actor tool (e.g., `apify-slash-rag-web-browser`) - you receive a preview of the output. The preview depends on the Actor's output format and length; for some Actors and runs, it may include the entire output, while for others, only a limited version is returned to avoid overwhelming the LLM. To retrieve the full output of an Actor run, use the `get-actor-output` tool (supports limit, offset, and field filtering) with the `datasetId` provided by the Actor call.\n\n### Tools configuration\n\nThe `tools` configuration parameter is used to specify loaded tools - either categories or specific tools directly, and Apify Actors. For example, `tools=storage,runs` loads two categories; `tools=add-actor` loads just one tool.\n\nWhen no query parameters are provided, the MCP server loads the following `tools` by default:\n\n- `actors`\n- `docs`\n- `apify/rag-web-browser`\n\nIf the tools parameter is specified, only the listed tools or categories will be enabled - no default tools will be included.\n\n\u003e **Easy configuration:**\n\u003e\n\u003e Use the [UI configurator](https://mcp.apify.com/) to configure your server, then copy the configuration to your client.\n\n**Configuring the hosted server:**\n\nThe hosted server can be configured using query parameters in the URL. For example, to load the default tools, use:\n\n```\nhttps://mcp.apify.com?tools=actors,docs,apify/rag-web-browser\n```\n\nFor minimal configuration, if you want to use only a single Actor tool - without any discovery or generic calling tools, the server can be configured as follows:\n\n```\nhttps://mcp.apify.com?tools=apify/my-actor\n```\n\nThis setup exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n**Configuring the CLI:**\n\nThe CLI can be configured using command-line flags. For example, to load the same tools as in the hosted server configuration, use:\n\n```bash\nnpx @apify/actors-mcp-server --tools actors,docs,apify/rag-web-browser\n```\n\nThe minimal configuration is similar to the hosted server configuration:\n\n```bash\nnpx @apify/actors-mcp-server --tools apify/my-actor\n```\n\nAs above, this exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n\u003e **⚠️ Important recommendation**\n\u003e\n\u003e **The default tools configuration may change in future versions.** When no `tools` parameter is specified, the server currently loads default tools, but this behavior is subject to change.\n\u003e\n\u003e **For production use and stable interfaces, always explicitly specify the `tools` parameter** to ensure your configuration remains consistent across updates.\n\n### Backward compatibility\n\nThe v2 configuration preserves backward compatibility with v1 usage. Notes:\n\n- `actors` param (URL) and `--actors` flag (CLI) are still supported.\n  - Internally they are merged into `tools` selectors.\n  - Examples: `?actors=apify/rag-web-browser` ≡ `?tools=apify/rag-web-browser`; `--actors apify/rag-web-browser` ≡ `--tools apify/rag-web-browser`.\n- `enable-adding-actors` (CLI) and `enableAddingActors` (URL) are supported but deprecated.\n  - Prefer `tools=experimental` or including the specific tool `tools=add-actor`.\n  - Behavior remains: when enabled with no `tools` specified, the server exposes only `add-actor`; when categories/tools are selected, `add-actor` is also included.\n- `enableActorAutoLoading` remains as a legacy alias for `enableAddingActors` and is mapped automatically.\n- Defaults remain compatible: when no `tools` are specified, the server loads `actors`, `docs`, and `apify/rag-web-browser`.\n  - If any `tools` are specified, the defaults are not added (same as v1 intent for explicit selection).\n- `call-actor` is now included by default via the `actors` category (additive change). To exclude it, specify an explicit `tools` list without `actors`.\n- `preview` category is deprecated and removed. Use specific tool names instead.\n\nExisting URLs and commands using `?actors=...` or `--actors` continue to work unchanged.\n\n### Prompts\n\nThe server provides a set of predefined example prompts to help you get started interacting with Apify through MCP. For example, there is a `GetLatestNewsOnTopic` prompt that allows you to easily retrieve the latest news on a specific topic using the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor.\n\n### Resources\n\nThe server does not yet provide any resources.\n\n### Debugging the NPM package\n\nTo debug the server, use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) tool:\n\n```shell\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector npx -y @apify/actors-mcp-server\n```\n\n# ⚙️ Development\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/en) (v18 or higher)\n\nCreate an environment file, `.env`, with the following content:\n```text\nAPIFY_TOKEN=\"your-apify-token\"\n```\n\nBuild the `actor-mcp-server` package:\n\n```bash\nnpm run build\n```\n\n## Start HTTP streamable MCP server\n\nRun using Apify CLI:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nexport APIFY_META_ORIGIN=STANDBY\napify run -p\n```\n\nOnce the server is running, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to debug the server exposed at `http://localhost:3001`.\n\n## Start standard input/output (stdio) MCP server\n\nYou can launch the MCP Inspector with this command:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.\n\n## 🐦 Canary PR releases\n\nApify MCP is split across two repositories: this one for core MCP logic and the private `apify-mcp-server-internal` for the hosted server.\nChanges must be synchronized between both.\n\nTo create a canary release, add the `beta` tag to your PR branch.\nThis publishes the package to [pkg.pr.new](https://pkg.pr.new/) for staging and testing before merging.\nSee [the workflow file](.github/workflows/pre_release.yaml) for details.\n\n## 🐋 Docker Hub integration\nThe Apify MCP Server is also available on [Docker Hub](https://hub.docker.com/mcp/server/apify-mcp-server/overview), registered via the [mcp-registry](https://github.com/docker/mcp-registry) repository. The entry in `servers/apify-mcp-server/server.yaml` should be deployed automatically by the Docker Hub MCP registry (deployment frequency is unknown). **Before making major changes to the `stdio` server version, be sure to test it locally to ensure the Docker build passes.** To test, change the `source.branch` to your PR branch and run `task build -- apify-mcp-server`. For more details, see [CONTRIBUTING.md](https://github.com/docker/mcp-registry/blob/main/CONTRIBUTING.md).\n\n# 🐛 Troubleshooting (local MCP server)\n\n- Make sure you have `node` installed by running `node -v`.\n- Make sure the `APIFY_TOKEN` environment variable is set.\n- Always use the latest version of the MCP server by using `@apify/actors-mcp-server@latest`.\n\n## 💡 Limitations\n\nThe Actor input schema is processed to be compatible with most MCP clients while adhering to [JSON Schema](https://json-schema.org/) standards. The processing includes:\n- **Descriptions** are truncated to 500 characters (as defined in `MAX_DESCRIPTION_LENGTH`).\n- **Enum fields** are truncated to a maximum combined length of 2000 characters for all elements (as defined in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.\n- **Nested properties** are built for special cases like proxy configuration and request list sources to ensure the correct input structure.\n- **Array item types** are inferred when not explicitly defined in the schema, using a priority order: explicit type in items \u003e prefill type \u003e default value type \u003e editor type.\n- **Enum values and examples** are added to property descriptions to ensure visibility, even if the client doesn't fully support the JSON schema.\n- **Rental Actors** are only available for use with the hosted MCP server at https://mcp.apify.com. When running the server locally via stdio, you can only access Actors that are already added to your local toolset. To dynamically search for and use any Actor from the Apify Store—including rental Actors—connect to the hosted endpoint.\n\n# 🤝 Contributing\n\nWe welcome contributions to improve the Apify MCP Server! Here's how you can help:\n\n- **🐛 Report issues**: Find a bug or have a feature request? [Open an issue](https://github.com/apify/apify-mcp-server/issues).\n- **🔧 Submit pull requests**: Fork the repo and submit pull requests with enhancements or fixes.\n- **📚 Documentation**: Improvements to docs and examples are always welcome.\n- **💡 Share use cases**: Contribute examples to help other users.\n\nFor major changes, please open an issue first to discuss your proposal and ensure it aligns with the project's goals.\n\n# 📚 Learn more\n\n- [Model Context Protocol](https://modelcontextprotocol.org/)\n- [What are AI Agents?](https://blog.apify.com/what-are-ai-agents/)\n- [What is MCP and why does it matter?](https://blog.apify.com/what-is-model-context-protocol/)\n- [How to use MCP with Apify Actors](https://blog.apify.com/how-to-use-mcp/)\n- [Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- [Webinar: Building and Monetizing MCP Servers on Apify](https://www.youtube.com/watch?v=w3AH3jIrXXo)\n- [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)\n"},"version":"0.0.1-seed","created_at":"2025-09-24T16:27:56Z","updated_at":"2025-10-22T11:21:47Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.apify.com"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"1aef85cf-1597-47f1-84ae-3e9a771c2162","is_latest":true,"published_at":"2025-09-09T10:55:22.907061Z","updated_at":"2025-09-09T10:55:22.907061Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Apify","homepage_url":"https://mcp.apify.com","is_in_organization":true,"license":"MIT License","name":"apify-mcp-server","name_with_owner":"apify/apify-mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/c763cc917a72df353a45d7b2e88916cfd861b254c6124ac8ee9b028154b38aee/apify/apify-mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/24586296?v=4","preferred_image":"https://avatars.githubusercontent.com/u/24586296?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-21T21:31:19Z","readme":"\u003ch1 align=\"center\"\u003e\n    \u003ca href=\"https://mcp.apify.com\"\u003e\n        \u003cpicture\u003e\n            \u003csource media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_dark_background.png\"\u003e\n            \u003cimg alt=\"Apify MCP Server\" src=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_white_background.png\" width=\"500\"\u003e\n        \u003c/picture\u003e\n    \u003c/a\u003e\n    \u003cbr\u003e\n    \u003csmall\u003e\u003ca href=\"https://mcp.apify.com\"\u003emcp.apify.com\u003c/a\u003e\u003c/small\u003e\n\u003c/h1\u003e\n\n\u003cp align=center\u003e\n    \u003ca href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"\u003e\u003cimg src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" alt=\"NPM latest version\" data-canonical-src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"\u003e\u003cimg src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml\"\u003e\u003cimg src=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n    \u003ca href=\"https://apify.com/apify/actors-mcp-server\"\u003e\u003cimg src=\"https://apify.com/actor-badge?actor=apify/actors-mcp-server\" alt=\"Actor runs\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\nThe Apify Model Context Protocol (MCP) server at [**mcp.apify.com**](https://mcp.apify.com) enables your AI agents to extract data from social media, search engines, maps, e-commerce sites, or any other website using thousands of ready-made scrapers, crawlers, and automation tools available on the [Apify Store](https://apify.com/store).\n\n\u003e **🚀 Try the hosted Apify MCP Server!**\n\u003e\n\u003e For the easiest setup and most powerful features, including the ability to find and use any Actor from Apify Store, connect your AI assistant to our hosted server:\n\u003e\n\u003e **[`https://mcp.apify.com`](https://mcp.apify.com)**\n\u003e\n\u003e It supports OAuth, so you can connect from clients like Claude.ai or Visual Studio Code with just the URL.\n\n![Apify-MCP-server](https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify-mcp-server.png)\n\n## Table of Contents\n- [🌐 Introducing the Apify MCP server](#-introducing-the-apify-mcp-server)\n- [🚀 Quickstart](#-quickstart)\n- [🤖 MCP clients and examples](#-mcp-clients-and-examples)\n- [🪄 Try Apify MCP instantly](#-try-apify-mcp-instantly)\n- [🛠️ Tools, resources, and prompts](#-tools-resources-and-prompts)\n- [🐛 Troubleshooting (local MCP server)](#-troubleshooting-local-mcp-server)\n- [⚙️ Development](#-development)\n- [🤝 Contributing](#-contributing)\n- [📚 Learn more](#-learn-more)\n\n# 🌐 Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.\nFor example, it can:\n- Use [Facebook Posts Scraper](https://apify.com/apify/facebook-posts-scraper) to extract data from Facebook posts from multiple pages/profiles.\n- Use [Google Maps Email Extractor](https://apify.com/lukaskrivka/google-maps-with-contact-details) to extract contact details from Google Maps.\n- Use [Google Search Results Scraper](https://apify.com/apify/google-search-scraper) to scrape Google Search Engine Results Pages (SERPs).\n- Use [Instagram Scraper](https://apify.com/apify/instagram-scraper) to scrape Instagram posts, profiles, places, photos, and comments.\n- Use [RAG Web Browser](https://apify.com/apify/web-scraper) to search the web, scrape the top N URLs, and return their content.\n\n**Video tutorial: Integrate 5,000+ Apify Actors and Agents with Claude**\n\n[![Apify MCP Server Tutorial: Integrate 5,000+ Apify Actors and Agents with Claude](https://img.youtube.com/vi/BKu8H91uCTg/hqdefault.jpg)](https://www.youtube.com/watch?v=BKu8H91uCTg)\n\n# 🚀 Quickstart\n\nYou can use the Apify MCP Server in two ways:\n\n**HTTPS Endpoint (mcp.apify.com)**: Connect from your MCP client via OAuth or by including the `Authorization: Bearer \u003cAPIFY_TOKEN\u003e` header in your requests. This is the recommended method for most use cases. Because it supports OAuth, you can connect from clients like [Claude.ai](https://claude.ai) or [Visual Studio Code](https://code.visualstudio.com/) using just the URL: `https://mcp.apify.com`.\n- `https://mcp.apify.com` streamable transport\n\n**Standard Input/Output (stdio)**: Ideal for local integrations and command-line tools like the Claude for Desktop client.\n- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.\n\nYou can find detailed instructions for setting up the MCP server in the [Apify documentation](https://docs.apify.com/platform/integrations/mcp).\n\n# 🤖 MCP clients and examples\n\nTo interact with the Apify MCP server, you can use various MCP clients, such as:\n- [Claude Desktop](https://claude.ai/download)\n- [Visual Studio Code](https://code.visualstudio.com/)\n- [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- Other clients at [https://modelcontextprotocol.io/clients](https://modelcontextprotocol.io/clients)\n- More clients at [https://glama.ai/mcp/clients](https://glama.ai/mcp/clients)\n\nWith MCP server integrated, you can ask your AI assistant things like:\n- \"Search the web and summarize recent trends in AI Agents.\"\n- \"Find the top 10 Italian restaurants in San Francisco.\"\n- \"Find and analyze the Instagram profile of The Rock.\"\n- \"Provide a step-by-step guide on using the Model Context Protocol, including source URLs.\"\n- \"What Apify Actors can I use?\"\n\n### Supported clients matrix\n\nThe following table outlines the tested MCP clients and their level of support for key features.\n\n| Client | Dynamic Tool Discovery | Notes |\n| --- | --- | --- |\n| **Claude.ai (web)** | ✅ Full | |\n| **Claude Desktop** | 🟡 Partial | Tools may need to be reloaded manually in the client. |\n| **VS Code (Genie)** | ✅ Full | |\n| **Apify Tester MCP Client** | ✅ Full | Designed for testing Apify MCP servers. |\n\nApify MCP Server is compatible with any MCP client that adheres to the [Model Context Protocol](https://modelcontextprotocol.org/), but the level of support for dynamic tool discovery and other features may vary between clients. Therefore, the server uses [mcp-client-capabilities](https://github.com/apify/mcp-client-capabilities) to detect client capabilities and adjust its behavior accordingly.\n\n**Smart tool selection based on client capabilities:**\n\nWhen the `actors` tool category is requested, the server intelligently selects the most appropriate Actor-related tools based on the client's capabilities:\n\n- **Clients with dynamic tool support** (e.g., Claude.ai web, VS Code Genie): The server provides the `add-actor` tool instead of `call-actor`. This allows for a better user experience where users can dynamically discover and add new Actors as tools during their conversation.\n\n- **Clients with limited dynamic tool support** (e.g., Claude Desktop): The server provides the standard `call-actor` tool along with other Actor category tools, ensuring compatibility while maintaining functionality.\n\n# 🪄 Try Apify MCP instantly\n\nWant to try Apify MCP without any setup?\n\nCheck out [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n\nThis interactive, chat-like interface provides an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!\n\nOr use the MCP bundle file (formerly known as Anthropic Desktop extension file, or DXT) for one-click installation: [Apify MCP server MCPB file](https://github.com/apify/apify-mcp-server/releases/latest/download/apify-mcp-server.mcpb)\n\n# 🛠️ Tools, resources, and prompts\n\nThe MCP server provides a set of tools for interacting with Apify Actors.\nSince the Apify Store is large and growing rapidly, the MCP server provides a way to dynamically discover and use new Actors.\n\n### Actors\n\nAny [Apify Actor](https://apify.com/store) can be used as a tool.\nBy default, the server is pre-configured with one Actor, `apify/rag-web-browser`, and several helper tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.\n\n\nFor example, for the `apify/rag-web-browser` Actor, the input parameters are:\n\n```json\n{\n  \"query\": \"restaurants in San Francisco\",\n  \"maxResults\": 3\n}\n```\nYou don't need to manually specify which Actor to call or its input parameters; the LLM handles this automatically.\nWhen a tool is called, the arguments are automatically passed to the Actor by the LLM.\nYou can refer to the specific Actor's documentation for a list of available arguments.\n\n### Helper tools\n\nOne of the most powerful features of using MCP with Apify is dynamic tool discovery.\nIt gives an AI agent the ability to find new tools (Actors) as needed and incorporate them.\nHere are some special MCP operations and how the Apify MCP Server supports them:\n\n- **Apify Actors**: Search for Actors, view their details, and use them as tools for the AI.\n- **Apify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs**: Get lists of your Actor runs, inspect their details, and retrieve logs.\n- **Apify storage**: Access data from your datasets and key-value stores.\n\n### Overview of available tools\n\nHere is an overview list of all the tools provided by the Apify MCP Server.\n\n| Tool name | Category | Description | Enabled by default |\n| :--- | :--- | :--- | :---: |\n| `search-actors` | actors | Search for Actors in the Apify Store. | ✅ |\n| `fetch-actor-details` | actors | Retrieve detailed information about a specific Actor. | ✅ |\n| `call-actor`* | actors | Call an Actor and get its run results. | ❔ |\n| [`apify-slash-rag-web-browser`](https://apify.com/apify/rag-web-browser) | Actor (see [tool configuration](#tools-configuration)) | An Actor tool to browse the web. | ✅ |\n| `search-apify-docs` | docs | Search the Apify documentation for relevant pages. | ✅ |\n| `fetch-apify-docs` | docs | Fetch the full content of an Apify documentation page by its URL. | ✅ |\n| `get-actor-run` | runs | Get detailed information about a specific Actor run. |  |\n| `get-actor-run-list` | runs | Get a list of an Actor's runs, filterable by status. |  |\n| `get-actor-log` | runs | Retrieve the logs for a specific Actor run. |  |\n| `get-dataset` | storage | Get metadata about a specific dataset. |  |\n| `get-dataset-items` | storage | Retrieve items from a dataset with support for filtering and pagination. |  |\n| `get-dataset-schema` | storage | Generate a JSON schema from dataset items. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user. |  |\n| `get-key-value-store-list`| storage | List all available key-value stores for the user. |  |\n| `add-actor`* | experimental | Add an Actor as a new tool for the user to call. | ❔ |\n| `get-actor-output`* | - | Retrieve the output from an Actor call which is not included in the output preview of the Actor tool. | ✅ |\n\n\u003e **Note:**\n\u003e\n\u003e When using the `actors` tool category, clients that support dynamic tool discovery (like Claude.ai web and VS Code) automatically receive the `add-actor` tool instead of `call-actor` for enhanced Actor discovery capabilities.\n\n\u003e The `get-actor-output` tool is automatically included with any Actor-related tool, such as `call-actor`, `add-actor`, or any specific Actor tool like `apify-slash-rag-web-browser`. When you call an Actor - either through the `call-actor` tool or directly via an Actor tool (e.g., `apify-slash-rag-web-browser`) - you receive a preview of the output. The preview depends on the Actor's output format and length; for some Actors and runs, it may include the entire output, while for others, only a limited version is returned to avoid overwhelming the LLM. To retrieve the full output of an Actor run, use the `get-actor-output` tool (supports limit, offset, and field filtering) with the `datasetId` provided by the Actor call.\n\n### Tools configuration\n\nThe `tools` configuration parameter is used to specify loaded tools - either categories or specific tools directly, and Apify Actors. For example, `tools=storage,runs` loads two categories; `tools=add-actor` loads just one tool.\n\nWhen no query parameters are provided, the MCP server loads the following `tools` by default:\n\n- `actors`\n- `docs`\n- `apify/rag-web-browser`\n\nIf the tools parameter is specified, only the listed tools or categories will be enabled - no default tools will be included.\n\n\u003e **Easy configuration:**\n\u003e\n\u003e Use the [UI configurator](https://mcp.apify.com/) to configure your server, then copy the configuration to your client.\n\n**Configuring the hosted server:**\n\nThe hosted server can be configured using query parameters in the URL. For example, to load the default tools, use:\n\n```\nhttps://mcp.apify.com?tools=actors,docs,apify/rag-web-browser\n```\n\nFor minimal configuration, if you want to use only a single Actor tool - without any discovery or generic calling tools, the server can be configured as follows:\n\n```\nhttps://mcp.apify.com?tools=apify/my-actor\n```\n\nThis setup exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n**Configuring the CLI:**\n\nThe CLI can be configured using command-line flags. For example, to load the same tools as in the hosted server configuration, use:\n\n```bash\nnpx @apify/actors-mcp-server --tools actors,docs,apify/rag-web-browser\n```\n\nThe minimal configuration is similar to the hosted server configuration:\n\n```bash\nnpx @apify/actors-mcp-server --tools apify/my-actor\n```\n\nAs above, this exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n\u003e **⚠️ Important recommendation**\n\u003e\n\u003e **The default tools configuration may change in future versions.** When no `tools` parameter is specified, the server currently loads default tools, but this behavior is subject to change.\n\u003e\n\u003e **For production use and stable interfaces, always explicitly specify the `tools` parameter** to ensure your configuration remains consistent across updates.\n\n### Backward compatibility\n\nThe v2 configuration preserves backward compatibility with v1 usage. Notes:\n\n- `actors` param (URL) and `--actors` flag (CLI) are still supported.\n  - Internally they are merged into `tools` selectors.\n  - Examples: `?actors=apify/rag-web-browser` ≡ `?tools=apify/rag-web-browser`; `--actors apify/rag-web-browser` ≡ `--tools apify/rag-web-browser`.\n- `enable-adding-actors` (CLI) and `enableAddingActors` (URL) are supported but deprecated.\n  - Prefer `tools=experimental` or including the specific tool `tools=add-actor`.\n  - Behavior remains: when enabled with no `tools` specified, the server exposes only `add-actor`; when categories/tools are selected, `add-actor` is also included.\n- `enableActorAutoLoading` remains as a legacy alias for `enableAddingActors` and is mapped automatically.\n- Defaults remain compatible: when no `tools` are specified, the server loads `actors`, `docs`, and `apify/rag-web-browser`.\n  - If any `tools` are specified, the defaults are not added (same as v1 intent for explicit selection).\n- `call-actor` is now included by default via the `actors` category (additive change). To exclude it, specify an explicit `tools` list without `actors`.\n- `preview` category is deprecated and removed. Use specific tool names instead.\n\nExisting URLs and commands using `?actors=...` or `--actors` continue to work unchanged.\n\n### Prompts\n\nThe server provides a set of predefined example prompts to help you get started interacting with Apify through MCP. For example, there is a `GetLatestNewsOnTopic` prompt that allows you to easily retrieve the latest news on a specific topic using the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor.\n\n### Resources\n\nThe server does not yet provide any resources.\n\n### Debugging the NPM package\n\nTo debug the server, use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) tool:\n\n```shell\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector npx -y @apify/actors-mcp-server\n```\n\n# ⚙️ Development\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/en) (v18 or higher)\n\nCreate an environment file, `.env`, with the following content:\n```text\nAPIFY_TOKEN=\"your-apify-token\"\n```\n\nBuild the `actor-mcp-server` package:\n\n```bash\nnpm run build\n```\n\n## Start HTTP streamable MCP server\n\nRun using Apify CLI:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nexport APIFY_META_ORIGIN=STANDBY\napify run -p\n```\n\nOnce the server is running, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to debug the server exposed at `http://localhost:3001`.\n\n## Start standard input/output (stdio) MCP server\n\nYou can launch the MCP Inspector with this command:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.\n\n## 🐦 Canary PR releases\n\nApify MCP is split across two repositories: this one for core MCP logic and the private `apify-mcp-server-internal` for the hosted server.\nChanges must be synchronized between both.\n\nTo create a canary release, add the `beta` tag to your PR branch.\nThis publishes the package to [pkg.pr.new](https://pkg.pr.new/) for staging and testing before merging.\nSee [the workflow file](.github/workflows/pre_release.yaml) for details.\n\n## 🐋 Docker Hub integration\nThe Apify MCP Server is also available on [Docker Hub](https://hub.docker.com/mcp/server/apify-mcp-server/overview), registered via the [mcp-registry](https://github.com/docker/mcp-registry) repository. The entry in `servers/apify-mcp-server/server.yaml` should be deployed automatically by the Docker Hub MCP registry (deployment frequency is unknown). **Before making major changes to the `stdio` server version, be sure to test it locally to ensure the Docker build passes.** To test, change the `source.branch` to your PR branch and run `task build -- apify-mcp-server`. For more details, see [CONTRIBUTING.md](https://github.com/docker/mcp-registry/blob/main/CONTRIBUTING.md).\n\n# 🐛 Troubleshooting (local MCP server)\n\n- Make sure you have `node` installed by running `node -v`.\n- Make sure the `APIFY_TOKEN` environment variable is set.\n- Always use the latest version of the MCP server by using `@apify/actors-mcp-server@latest`.\n\n## 💡 Limitations\n\nThe Actor input schema is processed to be compatible with most MCP clients while adhering to [JSON Schema](https://json-schema.org/) standards. The processing includes:\n- **Descriptions** are truncated to 500 characters (as defined in `MAX_DESCRIPTION_LENGTH`).\n- **Enum fields** are truncated to a maximum combined length of 2000 characters for all elements (as defined in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.\n- **Nested properties** are built for special cases like proxy configuration and request list sources to ensure the correct input structure.\n- **Array item types** are inferred when not explicitly defined in the schema, using a priority order: explicit type in items \u003e prefill type \u003e default value type \u003e editor type.\n- **Enum values and examples** are added to property descriptions to ensure visibility, even if the client doesn't fully support the JSON schema.\n- **Rental Actors** are only available for use with the hosted MCP server at https://mcp.apify.com. When running the server locally via stdio, you can only access Actors that are already added to your local toolset. To dynamically search for and use any Actor from the Apify Store—including rental Actors—connect to the hosted endpoint.\n\n# 🤝 Contributing\n\nWe welcome contributions to improve the Apify MCP Server! Here's how you can help:\n\n- **🐛 Report issues**: Find a bug or have a feature request? [Open an issue](https://github.com/apify/apify-mcp-server/issues).\n- **🔧 Submit pull requests**: Fork the repo and submit pull requests with enhancements or fixes.\n- **📚 Documentation**: Improvements to docs and examples are always welcome.\n- **💡 Share use cases**: Contribute examples to help other users.\n\nFor major changes, please open an issue first to discuss your proposal and ensure it aligns with the project's goals.\n\n# 📚 Learn more\n\n- [Model Context Protocol](https://modelcontextprotocol.org/)\n- [What are AI Agents?](https://blog.apify.com/what-are-ai-agents/)\n- [What is MCP and why does it matter?](https://blog.apify.com/what-is-model-context-protocol/)\n- [How to use MCP with Apify Actors](https://blog.apify.com/how-to-use-mcp/)\n- [Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- [Webinar: Building and Monetizing MCP Servers on Apify](https://www.youtube.com/watch?v=w3AH3jIrXXo)\n- [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)\n","stargazer_count":475,"topics":["agents","ai","mcp","mcp-server"],"uses_custom_opengraph_image":false}}}},{"name":"chroma-core/chroma-mcp","description":"Provides data retrieval capabilities powered by Chroma, enabling AI models to create collections over generated data and user inputs, and retrieve that data using vector search, full text search, metadata filtering, and more.","status":"active","repository":{"url":"https://github.com/chroma-core/chroma-mcp","source":"github","id":"930632525","readme":"\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://trychroma.com\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png\" alt=\"Chroma logo\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n    \u003cb\u003eChroma - the open-source embedding database\u003c/b\u003e. \u003cbr /\u003e\n    The fastest way to build Python or JavaScript LLM apps with memory!\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://discord.gg/MMeYNTmh3x\" target=\"_blank\"\u003e\n      \u003cimg src=\"https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600\" alt=\"Discord\"\u003e\n  \u003c/a\u003e |\n  \u003ca href=\"https://github.com/chroma-core/chroma/blob/master/LICENSE\" target=\"_blank\"\u003e\n      \u003cimg src=\"https://img.shields.io/static/v1?label=license\u0026message=Apache 2.0\u0026color=white\" alt=\"License\"\u003e\n  \u003c/a\u003e |\n  \u003ca href=\"https://docs.trychroma.com/\" target=\"_blank\"\u003e\n      Docs\n  \u003c/a\u003e |\n  \u003ca href=\"https://www.trychroma.com/\" target=\"_blank\"\u003e\n      Homepage\n  \u003c/a\u003e\n\u003c/p\u003e\n\n# Chroma MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@chroma-core/chroma-mcp)](https://smithery.ai/server/@chroma-core/chroma-mcp)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol designed for effortless integration between LLM applications and external data sources or tools, offering a standardized framework to seamlessly provide LLMs with the context they require.\n\nThis server provides data retrieval capabilities powered by Chroma, enabling AI models to create collections over generated data and user inputs, and retrieve that data using vector search, full text search, metadata filtering, and more.\n\nThis is a MCP server for self-hosting your access to Chroma. If you are looking for [Package Search](https://www.trychroma.com/package-search) you can find the repository for that [here](https://github.com/chroma-core/package-search).\n\n## Features\n\n- **Flexible Client Types**\n  - Ephemeral (in-memory) for testing and development\n  - Persistent for file-based storage\n  - HTTP client for self-hosted Chroma instances\n  - Cloud client for Chroma Cloud integration (automatically connects to api.trychroma.com)\n\n- **Collection Management**\n  - Create, modify, and delete collections\n  - List all collections with pagination support\n  - Get collection information and statistics\n  - Configure HNSW parameters for optimized vector search\n  - Select embedding functions when creating collections\n\n- **Document Operations**\n  - Add documents with optional metadata and custom IDs\n  - Query documents using semantic search\n  - Advanced filtering using metadata and document content\n  - Retrieve documents by IDs or filters\n  - Full text search capabilities\n\n### Supported Tools\n\n- `chroma_list_collections` - List all collections with pagination support\n- `chroma_create_collection` - Create a new collection with optional HNSW configuration\n- `chroma_peek_collection` - View a sample of documents in a collection\n- `chroma_get_collection_info` - Get detailed information about a collection\n- `chroma_get_collection_count` - Get the number of documents in a collection\n- `chroma_modify_collection` - Update a collection's name or metadata\n- `chroma_delete_collection` - Delete a collection\n- `chroma_add_documents` - Add documents with optional metadata and custom IDs\n- `chroma_query_documents` - Query documents using semantic search with advanced filtering\n- `chroma_get_documents` - Retrieve documents by IDs or filters with pagination\n- `chroma_update_documents` - Update existing documents' content, metadata, or embeddings\n- `chroma_delete_documents` - Delete specific documents from a collection\n\n### Embedding Functions\nChroma MCP supports several embedding functions: `default`, `cohere`, `openai`, `jina`, `voyageai`, and `roboflow`.\n\nThe embedding functions utilize Chroma's collection configuration, which persists the selected embedding function of a collection for retrieval. Once a collection is created using the collection configuration, on retrieval for future queries and inserts, the same embedding function will be used, without needing to specify the embedding function again. Embedding function persistance was added in v1.0.0 of Chroma, so if you created a collection using version \u003c=0.6.3, this feature is not supported.\n\nWhen accessing embedding functions that utilize external APIs, please be sure to add the environment variable for the API key with the correct format, found in [Embedding Function Environment Variables](#embedding-function-environment-variables)\n\n## Usage with Claude Desktop\n\n1. To add an ephemeral client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\"\n    ]\n}\n```\n\n2. To add a persistent client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"persistent\",\n        \"--data-dir\",\n        \"/full/path/to/your/data/directory\"\n    ]\n}\n```\n\nThis will create a persistent client that will use the data directory specified.\n\n3. To connect to Chroma Cloud, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"cloud\",\n        \"--tenant\",\n        \"your-tenant-id\",\n        \"--database\",\n        \"your-database-name\",\n        \"--api-key\",\n        \"your-api-key\"\n    ]\n}\n```\n\nThis will create a cloud client that automatically connects to api.trychroma.com using SSL.\n\n**Note:** Adding API keys in arguments is fine on local devices, but for safety, you can also specify a custom path for your environment configuration file using the `--dotenv-path` argument within the `args` list, for example: `\"args\": [\"chroma-mcp\", \"--dotenv-path\", \"/custom/path/.env\"]`.\n\n4. To connect to a [self-hosted Chroma instance on your own cloud provider](https://docs.trychroma.com/\nproduction/deployment), add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"chroma-mcp\", \n      \"--client-type\", \n      \"http\", \n      \"--host\", \n      \"your-host\", \n      \"--port\", \n      \"your-port\", \n      \"--custom-auth-credentials\",\n      \"your-custom-auth-credentials\",\n      \"--ssl\",\n      \"true\"\n    ]\n}\n```\n\nThis will create an HTTP client that connects to your self-hosted Chroma instance.\n\n### Demos\n\nFind reference usages, such as shared knowledge bases \u0026 adding memory to context windows in the [Chroma MCP Docs](https://docs.trychroma.com/integrations/frameworks/anthropic-mcp#using-chroma-with-claude)\n\n### Using Environment Variables\n\nYou can also use environment variables to configure the client. The server will automatically load variables from a `.env` file located at the path specified by `--dotenv-path` (defaults to `.chroma_env` in the working directory) or from system environment variables. Command-line arguments take precedence over environment variables.\n\n```bash\n# Common variables\nexport CHROMA_CLIENT_TYPE=\"http\"  # or \"cloud\", \"persistent\", \"ephemeral\"\n\n# For persistent client\nexport CHROMA_DATA_DIR=\"/full/path/to/your/data/directory\"\n\n# For cloud client (Chroma Cloud)\nexport CHROMA_TENANT=\"your-tenant-id\"\nexport CHROMA_DATABASE=\"your-database-name\"\nexport CHROMA_API_KEY=\"your-api-key\"\n\n# For HTTP client (self-hosted)\nexport CHROMA_HOST=\"your-host\"\nexport CHROMA_PORT=\"your-port\"\nexport CHROMA_CUSTOM_AUTH_CREDENTIALS=\"your-custom-auth-credentials\"\nexport CHROMA_SSL=\"true\"\n\n# Optional: Specify path to .env file (defaults to .chroma_env)\nexport CHROMA_DOTENV_PATH=\"/path/to/your/.env\" \n```\n\n#### Embedding Function Environment Variables\nWhen using external embedding functions that access an API key, follow the naming convention\n`CHROMA_\u003c\u003e_API_KEY=\"\u003ckey\u003e\"`.\nSo to set a Cohere API key, set the environment variable `CHROMA_COHERE_API_KEY=\"\"`. We recommend adding this to a .env file somewhere and using the `CHROMA_DOTENV_PATH` environment variable or `--dotenv-path` flag to set that location for safekeeping.\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:29Z","updated_at":"2025-10-22T11:21:43Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"pypi","identifier":"chroma-mcp","version":"v0.2.6","runtime_hint":"uvx","environment_variables":[{"value":"{chroma_client_type}","variables":{"chroma_client_type":{"description":"Client type: ephemeral (default), persistent, cloud, or http."}},"name":"CHROMA_CLIENT_TYPE"},{"value":"{chroma_dotenv_path}","variables":{"chroma_dotenv_path":{"description":"Optional path to .env (defaults to ./.chroma_env)."}},"name":"CHROMA_DOTENV_PATH"},{"value":"{chroma_data_dir}","variables":{"chroma_data_dir":{"description":"Data directory for persistent client."}},"name":"CHROMA_DATA_DIR"},{"value":"{chroma_tenant}","variables":{"chroma_tenant":{"description":"Chroma Cloud tenant ID."}},"name":"CHROMA_TENANT"},{"value":"{chroma_database}","variables":{"chroma_database":{"description":"Chroma Cloud database name."}},"name":"CHROMA_DATABASE"},{"value":"{chroma_api_key}","variables":{"chroma_api_key":{"description":"Chroma Cloud API key.","is_secret":true}},"name":"CHROMA_API_KEY"},{"value":"{chroma_host}","variables":{"chroma_host":{"description":"Self-hosted Chroma host."}},"name":"CHROMA_HOST"},{"value":"{chroma_port}","variables":{"chroma_port":{"description":"Self-hosted Chroma port."}},"name":"CHROMA_PORT"},{"value":"{chroma_custom_auth}","variables":{"chroma_custom_auth":{"description":"Custom auth credentials for self-hosted Chroma.","is_secret":true}},"name":"CHROMA_CUSTOM_AUTH_CREDENTIALS"},{"value":"{chroma_ssl}","variables":{"chroma_ssl":{"description":"Use SSL (true/false) for self-hosted HTTP."}},"name":"CHROMA_SSL"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"54222ba5-0bc5-428d-bdb0-5dbcf2574b83","is_latest":true,"published_at":"2025-09-09T10:55:22.908927Z","updated_at":"2025-09-09T10:55:22.908927Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Chroma","is_in_organization":true,"license":"Apache License 2.0","name":"chroma-mcp","name_with_owner":"chroma-core/chroma-mcp","opengraph_image_url":"https://opengraph.githubassets.com/83e0cdf54868e93de94a24ffd3e3e342fc15bfe41f3b4a482bce24ab3d4fa05d/chroma-core/chroma-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/105881770?v=4","preferred_image":"https://avatars.githubusercontent.com/u/105881770?v=4","primary_language":"Python","primary_language_color":"#3572A5","pushed_at":"2025-09-17T20:20:13Z","readme":"\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://trychroma.com\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png\" alt=\"Chroma logo\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n    \u003cb\u003eChroma - the open-source embedding database\u003c/b\u003e. \u003cbr /\u003e\n    The fastest way to build Python or JavaScript LLM apps with memory!\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://discord.gg/MMeYNTmh3x\" target=\"_blank\"\u003e\n      \u003cimg src=\"https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600\" alt=\"Discord\"\u003e\n  \u003c/a\u003e |\n  \u003ca href=\"https://github.com/chroma-core/chroma/blob/master/LICENSE\" target=\"_blank\"\u003e\n      \u003cimg src=\"https://img.shields.io/static/v1?label=license\u0026message=Apache 2.0\u0026color=white\" alt=\"License\"\u003e\n  \u003c/a\u003e |\n  \u003ca href=\"https://docs.trychroma.com/\" target=\"_blank\"\u003e\n      Docs\n  \u003c/a\u003e |\n  \u003ca href=\"https://www.trychroma.com/\" target=\"_blank\"\u003e\n      Homepage\n  \u003c/a\u003e\n\u003c/p\u003e\n\n# Chroma MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@chroma-core/chroma-mcp)](https://smithery.ai/server/@chroma-core/chroma-mcp)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol designed for effortless integration between LLM applications and external data sources or tools, offering a standardized framework to seamlessly provide LLMs with the context they require.\n\nThis server provides data retrieval capabilities powered by Chroma, enabling AI models to create collections over generated data and user inputs, and retrieve that data using vector search, full text search, metadata filtering, and more.\n\nThis is a MCP server for self-hosting your access to Chroma. If you are looking for [Package Search](https://www.trychroma.com/package-search) you can find the repository for that [here](https://github.com/chroma-core/package-search).\n\n## Features\n\n- **Flexible Client Types**\n  - Ephemeral (in-memory) for testing and development\n  - Persistent for file-based storage\n  - HTTP client for self-hosted Chroma instances\n  - Cloud client for Chroma Cloud integration (automatically connects to api.trychroma.com)\n\n- **Collection Management**\n  - Create, modify, and delete collections\n  - List all collections with pagination support\n  - Get collection information and statistics\n  - Configure HNSW parameters for optimized vector search\n  - Select embedding functions when creating collections\n\n- **Document Operations**\n  - Add documents with optional metadata and custom IDs\n  - Query documents using semantic search\n  - Advanced filtering using metadata and document content\n  - Retrieve documents by IDs or filters\n  - Full text search capabilities\n\n### Supported Tools\n\n- `chroma_list_collections` - List all collections with pagination support\n- `chroma_create_collection` - Create a new collection with optional HNSW configuration\n- `chroma_peek_collection` - View a sample of documents in a collection\n- `chroma_get_collection_info` - Get detailed information about a collection\n- `chroma_get_collection_count` - Get the number of documents in a collection\n- `chroma_modify_collection` - Update a collection's name or metadata\n- `chroma_delete_collection` - Delete a collection\n- `chroma_add_documents` - Add documents with optional metadata and custom IDs\n- `chroma_query_documents` - Query documents using semantic search with advanced filtering\n- `chroma_get_documents` - Retrieve documents by IDs or filters with pagination\n- `chroma_update_documents` - Update existing documents' content, metadata, or embeddings\n- `chroma_delete_documents` - Delete specific documents from a collection\n\n### Embedding Functions\nChroma MCP supports several embedding functions: `default`, `cohere`, `openai`, `jina`, `voyageai`, and `roboflow`.\n\nThe embedding functions utilize Chroma's collection configuration, which persists the selected embedding function of a collection for retrieval. Once a collection is created using the collection configuration, on retrieval for future queries and inserts, the same embedding function will be used, without needing to specify the embedding function again. Embedding function persistance was added in v1.0.0 of Chroma, so if you created a collection using version \u003c=0.6.3, this feature is not supported.\n\nWhen accessing embedding functions that utilize external APIs, please be sure to add the environment variable for the API key with the correct format, found in [Embedding Function Environment Variables](#embedding-function-environment-variables)\n\n## Usage with Claude Desktop\n\n1. To add an ephemeral client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\"\n    ]\n}\n```\n\n2. To add a persistent client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"persistent\",\n        \"--data-dir\",\n        \"/full/path/to/your/data/directory\"\n    ]\n}\n```\n\nThis will create a persistent client that will use the data directory specified.\n\n3. To connect to Chroma Cloud, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"cloud\",\n        \"--tenant\",\n        \"your-tenant-id\",\n        \"--database\",\n        \"your-database-name\",\n        \"--api-key\",\n        \"your-api-key\"\n    ]\n}\n```\n\nThis will create a cloud client that automatically connects to api.trychroma.com using SSL.\n\n**Note:** Adding API keys in arguments is fine on local devices, but for safety, you can also specify a custom path for your environment configuration file using the `--dotenv-path` argument within the `args` list, for example: `\"args\": [\"chroma-mcp\", \"--dotenv-path\", \"/custom/path/.env\"]`.\n\n4. To connect to a [self-hosted Chroma instance on your own cloud provider](https://docs.trychroma.com/\nproduction/deployment), add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"chroma-mcp\", \n      \"--client-type\", \n      \"http\", \n      \"--host\", \n      \"your-host\", \n      \"--port\", \n      \"your-port\", \n      \"--custom-auth-credentials\",\n      \"your-custom-auth-credentials\",\n      \"--ssl\",\n      \"true\"\n    ]\n}\n```\n\nThis will create an HTTP client that connects to your self-hosted Chroma instance.\n\n### Demos\n\nFind reference usages, such as shared knowledge bases \u0026 adding memory to context windows in the [Chroma MCP Docs](https://docs.trychroma.com/integrations/frameworks/anthropic-mcp#using-chroma-with-claude)\n\n### Using Environment Variables\n\nYou can also use environment variables to configure the client. The server will automatically load variables from a `.env` file located at the path specified by `--dotenv-path` (defaults to `.chroma_env` in the working directory) or from system environment variables. Command-line arguments take precedence over environment variables.\n\n```bash\n# Common variables\nexport CHROMA_CLIENT_TYPE=\"http\"  # or \"cloud\", \"persistent\", \"ephemeral\"\n\n# For persistent client\nexport CHROMA_DATA_DIR=\"/full/path/to/your/data/directory\"\n\n# For cloud client (Chroma Cloud)\nexport CHROMA_TENANT=\"your-tenant-id\"\nexport CHROMA_DATABASE=\"your-database-name\"\nexport CHROMA_API_KEY=\"your-api-key\"\n\n# For HTTP client (self-hosted)\nexport CHROMA_HOST=\"your-host\"\nexport CHROMA_PORT=\"your-port\"\nexport CHROMA_CUSTOM_AUTH_CREDENTIALS=\"your-custom-auth-credentials\"\nexport CHROMA_SSL=\"true\"\n\n# Optional: Specify path to .env file (defaults to .chroma_env)\nexport CHROMA_DOTENV_PATH=\"/path/to/your/.env\" \n```\n\n#### Embedding Function Environment Variables\nWhen using external embedding functions that access an API key, follow the naming convention\n`CHROMA_\u003c\u003e_API_KEY=\"\u003ckey\u003e\"`.\nSo to set a Cohere API key, set the environment variable `CHROMA_COHERE_API_KEY=\"\"`. We recommend adding this to a .env file somewhere and using the `CHROMA_DOTENV_PATH` environment variable or `--dotenv-path` flag to set that location for safekeeping.\n","stargazer_count":397,"uses_custom_opengraph_image":false}}}},{"name":"getsentry/sentry-mcp","description":"Retrieve and analyze application errors and performance issues from Sentry projects.","status":"active","repository":{"url":"https://github.com/getsentry/sentry-mcp","source":"github","id":"957245447","readme":"# sentry-mcp\n\n[![codecov](https://codecov.io/gh/getsentry/sentry-mcp/graph/badge.svg?token=khVKvJP5Ig)](https://codecov.io/gh/getsentry/sentry-mcp)\n\nSentry's MCP service is primarily designed for human-in-the-loop coding agents. Our tool selection and priorities are focused on developer workflows and debugging use cases, rather than providing a general-purpose MCP server for all Sentry functionality.\n\nThis remote MCP server acts as middleware to the upstream Sentry API, optimized for coding assistants like Cursor, Claude Code, and similar development tools. It's based on [Cloudflare's work towards remote MCPs](https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/).\n\n## Getting Started\n\nYou'll find everything you need to know by visiting the deployed service in production:\n\n\u003chttps://mcp.sentry.dev\u003e\n\nIf you're looking to contribute, learn how it works, or to run this for self-hosted Sentry, continue below.\n\n### Stdio vs Remote\n\nWhile this repository is focused on acting as an MCP service, we also support a `stdio` transport. This is still a work in progress, but is the easiest way to adapt run the MCP against a self-hosted Sentry install.\n\n**Note:** The AI-powered search tools (`search_events` and `search_issues`) require an OpenAI API key. These tools use natural language processing to translate queries into Sentry's query syntax. Without the API key, these specific tools will be unavailable, but all other tools will function normally.\n\nTo utilize the `stdio` transport, you'll need to create an User Auth Token in Sentry with the necessary scopes. As of writing this is:\n\n```\norg:read\nproject:read\nproject:write\nteam:read\nteam:write\nevent:write\n```\n\nLaunch the transport:\n\n```shell\nnpx @sentry/mcp-server@latest --access-token=sentry-user-token\n```\n\nNeed to connect to a self-hosted deployment? Add \u003ccode\u003e--host\u003c/code\u003e (hostname\nonly, e.g. \u003ccode\u003e--host=sentry.example.com\u003c/code\u003e) when you run the command.\n\nNote: You can also use environment variables:\n\n```shell\nSENTRY_ACCESS_TOKEN=\n# Optional overrides for self-hosted deployments\nSENTRY_HOST=\nOPENAI_API_KEY=  # Required for AI-powered search tools (search_events, search_issues)\n```\n\nIf you leave the host variable unset, the CLI automatically targets the Sentry\nSaaS service. Only set the override when you operate self-hosted Sentry.\n\n### MCP Inspector\n\nMCP includes an [Inspector](https://modelcontextprotocol.io/docs/tools/inspector), to easily test the service:\n\n```shell\npnpm inspector\n```\n\nEnter the MCP server URL (\u003chttp://localhost:5173\u003e) and hit connect. This should trigger the authentication flow for you.\n\nNote: If you have issues with your OAuth flow when accessing the inspector on `127.0.0.1`, try using `localhost` instead by visiting `http://localhost:6274`.\n\n## Local Development\n\nTo contribute changes, you'll need to set up your local environment:\n\n1. **Set up environment files:**\n\n   ```shell\n   make setup-env  # Creates both .env files from examples\n   ```\n\n2. **Create an OAuth App in Sentry** (Settings =\u003e API =\u003e [Applications](https://sentry.io/settings/account/api/applications/)):\n\n   - Homepage URL: `http://localhost:5173`\n   - Authorized Redirect URIs: `http://localhost:5173/oauth/callback`\n   - Note your Client ID and generate a Client secret\n\n3. **Configure your credentials:**\n\n   - Edit `.env` in the root directory and add your `OPENAI_API_KEY`\n   - Edit `packages/mcp-cloudflare/.env` and add:\n     - `SENTRY_CLIENT_ID=your_development_sentry_client_id`\n     - `SENTRY_CLIENT_SECRET=your_development_sentry_client_secret`\n     - `COOKIE_SECRET=my-super-secret-cookie`\n\n4. **Start the development server:**\n\n   ```shell\n   pnpm dev\n   ```\n\n### Verify\n\nRun the server locally to make it available at `http://localhost:5173`\n\n```shell\npnpm dev\n```\n\nTo test the local server, enter `http://localhost:5173/mcp` into Inspector and hit connect. Once you follow the prompts, you'll be able to \"List Tools\".\n\n### Tests\n\nThere are two test suites included: basic unit tests, and some evaluations.\n\nUnit tests can be run using:\n\n```shell\npnpm test\n```\n\nEvals will require a `.env` file in the project root with some config:\n\n```shell\n# .env (in project root)\nOPENAI_API_KEY=  # Also required for AI-powered search tools in production\n```\n\nNote: The root `.env` file provides defaults for all packages. Individual packages can have their own `.env` files to override these defaults during development.\n\nOnce that's done you can run them using:\n\n```shell\npnpm eval\n```\n\n## Development Notes\n\n### Automated Code Review\n\nThis repository uses automated code review tools (like Cursor BugBot) to help identify potential issues in pull requests. These tools provide helpful feedback and suggestions, but **we do not recommend making these checks required** as the accuracy is still evolving and can produce false positives.\n\nThe automated reviews should be treated as:\n\n- ✅ **Helpful suggestions** to consider during code review\n- ✅ **Starting points** for discussion and improvement\n- ❌ **Not blocking requirements** for merging PRs\n- ❌ **Not replacements** for human code review\n\nWhen addressing automated feedback, focus on the underlying concerns rather than strictly following every suggestion.\n\n### Contributor Documentation\n\nLooking to contribute or explore the full documentation map? See `CLAUDE.md` (also available as `AGENTS.md`) for contributor workflows and the complete docs index. The `docs/` folder contains the per-topic guides and tool-integrated `.mdc` files.\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:43Z","updated_at":"2025-10-22T11:21:00Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.sentry.dev/sse"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"29bf7a98-e581-45da-a327-1ae890f17464","is_latest":true,"published_at":"2025-09-09T10:55:22.907078Z","updated_at":"2025-09-09T10:55:22.907078Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Sentry","homepage_url":"https://mcp.sentry.dev","is_in_organization":true,"license":"Other","name":"sentry-mcp","name_with_owner":"getsentry/sentry-mcp","opengraph_image_url":"https://opengraph.githubassets.com/42a0771284877837da1d00a91aa800cec2ec145150c9c4e83ebb3fd3c37b8649/getsentry/sentry-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/1396951?v=4","preferred_image":"https://avatars.githubusercontent.com/u/1396951?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-22T07:46:20Z","readme":"# sentry-mcp\n\n[![codecov](https://codecov.io/gh/getsentry/sentry-mcp/graph/badge.svg?token=khVKvJP5Ig)](https://codecov.io/gh/getsentry/sentry-mcp)\n\nSentry's MCP service is primarily designed for human-in-the-loop coding agents. Our tool selection and priorities are focused on developer workflows and debugging use cases, rather than providing a general-purpose MCP server for all Sentry functionality.\n\nThis remote MCP server acts as middleware to the upstream Sentry API, optimized for coding assistants like Cursor, Claude Code, and similar development tools. It's based on [Cloudflare's work towards remote MCPs](https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/).\n\n## Getting Started\n\nYou'll find everything you need to know by visiting the deployed service in production:\n\n\u003chttps://mcp.sentry.dev\u003e\n\nIf you're looking to contribute, learn how it works, or to run this for self-hosted Sentry, continue below.\n\n### Stdio vs Remote\n\nWhile this repository is focused on acting as an MCP service, we also support a `stdio` transport. This is still a work in progress, but is the easiest way to adapt run the MCP against a self-hosted Sentry install.\n\n**Note:** The AI-powered search tools (`search_events` and `search_issues`) require an OpenAI API key. These tools use natural language processing to translate queries into Sentry's query syntax. Without the API key, these specific tools will be unavailable, but all other tools will function normally.\n\nTo utilize the `stdio` transport, you'll need to create an User Auth Token in Sentry with the necessary scopes. As of writing this is:\n\n```\norg:read\nproject:read\nproject:write\nteam:read\nteam:write\nevent:write\n```\n\nLaunch the transport:\n\n```shell\nnpx @sentry/mcp-server@latest --access-token=sentry-user-token\n```\n\nNeed to connect to a self-hosted deployment? Add \u003ccode\u003e--host\u003c/code\u003e (hostname\nonly, e.g. \u003ccode\u003e--host=sentry.example.com\u003c/code\u003e) when you run the command.\n\nNote: You can also use environment variables:\n\n```shell\nSENTRY_ACCESS_TOKEN=\n# Optional overrides for self-hosted deployments\nSENTRY_HOST=\nOPENAI_API_KEY=  # Required for AI-powered search tools (search_events, search_issues)\n```\n\nIf you leave the host variable unset, the CLI automatically targets the Sentry\nSaaS service. Only set the override when you operate self-hosted Sentry.\n\n### MCP Inspector\n\nMCP includes an [Inspector](https://modelcontextprotocol.io/docs/tools/inspector), to easily test the service:\n\n```shell\npnpm inspector\n```\n\nEnter the MCP server URL (\u003chttp://localhost:5173\u003e) and hit connect. This should trigger the authentication flow for you.\n\nNote: If you have issues with your OAuth flow when accessing the inspector on `127.0.0.1`, try using `localhost` instead by visiting `http://localhost:6274`.\n\n## Local Development\n\nTo contribute changes, you'll need to set up your local environment:\n\n1. **Set up environment files:**\n\n   ```shell\n   make setup-env  # Creates both .env files from examples\n   ```\n\n2. **Create an OAuth App in Sentry** (Settings =\u003e API =\u003e [Applications](https://sentry.io/settings/account/api/applications/)):\n\n   - Homepage URL: `http://localhost:5173`\n   - Authorized Redirect URIs: `http://localhost:5173/oauth/callback`\n   - Note your Client ID and generate a Client secret\n\n3. **Configure your credentials:**\n\n   - Edit `.env` in the root directory and add your `OPENAI_API_KEY`\n   - Edit `packages/mcp-cloudflare/.env` and add:\n     - `SENTRY_CLIENT_ID=your_development_sentry_client_id`\n     - `SENTRY_CLIENT_SECRET=your_development_sentry_client_secret`\n     - `COOKIE_SECRET=my-super-secret-cookie`\n\n4. **Start the development server:**\n\n   ```shell\n   pnpm dev\n   ```\n\n### Verify\n\nRun the server locally to make it available at `http://localhost:5173`\n\n```shell\npnpm dev\n```\n\nTo test the local server, enter `http://localhost:5173/mcp` into Inspector and hit connect. Once you follow the prompts, you'll be able to \"List Tools\".\n\n### Tests\n\nThere are two test suites included: basic unit tests, and some evaluations.\n\nUnit tests can be run using:\n\n```shell\npnpm test\n```\n\nEvals will require a `.env` file in the project root with some config:\n\n```shell\n# .env (in project root)\nOPENAI_API_KEY=  # Also required for AI-powered search tools in production\n```\n\nNote: The root `.env` file provides defaults for all packages. Individual packages can have their own `.env` files to override these defaults during development.\n\nOnce that's done you can run them using:\n\n```shell\npnpm eval\n```\n\n## Development Notes\n\n### Automated Code Review\n\nThis repository uses automated code review tools (like Cursor BugBot) to help identify potential issues in pull requests. These tools provide helpful feedback and suggestions, but **we do not recommend making these checks required** as the accuracy is still evolving and can produce false positives.\n\nThe automated reviews should be treated as:\n\n- ✅ **Helpful suggestions** to consider during code review\n- ✅ **Starting points** for discussion and improvement\n- ❌ **Not blocking requirements** for merging PRs\n- ❌ **Not replacements** for human code review\n\nWhen addressing automated feedback, focus on the underlying concerns rather than strictly following every suggestion.\n\n### Contributor Documentation\n\nLooking to contribute or explore the full documentation map? See `CLAUDE.md` (also available as `AGENTS.md`) for contributor workflows and the complete docs index. The `docs/` folder contains the per-topic guides and tool-integrated `.mdc` files.\n","stargazer_count":397,"topics":["mcp-server","tag-production"],"uses_custom_opengraph_image":false}}}},{"name":"mondaycom/mcp","description":"Enable AI agents to work reliably - giving them secure access to structured data, tools to take action, and the context needed to make smart decisions.","status":"active","repository":{"url":"https://github.com/mondaycom/mcp","source":"github","id":"961296260","readme":"\u003cdiv align=\"center\"\u003e\n\n# 🚀 monday.com MCP\n\n\u003cp\u003e\n  \u003ca href=\"https://npmjs.com/package/@mondaydotcomorg/monday-api-mcp\"\u003e\u003cimg src=\"https://img.shields.io/npm/v/@mondaydotcomorg/monday-api-mcp.svg?style=flat\" alt=\"npm version\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/mondaycom/mcp/blob/main/LICENSE\"\u003e\u003cimg src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/mondaycom/mcp\"\u003e\u003cimg src=\"https://img.shields.io/github/stars/mondaycom/mcp.svg?style=social\" alt=\"GitHub Stars\"\u003e\u003c/a\u003e\n  \u003cimg src=\"https://img.shields.io/badge/Node.js-v20+-green.svg\" alt=\"Node.js Version\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/MCP-Compatible-blueviolet\" alt=\"MCP Compatible\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/Claude-Ready-orange\" alt=\"Claude Ready\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/OpenAI-Compatible-lightgrey\" alt=\"OpenAI Compatible\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/TypeScript-Powered-blue\" alt=\"TypeScript\"\u003e\n\u003c/p\u003e\n\n**Enable AI agents to operate reliably within real workflows. This MCP is monday.com's open framework for connecting agents into your work OS - giving them secure access to structured data, tools to take action, and the context needed to make smart decisions.**\n\n\u003c/div\u003e\n\n## 🌟 Overview\n\nThis repository, maintained by the monday.com AI team, provides a comprehensive set of tools for AI agent developers who want to integrate with monday.com. Whether you're building AI assistants, automations, or custom integrations, our tools make it easy to connect to the monday.com platform.\n\n\u003chttps://github.com/user-attachments/assets/ed8d24e1-256b-4f6b-9d84-38e54a8703fd\u003e\n\n## 🔑 What is monday.com?\n\n[monday.com](https://monday.com) is a work operating system that powers teams to run processes, projects, and everyday work. Teams use monday.com to plan, track, and manage their work in one centralized platform. It provides a visual, intuitive interface where teams can:\n\n- Create and manage projects with customizable boards\n- Track tasks through different stages with status columns\n- Collaborate with team members through updates and mentions\n- Automate workflows and integrate with other tools\n- Visualize data with dashboards and reports\n\n## 📦 What's Inside\n\n### 💻 monday API MCP Server\n\nThe `@mondaydotcomorg/monday-api-mcp` package provides a plug-and-play server implementation for the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). It allows AI agents to interact with the monday.com API without needing to build complex integrations.\n\n### 🤖 Agent Toolkit\n\nThe `@mondaydotcomorg/agent-toolkit` package provides a powerful set of tools and utilities for building AI agents that interact with the monday.com API, supporting both OpenAI and Model Context Protocol (MCP) implementations.\n\n## 🏁 Complete Installation Guide\n\n### Step 1: Create a monday.com Account\n\nIf you don't already have a monday.com account:\n\n1. Go to [monday.com](https://monday.com) and sign up for an account\n2. Create your first workspace and board to get started\n\n### Step 2: Generate an API Token\n\nTo interact with monday.com's API, you'll need an API token:\n\n1. Log in to your monday.com account\n2. Click on your avatar in the bottom-left corner\n3. Select \"Developers\"\n4. Click \"My access tokens\" on the left menu\n5. Copy your personal access token\n\n### Step 3: Configure Your MCP Client\n\n#### For Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Settings → MCP Servers\n3. Add a new server with this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@mondaydotcomorg/monday-api-mcp\",\n        \"-t\",\n        \"your_monday_api_token\"\n      ]\n    }\n  }\n}\n```\n\n#### For Cursor or Other MCP Clients\n\nAdd to your settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@mondaydotcomorg/monday-api-mcp\",\n        \"-t\",\n        \"your_monday_api_token\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Step 5: Test Your Integration\n\n1. Ask Claude or your AI assistant a question like:\n   - \"What items do I have in board 123?\"\n   - \"Can you create a board to manage my project?\"\n\n2. Your assistant should now be able to interact with your monday.com account!\n\n## 🌩️ Using the Hosted MCP Service\n\n### Option 1: Using OAuth\n\nInstead of running the MCP server locally, you can use monday.com's hosted MCP service for a simpler setup.\n\n#### Step 1: Install the Monday MCP App\n\nBefore using the hosted service, you need to install the Monday MCP app from the marketplace:\n\n1. Visit [monday MCP app in the marketplace](https://monday.com/marketplace/listing/10000806/monday-mcp)\n2. Click \"Install\" and follow the instructions to add it to your account\n\n#### Step 2: Configure Your MCP Client for the Hosted Service\n\nAdd this configuration to your MCP client settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n      ],\n    }\n  }\n}\n```\n\n### Option 2: Using Authorization header\n\nTo specify an authorization header and API version:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted-dev\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-p\",\n        \"node@20\",\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n        \"--header\",\n        \"Authorization:${AUTH_HEADER}\",\n      ],\n      \"env\": {\n        \"AUTH_HEADER\": \"Bearer \u003cyour_token\u003e\",\n      }\n    }\n  }\n}\n```\n\n### Additional Configuration for Hosted MCP\n\nYou can specify the Api version you want to use using the **--header** param:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n        \"--header\",\n        \"Api-Version:${API_VERSION}\"\n      ],\n      \"env\": {\n        \"API_VERSION\": \"2025-07\"\n      }\n    }\n  }\n}\n```\n\n### Benefits of the Hosted Service\n\n- No need to manage your own server\n- Automatic updates with the latest features\n- Improved reliability and performance\n- Instead of adding the token yourself, our OAuth mechanism takes control of it\n- You can limit the mcp to work on specific workspaces\n\n## 🧰 Available Tools\n\nOur MCP server provides a rich set of tools that give AI assistants the ability to interact with monday.com:\n\n| Category | Tool | Description |\n|----------|------|-------------|\n| **Item Operations** | create_item | Create a new item in a monday.com board with specified column values |\n| | delete_item | Delete an item from a board permanently |\n| | get_board_items_by_name | Search for items by board ID and term/name |\n| | create_update | Add an update/comment to a specific item |\n| | change_item_column_values | Modify the column values of an existing item |\n| | move_item_to_group | Move an item to a different group within the same board |\n| **Board Operations** | create_board | Create a new monday.com board with specified columns |\n| | get_board_schema | Retrieve the structure of columns and groups for a board |\n| | create_group | Create a new group in a monday.com board |\n| | create_column | Add a new column to an existing board |\n| | delete_column | Remove a column from a board |\n| **Account Operations** | list_users_and_teams | Retrieve user or team's details by id, name or by searching the account |\n| **WorkForms Operations** | create_form | Create a new monday.com form |\n| | get_form | Get a form by its token |\n\n## 🔮 Dynamic API Tools (Beta)\n\nOur Dynamic API Tools feature represents a significant advancement in how AI agents can interact with monday.com. While our standard tools cover common operations, Dynamic API Tools unlock the **full potential** of the monday.com GraphQL API.\n\n### What are Dynamic API Tools?\n\nDynamic API Tools provide AI agents with complete, adaptable access to monday.com's entire API surface. This means your AI assistant can:\n\n1. **Access any API endpoint** - Not just the predefined operations we've built\n2. **Generate custom GraphQL queries** - Create exactly the query needed for any situation\n3. **Dynamically explore monday.com's schema** - Understand all available data types and their relationships\n\n### Key Dynamic API Tools\n\n| Tool | Description |\n|------|-------------|\n| all_monday_api | Generate and execute any GraphQL query or mutation dynamically |\n| get_graphql_schema | Fetch monday.com's GraphQL schema to understand available operations |\n| get_type_details | Retrieve detailed information about specific GraphQL types |\n\n### Unlocked Possibilities\n\nWith Dynamic API Tools, your AI assistants can:\n\n- **Create complex reports** spanning multiple boards, items, and data points\n- **Perform batch operations** across many items simultaneously\n- **Integrate deeply** with monday.com's advanced features like docs, workspaces, and activity logs\n- **Discover new capabilities** as monday.com adds features to their API\n\n### How to Enable\n\nDynamic API Tools are in beta and disabled by default. Enable them with:\n\n```bash\nnpx @mondaydotcomorg/monday-api-mcp -t your_token --enable-dynamic-api-tools true\n```\n\nYou can also use the 'only' mode to exclusively enable Dynamic API Tools:\n\n```bash\nnpx @mondaydotcomorg/monday-api-mcp -t your_token --enable-dynamic-api-tools only\n```\n\nWhen 'only' mode is enabled, the server will provide just the Dynamic API Tools, filtering out all other standard tools. This is useful for advanced users who want to work directly with the GraphQL API.\n\n\u003e ⚠️ **Note**: Dynamic API Tools require full API access and are not compatible with read-only mode.\n\n## 🖥️ MCP Server Configuration\n\n| Argument | Flags | Description | Required | Default |\n|----------|-------|-------------|----------|---------|\n| monday.com API Token | `--token`, `-t` | monday.com API token | Yes | - |\n| API Version | `--version`, `-v` | monday.com API version | No | `current` |\n| Read Only Mode | `--read-only`, `-ro` | Enable read-only mode | No | `false` |\n| Dynamic API Tools | `--enable-dynamic-api-tools`, `-edat` | Enable dynamic API tools | No | `false` |\n\n## 🔐 Authentication \u0026 Security\n\nThe server requires a monday.com API token to authenticate with the monday.com API. You can provide this token in two ways:\n\n1. Command line argument: `-t your_monday_api_token`\n2. Environment variable: `monday_token=your_monday_api_token`\n\n### Security Best Practices\n\n- **Never share your API token** in public repositories or discussions\n- Consider using **read-only mode** (`--read-only`) when you only need to retrieve data\n- **Regularly rotate** your API tokens for enhanced security\n\n## 📚 Example Use Cases\n\nHere are some examples of what you can build with our tools:\n\n### 1. AI Assistant for Project Management\n\n- Create and manage tasks in monday.com boards\n- Get updates on project status\n- Move items between groups as they progress\n\n### 2. Data Analysis \u0026 Reporting\n\n- Extract data from monday.com boards\n- Generate reports and insights\n- Create new boards for reporting\n\n## 🌐 Community \u0026 Support\n\n- **GitHub Issues**: For bug reports and feature requests\n- **Discussions**: For questions and community discussions\n- **[monday.com Developer Documentation](https://developer.monday.com/api-reference/docs)**: Learn more about the monday.com API\n\n## 📚 Documentation\n\n- [monday API MCP Documentation](./packages/monday-api-mcp/README.md)\n- [Agent Toolkit Documentation](./packages/agent-toolkit/README.md)\n- [monday.com API Reference](https://developer.monday.com/api-reference/docs)\n\n## 📋 Prerequisites\n\nBefore using these tools, make sure you have:\n\n1. Node.js v20 or higher installed\n2. NPM v5.2.0 or higher installed\n3. A [monday.com API token](https://developer.monday.com/api-reference/docs/authentication)\n\n## 🛠️ How to develop in the repo\n\nTo develop for the repo:\n\n1. Clone the repository\n2. Install dependencies: `yarn install`\n3. Build the project: `yarn build`\n4. Copy the path of the dist/index.js file in the of the `monday-api-mcp` package.\n5. Change the config to work locally\n\n```bash\n    \"monday-api-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"\u003cyour_full_path_to_the_package\u003e/dist/index.js\",\n        \"-t\",\n        \"123\",\n        \"--enable-dynamic-api-tools\",\n        \"true\"\n      ],\n      \"env\": {}\n    }\n```\n\n## 🤝 Contributing\n\nWe welcome contributions from the community! Whether it's fixing bugs, improving documentation, or adding new features, your help is appreciated.\n\n1. Fork the repository\n2. Create your feature branch: `git checkout -b feature/amazing-feature`\n3. Commit your changes: `git commit -m 'Add some amazing feature'`\n4. Push to the branch: `git push origin feature/amazing-feature`\n5. Open a Pull Request\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.\n\nIt is clarified that the server uses the monday.com API, which is subject to monday.com's [Developer Terms](https://monday.com/l/marketplace-developers/developer-terms/)\n\n---\n\n\u003cdiv align=\"center\"\u003e\n  \u003cp\u003eBuilt with ❤️ by the monday.com AI Team\u003c/p\u003e\n  \u003cp\u003e\n    \u003ca href=\"https://monday.com\"\u003emonday.com\u003c/a\u003e |\n    \u003ca href=\"https://developer.monday.com\"\u003eDeveloper Platform\u003c/a\u003e |\n    \u003ca href=\"https://github.com/mondaycom/mcp\"\u003eGitHub\u003c/a\u003e\n  \u003c/p\u003e\n\u003c/div\u003e\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:23Z","updated_at":"2025-10-22T11:21:37Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"@mondaydotcomorg/monday-api-mcp","version":"latest","runtime_hint":"npx","runtime_arguments":[{"is_required":true,"format":"string","value":"-y","type":"positional"}],"environment_variables":[{"value":"{monday_api_token}","variables":{"monday_api_token":{"description":"Your monday.com API token (can also be passed with -t/--token).","is_required":true,"is_secret":true}},"name":"monday_token"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"52673640-90be-4bc5-9899-87e0886cb9ff","is_latest":true,"published_at":"2025-08-22T00:00:00Z","updated_at":"2025-08-22T00:00:00Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Monday.com","homepage_url":"https://monday.com","is_in_organization":true,"license":"MIT License","name":"mcp","name_with_owner":"mondaycom/mcp","opengraph_image_url":"https://opengraph.githubassets.com/faaa627e1510be7447b9057f09ed01a715547118043ec4ca0b767ed4e35715d0/mondaycom/mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/61420283?v=4","preferred_image":"https://avatars.githubusercontent.com/u/61420283?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-22T11:09:38Z","readme":"\u003cdiv align=\"center\"\u003e\n\n# 🚀 monday.com MCP\n\n\u003cp\u003e\n  \u003ca href=\"https://npmjs.com/package/@mondaydotcomorg/monday-api-mcp\"\u003e\u003cimg src=\"https://img.shields.io/npm/v/@mondaydotcomorg/monday-api-mcp.svg?style=flat\" alt=\"npm version\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/mondaycom/mcp/blob/main/LICENSE\"\u003e\u003cimg src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"\u003e\u003c/a\u003e\n  \u003ca href=\"https://github.com/mondaycom/mcp\"\u003e\u003cimg src=\"https://img.shields.io/github/stars/mondaycom/mcp.svg?style=social\" alt=\"GitHub Stars\"\u003e\u003c/a\u003e\n  \u003cimg src=\"https://img.shields.io/badge/Node.js-v20+-green.svg\" alt=\"Node.js Version\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/MCP-Compatible-blueviolet\" alt=\"MCP Compatible\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/Claude-Ready-orange\" alt=\"Claude Ready\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/OpenAI-Compatible-lightgrey\" alt=\"OpenAI Compatible\"\u003e\n  \u003cimg src=\"https://img.shields.io/badge/TypeScript-Powered-blue\" alt=\"TypeScript\"\u003e\n\u003c/p\u003e\n\n**Enable AI agents to operate reliably within real workflows. This MCP is monday.com's open framework for connecting agents into your work OS - giving them secure access to structured data, tools to take action, and the context needed to make smart decisions.**\n\n\u003c/div\u003e\n\n## 🌟 Overview\n\nThis repository, maintained by the monday.com AI team, provides a comprehensive set of tools for AI agent developers who want to integrate with monday.com. Whether you're building AI assistants, automations, or custom integrations, our tools make it easy to connect to the monday.com platform.\n\n\u003chttps://github.com/user-attachments/assets/ed8d24e1-256b-4f6b-9d84-38e54a8703fd\u003e\n\n## 🔑 What is monday.com?\n\n[monday.com](https://monday.com) is a work operating system that powers teams to run processes, projects, and everyday work. Teams use monday.com to plan, track, and manage their work in one centralized platform. It provides a visual, intuitive interface where teams can:\n\n- Create and manage projects with customizable boards\n- Track tasks through different stages with status columns\n- Collaborate with team members through updates and mentions\n- Automate workflows and integrate with other tools\n- Visualize data with dashboards and reports\n\n## 📦 What's Inside\n\n### 💻 monday API MCP Server\n\nThe `@mondaydotcomorg/monday-api-mcp` package provides a plug-and-play server implementation for the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). It allows AI agents to interact with the monday.com API without needing to build complex integrations.\n\n### 🤖 Agent Toolkit\n\nThe `@mondaydotcomorg/agent-toolkit` package provides a powerful set of tools and utilities for building AI agents that interact with the monday.com API, supporting both OpenAI and Model Context Protocol (MCP) implementations.\n\n## 🏁 Complete Installation Guide\n\n### Step 1: Create a monday.com Account\n\nIf you don't already have a monday.com account:\n\n1. Go to [monday.com](https://monday.com) and sign up for an account\n2. Create your first workspace and board to get started\n\n### Step 2: Generate an API Token\n\nTo interact with monday.com's API, you'll need an API token:\n\n1. Log in to your monday.com account\n2. Click on your avatar in the bottom-left corner\n3. Select \"Developers\"\n4. Click \"My access tokens\" on the left menu\n5. Copy your personal access token\n\n### Step 3: Configure Your MCP Client\n\n#### For Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Settings → MCP Servers\n3. Add a new server with this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@mondaydotcomorg/monday-api-mcp\",\n        \"-t\",\n        \"your_monday_api_token\"\n      ]\n    }\n  }\n}\n```\n\n#### For Cursor or Other MCP Clients\n\nAdd to your settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@mondaydotcomorg/monday-api-mcp\",\n        \"-t\",\n        \"your_monday_api_token\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Step 5: Test Your Integration\n\n1. Ask Claude or your AI assistant a question like:\n   - \"What items do I have in board 123?\"\n   - \"Can you create a board to manage my project?\"\n\n2. Your assistant should now be able to interact with your monday.com account!\n\n## 🌩️ Using the Hosted MCP Service\n\n### Option 1: Using OAuth\n\nInstead of running the MCP server locally, you can use monday.com's hosted MCP service for a simpler setup.\n\n#### Step 1: Install the Monday MCP App\n\nBefore using the hosted service, you need to install the Monday MCP app from the marketplace:\n\n1. Visit [monday MCP app in the marketplace](https://monday.com/marketplace/listing/10000806/monday-mcp)\n2. Click \"Install\" and follow the instructions to add it to your account\n\n#### Step 2: Configure Your MCP Client for the Hosted Service\n\nAdd this configuration to your MCP client settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n      ],\n    }\n  }\n}\n```\n\n### Option 2: Using Authorization header\n\nTo specify an authorization header and API version:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted-dev\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-p\",\n        \"node@20\",\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n        \"--header\",\n        \"Authorization:${AUTH_HEADER}\",\n      ],\n      \"env\": {\n        \"AUTH_HEADER\": \"Bearer \u003cyour_token\u003e\",\n      }\n    }\n  }\n}\n```\n\n### Additional Configuration for Hosted MCP\n\nYou can specify the Api version you want to use using the **--header** param:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n        \"--header\",\n        \"Api-Version:${API_VERSION}\"\n      ],\n      \"env\": {\n        \"API_VERSION\": \"2025-07\"\n      }\n    }\n  }\n}\n```\n\n### Benefits of the Hosted Service\n\n- No need to manage your own server\n- Automatic updates with the latest features\n- Improved reliability and performance\n- Instead of adding the token yourself, our OAuth mechanism takes control of it\n- You can limit the mcp to work on specific workspaces\n\n## 🧰 Available Tools\n\nOur MCP server provides a rich set of tools that give AI assistants the ability to interact with monday.com:\n\n| Category | Tool | Description |\n|----------|------|-------------|\n| **Item Operations** | create_item | Create a new item in a monday.com board with specified column values |\n| | delete_item | Delete an item from a board permanently |\n| | get_board_items_by_name | Search for items by board ID and term/name |\n| | create_update | Add an update/comment to a specific item |\n| | change_item_column_values | Modify the column values of an existing item |\n| | move_item_to_group | Move an item to a different group within the same board |\n| **Board Operations** | create_board | Create a new monday.com board with specified columns |\n| | get_board_schema | Retrieve the structure of columns and groups for a board |\n| | create_group | Create a new group in a monday.com board |\n| | create_column | Add a new column to an existing board |\n| | delete_column | Remove a column from a board |\n| **Account Operations** | list_users_and_teams | Retrieve user or team's details by id, name or by searching the account |\n| **WorkForms Operations** | create_form | Create a new monday.com form |\n| | get_form | Get a form by its token |\n\n## 🔮 Dynamic API Tools (Beta)\n\nOur Dynamic API Tools feature represents a significant advancement in how AI agents can interact with monday.com. While our standard tools cover common operations, Dynamic API Tools unlock the **full potential** of the monday.com GraphQL API.\n\n### What are Dynamic API Tools?\n\nDynamic API Tools provide AI agents with complete, adaptable access to monday.com's entire API surface. This means your AI assistant can:\n\n1. **Access any API endpoint** - Not just the predefined operations we've built\n2. **Generate custom GraphQL queries** - Create exactly the query needed for any situation\n3. **Dynamically explore monday.com's schema** - Understand all available data types and their relationships\n\n### Key Dynamic API Tools\n\n| Tool | Description |\n|------|-------------|\n| all_monday_api | Generate and execute any GraphQL query or mutation dynamically |\n| get_graphql_schema | Fetch monday.com's GraphQL schema to understand available operations |\n| get_type_details | Retrieve detailed information about specific GraphQL types |\n\n### Unlocked Possibilities\n\nWith Dynamic API Tools, your AI assistants can:\n\n- **Create complex reports** spanning multiple boards, items, and data points\n- **Perform batch operations** across many items simultaneously\n- **Integrate deeply** with monday.com's advanced features like docs, workspaces, and activity logs\n- **Discover new capabilities** as monday.com adds features to their API\n\n### How to Enable\n\nDynamic API Tools are in beta and disabled by default. Enable them with:\n\n```bash\nnpx @mondaydotcomorg/monday-api-mcp -t your_token --enable-dynamic-api-tools true\n```\n\nYou can also use the 'only' mode to exclusively enable Dynamic API Tools:\n\n```bash\nnpx @mondaydotcomorg/monday-api-mcp -t your_token --enable-dynamic-api-tools only\n```\n\nWhen 'only' mode is enabled, the server will provide just the Dynamic API Tools, filtering out all other standard tools. This is useful for advanced users who want to work directly with the GraphQL API.\n\n\u003e ⚠️ **Note**: Dynamic API Tools require full API access and are not compatible with read-only mode.\n\n## 🖥️ MCP Server Configuration\n\n| Argument | Flags | Description | Required | Default |\n|----------|-------|-------------|----------|---------|\n| monday.com API Token | `--token`, `-t` | monday.com API token | Yes | - |\n| API Version | `--version`, `-v` | monday.com API version | No | `current` |\n| Read Only Mode | `--read-only`, `-ro` | Enable read-only mode | No | `false` |\n| Dynamic API Tools | `--enable-dynamic-api-tools`, `-edat` | Enable dynamic API tools | No | `false` |\n\n## 🔐 Authentication \u0026 Security\n\nThe server requires a monday.com API token to authenticate with the monday.com API. You can provide this token in two ways:\n\n1. Command line argument: `-t your_monday_api_token`\n2. Environment variable: `monday_token=your_monday_api_token`\n\n### Security Best Practices\n\n- **Never share your API token** in public repositories or discussions\n- Consider using **read-only mode** (`--read-only`) when you only need to retrieve data\n- **Regularly rotate** your API tokens for enhanced security\n\n## 📚 Example Use Cases\n\nHere are some examples of what you can build with our tools:\n\n### 1. AI Assistant for Project Management\n\n- Create and manage tasks in monday.com boards\n- Get updates on project status\n- Move items between groups as they progress\n\n### 2. Data Analysis \u0026 Reporting\n\n- Extract data from monday.com boards\n- Generate reports and insights\n- Create new boards for reporting\n\n## 🌐 Community \u0026 Support\n\n- **GitHub Issues**: For bug reports and feature requests\n- **Discussions**: For questions and community discussions\n- **[monday.com Developer Documentation](https://developer.monday.com/api-reference/docs)**: Learn more about the monday.com API\n\n## 📚 Documentation\n\n- [monday API MCP Documentation](./packages/monday-api-mcp/README.md)\n- [Agent Toolkit Documentation](./packages/agent-toolkit/README.md)\n- [monday.com API Reference](https://developer.monday.com/api-reference/docs)\n\n## 📋 Prerequisites\n\nBefore using these tools, make sure you have:\n\n1. Node.js v20 or higher installed\n2. NPM v5.2.0 or higher installed\n3. A [monday.com API token](https://developer.monday.com/api-reference/docs/authentication)\n\n## 🛠️ How to develop in the repo\n\nTo develop for the repo:\n\n1. Clone the repository\n2. Install dependencies: `yarn install`\n3. Build the project: `yarn build`\n4. Copy the path of the dist/index.js file in the of the `monday-api-mcp` package.\n5. Change the config to work locally\n\n```bash\n    \"monday-api-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"\u003cyour_full_path_to_the_package\u003e/dist/index.js\",\n        \"-t\",\n        \"123\",\n        \"--enable-dynamic-api-tools\",\n        \"true\"\n      ],\n      \"env\": {}\n    }\n```\n\n## 🤝 Contributing\n\nWe welcome contributions from the community! Whether it's fixing bugs, improving documentation, or adding new features, your help is appreciated.\n\n1. Fork the repository\n2. Create your feature branch: `git checkout -b feature/amazing-feature`\n3. Commit your changes: `git commit -m 'Add some amazing feature'`\n4. Push to the branch: `git push origin feature/amazing-feature`\n5. Open a Pull Request\n\n## 📄 License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.\n\nIt is clarified that the server uses the monday.com API, which is subject to monday.com's [Developer Terms](https://monday.com/l/marketplace-developers/developer-terms/)\n\n---\n\n\u003cdiv align=\"center\"\u003e\n  \u003cp\u003eBuilt with ❤️ by the monday.com AI Team\u003c/p\u003e\n  \u003cp\u003e\n    \u003ca href=\"https://monday.com\"\u003emonday.com\u003c/a\u003e |\n    \u003ca href=\"https://developer.monday.com\"\u003eDeveloper Platform\u003c/a\u003e |\n    \u003ca href=\"https://github.com/mondaycom/mcp\"\u003eGitHub\u003c/a\u003e\n  \u003c/p\u003e\n\u003c/div\u003e\n","stargazer_count":322,"topics":["agents","copilot","mcp","mcp-server","monday"],"uses_custom_opengraph_image":false}}}},{"name":"sunriseapps/imagesorcery-mcp","description":"Local image processing with computer vision capabilities including object detection, OCR, image editing, and transformations.","status":"active","repository":{"url":"https://github.com/sunriseapps/imagesorcery-mcp","source":"github","id":"982160482","readme":"# 🪄 ImageSorcery MCP\n**ComputerVision-based 🪄 sorcery of local image recognition and editing tools for AI assistants**\n\nOfficial website: [imagesorcery.net](https://imagesorcery.net?utm_source=readme)\n\n[![License](https://img.shields.io/badge/License-MIT-green)](https://opensource.org/licenses/MIT) [![MCP](https://img.shields.io/badge/Protocol-MCP-lightgrey)](https://github.com/microsoft/mcp)\n[![Claude](https://img.shields.io/badge/Works_with-Claude-orange)](https://claude.ai) [![Cursor](https://img.shields.io/badge/Works_with-Cursor-white)](https://cursor.so) [![Cline](https://img.shields.io/badge/Works_with-Cline-purple)](https://github.com/ClineLabs/cline)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/2620351a-15b1-4840-a93a-cbdbd23a6944) [![PyPI Downloads](https://static.pepy.tech/badge/imagesorcery-mcp)](https://pepy.tech/projects/imagesorcery-mcp)\n\n\u003ca href=\"https://glama.ai/mcp/servers/@sunriseapps/imagesorcery-mcp\"\u003e\n  \u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@sunriseapps/imagesorcery-mcp/badge\" /\u003e\n\u003c/a\u003e\n\n## ✅ With ImageSorcery MCP\n\n`🪄 ImageSorcery` empowers AI assistants with powerful image processing capabilities:\n\n- ✅ Crop, resize, and rotate images with precision\n- ✅ Remove background\n- ✅ Draw text and shapes on images\n- ✅ Add logos and watermarks\n- ✅ Detect objects using state-of-the-art models\n- ✅ Extract text from images with OCR\n- ✅ Use a wide range of pre-trained models for object detection, OCR, and more\n- ✅ Do all of this **locally**, without sending your images to any servers\n\nJust ask your AI to help with image tasks:\n\n\u003e \"copy photos with pets from folder `photos` to folder `pets`\"\n![Copying pets](https://i.imgur.com/wsaDWbf.gif)\n\n\u003e \"Find a cat at the photo.jpg and crop the image in a half in height and width to make the cat be centered\"\n![Centerizing cat](https://i.imgur.com/tD0O3l6.gif)\n😉 _**Hint:** Use full path to your files\"._\n\n\u003e \"Enumerate form fields on this `form.jpg` with `foduucom/web-form-ui-field-detection` model and fill the `form.md` with a list of described fields\"\n![Numerate form fields](https://i.imgur.com/1SNGfaP.gif)\n😉 _**Hint:** Specify the model and the confidence\"._\n\n😉 _**Hint:** Add \"use imagesorcery\" to make sure it will use the proper tool\"._\n\nYour tool will combine multiple tools listed below to achieve your goal.\n\n## 🛠️ Available Tools\n\n| Tool | Description | Example Prompt |\n|------|-------------|----------------|\n| `blur` | Blurs specified rectangular or polygonal areas of an image using OpenCV. Can also invert the provided areas e.g. to blur background. | \"Blur the area from (150, 100) to (250, 200) with a blur strength of 21 in my image 'test_image.png' and save it as 'output.png'\" |\n| `change_color` | Changes the color palette of an image | \"Convert my image 'test_image.png' to sepia and save it as 'output.png'\" |\n| `config` | View and update ImageSorcery MCP configuration settings | \"Show me the current configuration\" or \"Set the default detection confidence to 0.8\" |\n| `crop` | Crops an image using OpenCV's NumPy slicing approach | \"Crop my image 'input.png' from coordinates (10,10) to (200,200) and save it as 'cropped.png'\" |\n| `detect` | Detects objects in an image using models from Ultralytics. Can return segmentation masks (as PNG files) or polygons. | \"Detect objects in my image 'photo.jpg' with a confidence threshold of 0.4\" |\n| `draw_arrows` | Draws arrows on an image using OpenCV | \"Draw a red arrow from (50,50) to (150,100) on my image 'photo.jpg'\" |\n| `draw_circles` | Draws circles on an image using OpenCV | \"Draw a red circle with center (100,100) and radius 50 on my image 'photo.jpg'\" |\n| `draw_lines` | Draws lines on an image using OpenCV | \"Draw a red line from (50,50) to (150,100) on my image 'photo.jpg'\" |\n| `draw_rectangles` | Draws rectangles on an image using OpenCV | \"Draw a red rectangle from (50,50) to (150,100) and a filled blue rectangle from (200,150) to (300,250) on my image 'photo.jpg'\" |\n| `draw_texts` | Draws text on an image using OpenCV | \"Add text 'Hello World' at position (50,50) and 'Copyright 2023' at the bottom right corner of my image 'photo.jpg'\" |\n| `fill` | Fills specified rectangular, polygonal, or mask-based areas of an image with a color and opacity, or makes them transparent. Can also invert the provided areas e.g. to remove background. | \"Fill the area from (150, 100) to (250, 200) with semi-transparent red in my image 'test_image.png'\" |\n| `find` | Finds objects in an image based on a text description. Can return segmentation masks (as PNG files) or polygons. | \"Find all dogs in my image 'photo.jpg' with a confidence threshold of 0.4\" |\n| `get_metainfo` | Gets metadata information about an image file | \"Get metadata information about my image 'photo.jpg'\" |\n| `ocr` | Performs Optical Character Recognition (OCR) on an image using EasyOCR | \"Extract text from my image 'document.jpg' using OCR with English language\" |\n| `overlay` | Overlays one image on top of another, handling transparency | \"Overlay 'logo.png' on top of 'background.jpg' at position (10, 10)\" |\n| `resize` | Resizes an image using OpenCV | \"Resize my image 'photo.jpg' to 800x600 pixels and save it as 'resized_photo.jpg'\" |\n| `rotate` | Rotates an image using imutils.rotate_bound function | \"Rotate my image 'photo.jpg' by 45 degrees and save it as 'rotated_photo.jpg'\" |\n\n😉 _**Hint:** detailed information and usage instructions for each tool can be found in the tool's `/src/imagesorcery_mcp/tools/README.md`._\n\n## 📚 Available Resources\n\n| Resource URI | Description | Example Prompt |\n|--------------|-------------|----------------|\n| `models://list` | Lists all available models in the models directory | \"Which models are available in ImageSorcery?\" |\n\n😉 _**Hint:** detailed information and usage instructions for each resource can be found in the resource's `/src/imagesorcery_mcp/resources/README.md`._\n\n## 💬 Available Prompts\n\n| Prompt Name | Description | Example Usage |\n|-------------|-------------|---------------|\n| `remove-background` | Guides the AI through a comprehensive background removal workflow using object detection and masking tools | \"Use the remove-background prompt to remove the background from my photo 'portrait.jpg', keeping only the person\" |\n\n😉 _**Hint:** detailed information and usage instructions for each prompt can be found in the prompt's `/src/imagesorcery_mcp/prompts/README.md`._\n\n## 🚀 Getting Started\n\n### Requirements\n\n- `Python 3.10` or higher\n- `pipx` (recommended) - for easy installation and virtual environment management\n- `ffmpeg`, `libsm6`, `libxext6`, `libgl1-mesa-glx` - system libraries required by OpenCV\n- `Claude.app`, `Cline`, or another MCP client\n\n\u003cdetails\u003e\n\u003csummary\u003eVirtual environments (e.g. Docker)\u003c/summary\u003e\n\nThese dependencies are typically included with OpenCV installation and don't require separate installation. But they might be missing in some virtual environments like Docker.\n\n**For Ubuntu/Debian systems:**\n```bash\nsudo apt-get update \u0026\u0026 sudo apt-get install -y ffmpeg libsm6 libxext6 libgl1-mesa-glx\n```\n\n**For Docker containers:**\nAdd this line to your Dockerfile:\n```dockerfile\nRUN apt-get update \u0026\u0026 apt-get install -y ffmpeg libsm6 libxext6 libgl1-mesa-glx\n```\n\u003c/details\u003e\n\n### Installation\n\n![ImageSorcery Demo](https://i.imgur.com/pRfTlHV.gif)\n\u003cdetails\u003e\n\u003csummary\u003eFor Cline just copy-paste LLM-INSTALL.md\u003c/summary\u003e\n\n```md\n1.  Load MCP documentation if you have the tool for it. It will give you path to *MCP-configuration file* and other needed instructions.\n2.  Run command `pipx install imagesorcery-mcp`\n3.  Run command `imagesorcery-mcp --post-install`\n4.  Create or modify *MCP-configuration file*.\n5.  Add the `imagesorcery-mcp` server configuration to the `mcpServers` object in the *MCP-configuration file*\n    `\"imagesorcery-mcp\": {\"command\": \"imagesorcery-mcp\",\"timeout\": 100}`\n6.  Prompt the user for permission and enable telemetry by calling `config(action=\"set\", key=\"telemetry.enabled\", value=True, persist=True)`.\n7.  Attempt completion, indicating that the installation and configuration are complete.\n```\n\u003c/details\u003e\n\n#### Method 1: Using pipx (Recommended)\n\n`pipx` is the recommended way to install ImageSorcery MCP as it automatically handles virtual environment creation and management, making the installation process much simpler.\n\n\u003cdetails\u003e\n\u003csummary\u003e0.  Install pipx (if not already installed):\u003c/summary\u003e\n\n0.  **Install pipx (if not already installed):**\n    ```bash\n    # On macOS with Homebrew:\n    brew install pipx\n\n    # On Ubuntu/Debian:\n    sudo apt update \u0026\u0026 sudo apt install pipx\n\n    # On other systems with pip:\n    pip install --user pipx\n    pipx ensurepath\n    ```\n\u003c/details\u003e\n\n1.  **Install ImageSorcery MCP with pipx:**\n    ```bash\n    pipx install imagesorcery-mcp\n    ```\n\n2.  **Run the post-installation script:**\n    This step is crucial. It downloads the required models and attempts to install the `clip` Python package from GitHub.\n    ```bash\n    imagesorcery-mcp --post-install\n    ```\n\n#### Method 2: Manual Virtual Environment (Plan B)\n\n\u003cdetails\u003e\n\u003csummary\u003eIf pipx doesn't work for your system, you can manually create a virtual environment\u003c/summary\u003e\n\nFor reliable installation of all components, especially the `clip` package (installed via the post-install script), it is **strongly recommended to use Python's built-in `venv` module instead of `uv venv`**.\n\n1.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv imagesorcery-mcp\n    source imagesorcery-mcp/bin/activate  # For Linux/macOS\n    # source imagesorcery-mcp\\Scripts\\activate    # For Windows\n    ```\n\n2.  **Install the package into the activated virtual environment:**\n    You can use `pip` or `uv pip`.\n    ```bash\n    pip install imagesorcery-mcp\n    # OR, if you prefer using uv for installation into the venv:\n    # uv pip install imagesorcery-mcp\n    ```\n\n3.  **Run the post-installation script:**\n    This step is crucial. It downloads the required models and attempts to install the `clip` Python package from GitHub into the active virtual environment.\n    ```bash\n    imagesorcery-mcp --post-install\n    ```\n\n**Note:** When using this method, you'll need to provide the full path to the executable in your MCP client configuration (e.g., `/full/path/to/venv/bin/imagesorcery-mcp`).\n\u003c/details\u003e\n\n\n#### Additional Notes\n\u003cdetails\u003e\n\u003csummary\u003eWhat does the post-installation script do?\u003c/summary\u003e\nThe `imagesorcery-mcp --post-install` script performs the following actions:\n\n- **Creates a `config.toml` configuration file** in the current directory, allowing users to customize default tool parameters.\n- Creates a `models` directory (usually within the site-packages directory of your virtual environment, or a user-specific location if installed globally) to store pre-trained models.\n- Generates an initial `models/model_descriptions.json` file there.\n- Downloads default YOLO models (`yoloe-11l-seg-pf.pt`, `yoloe-11s-seg-pf.pt`, `yoloe-11l-seg.pt`, `yoloe-11s-seg.pt`) required by the `detect` tool into this `models` directory.\n- **Attempts to install the `clip` Python package** from Ultralytics' GitHub repository directly into the active Python environment. This is required for text prompt functionality in the `find` tool.\n- Downloads the CLIP model file required by the `find` tool into the `models` directory.\n\nYou can run this process anytime to restore the default models and attempt `clip` installation.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eImportant Notes for `uv` users (\u003ccode\u003euv venv\u003c/code\u003e and \u003ccode\u003euvx\u003c/code\u003e)\u003c/summary\u003e\n\n-   **Using `uv venv` to create virtual environments:**\n    Based on testing, virtual environments created with `uv venv` may not include `pip` in a way that allows the `imagesorcery-mcp --post-install` script to automatically install the `clip` package from GitHub (it might result in a \"No module named pip\" error during the `clip` installation step).\n    **If you choose to use `uv venv`:**\n    1.  Create and activate your `uv venv`.\n    2.  Install `imagesorcery-mcp`: `uv pip install imagesorcery-mcp`.\n    3.  Manually install the `clip` package into your active `uv venv`:\n        ```bash\n        uv pip install git+https://github.com/ultralytics/CLIP.git\n        ```\n    3.  Run `imagesorcery-mcp --post-install`. This will download models but may fail to install the `clip` Python package.\n    For a smoother automated `clip` installation via the post-install script, using `python -m venv` (as described in step 1 above) is the recommended method for creating the virtual environment.\n\n-   **Using `uvx imagesorcery-mcp --post-install`:**\n    Running the post-installation script directly with `uvx` (e.g., `uvx imagesorcery-mcp --post-install`) will likely fail to install the `clip` Python package. This is because the temporary environment created by `uvx` typically does not have `pip` available in a way the script can use. Models will be downloaded, but the `clip` package won't be installed by this command.\n    If you intend to use `uvx` to run the main `imagesorcery-mcp` server and require `clip` functionality, you'll need to ensure the `clip` package is installed in an accessible Python environment that `uvx` can find, or consider installing `imagesorcery-mcp` into a persistent environment created with `python -m venv`.\n\u003c/details\u003e\n\n## ⚙️ Configure MCP client\n\nAdd to your MCP client these settings.\n\n**For pipx installation (recommended):**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"imagesorcery-mcp\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\n**For manual venv installation:**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"/full/path/to/venv/bin/imagesorcery-mcp\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003cdetails\u003e\n\u003csummary\u003eIf you're using the server in HTTP mode, configure your client to connect to the HTTP endpoint:\u003c/summary\u003e\n\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"url\": \"http://127.0.0.1:8000/mcp\", // Use your custom host, port, and path if specified\n      \"transportType\": \"http\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eFor Windows\u003c/summary\u003e\n\n**For pipx installation (recommended):**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"imagesorcery-mcp.exe\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\n**For manual venv installation:**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"C:\\\\full\\\\path\\\\to\\\\venv\\\\Scripts\\\\imagesorcery-mcp.exe\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003c/details\u003e\n\n## 📦 Additional Models\n\nSome tools require specific models to be available in the `models` directory:\n\n```bash\n# Download models for the detect tool\ndownload-yolo-models --ultralytics yoloe-11l-seg\ndownload-yolo-models --huggingface ultralytics/yolov8:yolov8m.pt\n```\n\n\u003cdetails\u003e\n\u003csummary\u003eAbout Model Descriptions\u003c/summary\u003e\n\nWhen downloading models, the script automatically updates the `models/model_descriptions.json` file:\n\n- For Ultralytics models: Descriptions are predefined in `src/imagesorcery_mcp/scripts/create_model_descriptions.py` and include detailed information about each model's purpose, size, and characteristics.\n\n- For Hugging Face models: Descriptions are automatically extracted from the model card on Hugging Face Hub. The script attempts to use the model name from the model index or the first line of the description.\n\nAfter downloading models, it's recommended to check the descriptions in `models/model_descriptions.json` and adjust them if needed to provide more accurate or detailed information about the models' capabilities and use cases.\n\u003c/details\u003e\n\n### Running the Server\n\nImageSorcery MCP server can be run in different modes:\n- `STDIO` - default\n- `Streamable HTTP` - for web-based deployments\n- `Server-Sent Events (SSE)` - for web-based deployments that rely on SSE\n\n\u003cdetails\u003e\n\u003csummary\u003eAbout different modes:\u003c/summary\u003e\n\n1. **STDIO Mode (Default)** - This is the standard mode for local MCP clients:\n   ```bash\n   imagesorcery-mcp\n   ```\n\n2. **Streamable HTTP Mode** - For web-based deployments:\n   ```bash\n   imagesorcery-mcp --transport=streamable-http\n   ```\n   \n   With custom host, port, and path:\n   ```bash\n   imagesorcery-mcp --transport=streamable-http --host=0.0.0.0 --port=4200 --path=/custom-path\n   ```\n\nAvailable transport options:\n- `--transport`: Choose between \"stdio\" (default), \"streamable-http\", or \"sse\"\n- `--host`: Specify host for HTTP-based transports (default: 127.0.0.1)\n- `--port`: Specify port for HTTP-based transports (default: 8000)\n- `--path`: Specify endpoint path for HTTP-based transports (default: /mcp)\n\u003c/details\u003e\n\n## 🔒 Privacy \u0026 Telemetry\n\nWe are committed to your privacy. ImageSorcery MCP is designed to run locally, ensuring your images and data stay on your machine.\n\nTo help us understand which features are most popular and fix bugs faster, we've included optional, anonymous telemetry.\n\n-   **It is disabled by default.** You must explicitly opt-in to enable it.\n-   **What we collect:** Anonymized usage data, including features used (e.g., `crop`, `detect`), application version, operating system type (e.g., 'linux', 'win32'), and tool failures.\n-   **What we NEVER collect:** We do not collect any personal or sensitive information. This includes image data, file paths, IP addresses, or any other personally identifiable information.\n-   **How to enable/disable:** You can control telemetry by setting `enabled = true` or `enabled = false` in the `[telemetry]` section of your `config.toml` file.\n\n## ⚙️ Configuring the Server\n\nThe server can be configured using a `config.toml` file in the current directory. The file is created automatically during installation with default values. You can customize the default tool parameters in this file. More in [CONFIG.md](CONFIG.md).\n\n## 🤝 Contributing\n\u003cdetails\u003e\n\u003csummary\u003eWhether you're a 👤 human or an 🤖 AI agent, we welcome your contributions to this project!\u003c/summary\u003e\n\n### Directory Structure\n\nThis repository is organized as follows:\n\n```\n.\n├── .gitignore                 # Specifies intentionally untracked files that Git should ignore.\n├── pyproject.toml             # Configuration file for Python projects, including build system, dependencies, and tool settings.\n├── pytest.ini                 # Configuration file for the pytest testing framework.\n├── README.md                  # The main documentation file for the project.\n├── setup.sh                   # A shell script for quick setup (legacy, for reference or local use).\n├── models/                    # This directory stores pre-trained models used by tools like `detect` and `find`. It is typically ignored by Git due to the large file sizes.\n│   ├── model_descriptions.json  # Contains descriptions of the available models.\n│   ├── settings.json            # Contains settings related to model management and training runs.\n│   └── *.pt                     # Pre-trained model.\n├── src/                       # Contains the source code for the 🪄 ImageSorcery MCP server.\n│   └── imagesorcery_mcp/       # The main package directory for the server.\n│       ├── README.md            # High-level overview of the core architecture (server and middleware).\n│       ├── __init__.py          # Makes `imagesorcery_mcp` a Python package.\n│       ├── __main__.py          # Entry point for running the package as a script.\n│       ├── logging_config.py    # Configures the logging for the server.\n│       ├── server.py            # The main server file, responsible for initializing FastMCP and registering tools.\n│       ├── middleware.py        # Custom middleware for improved validation error handling.\n│       ├── logs/                # Directory for storing server logs.\n│       ├── scripts/             # Contains utility scripts for model management.\n│       │   ├── README.md        # Documentation for the scripts.\n│       │   ├── __init__.py      # Makes `scripts` a Python package.\n│       │   ├── create_model_descriptions.py # Script to generate model descriptions.\n│       │   ├── download_clip.py # Script to download CLIP models.\n│       │   ├── post_install.py  # Script to run post-installation tasks.\n│       │   └── download_models.py # Script to download other models (e.g., YOLO).\n│       ├── tools/               # Contains the implementation of individual MCP tools.\n│       │   ├── README.md        # Documentation for the tools.\n│       │   ├── __init__.py      # Makes `tools` a Python package.\n│       │   └── *.py           # Implements the tool.\n│       ├── prompts/             # Contains the implementation of individual MCP prompts.\n│       │   ├── README.md        # Documentation for the prompts.\n│       │   ├── __init__.py      # Makes `prompts` a Python package.\n│       │   └── *.py           # Implements the prompt.\n│       └── resources/           # Contains the implementation of individual MCP resources.\n│           ├── README.md        # Documentation for the resources.\n│           ├── __init__.py      # Makes `resources` a Python package.\n│           └── *.py           # Implements the resource.\n└── tests/                     # Contains test files for the project.\n    ├── test_server.py         # Tests for the main server functionality.\n    ├── data/                  # Contains test data, likely image files used in tests.\n    ├── tools/                 # Contains tests for individual tools.\n    ├── prompts/               # Contains tests for individual prompts.\n    └── resources/             # Contains tests for individual resources.\n```\n\n### Development Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/sunriseapps/imagesorcery-mcp.git # Or your fork\ncd imagesorcery-mcp\n```\n\n2. (Recommended) Create and activate a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate # For Linux/macOS\n# venv\\Scripts\\activate    # For Windows\n```\n\n3. Install the package in editable mode along with development dependencies:\n```bash\npip install -e \".[dev]\"\n```\nThis will install `imagesorcery-mcp` and all dependencies from `[project.dependencies]` and `[project.optional-dependencies].dev` (including `build` and `twine`).\n\n### Rules\n\nThese rules apply to all contributors: humans and AI.\n\n0. Read all the `README.md` files in the project. Understand the project structure and purpose. Understand the guidelines for contributing. Think through how it relates to your task, and how to make changes accordingly.\n1. Read `pyproject.toml`.\nPay attention to sections: `[tool.ruff]`, `[tool.ruff.lint]`, `[project.optional-dependencies]` and `[project]dependencies`.\nStrictly follow code style defined in `pyproject.toml`.\nStick to the stack defined in `pyproject.toml` dependencies and do not add any new dependencies without a good reason.\n2. Write your code in new and existing files.\nIf new dependencies are needed, update `pyproject.toml` and install them via `pip install -e .` or `pip install -e \".[dev]\"`. Do not install them directly via `pip install`.\nCheck out existing source codes for examples (e.g. `src/imagesorcery_mcp/server.py`, `src/imagesorcery_mcp/tools/crop.py`). Stick to the code style, naming conventions, input and output data formats, code structure, architecture, etc. of the existing code.\n3. Update related `README.md` files with your changes.\nStick to the format and structure of the existing `README.md` files.\n4. Write tests for your code.\nCheck out existing tests for examples (e.g. `tests/test_server.py`, `tests/tools/test_crop.py`).\nStick to the code style, naming conventions, input and output data formats, code structure, architecture, etc. of the existing tests.\n\n5. Run tests and linter to ensure everything works:\n```bash\npytest\nruff check .\n```\nIn case of failures - fix the code and tests. It is **strictly required** to have all new code to comply with the linter rules and pass all tests.\n\n\n### Coding hints\n- Use type hints where appropriate\n- Use pydantic for data validation and serialization\n\u003c/details\u003e\n\n## 📝 Questions?\n\nIf you have any questions, issues, or suggestions regarding this project, feel free to reach out to:\n\n- Project Author: [titulus](https://www.linkedin.com/in/titulus/) via LinkedIn\n- Sunrise Apps CEO: [Vlad Karm](https://www.linkedin.com/in/vladkarm/) via LinkedIn\n\nYou can also open an issue in the repository for bug reports or feature requests.\n\n## 📜 License\n\nThis project is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:46Z","updated_at":"2025-10-22T11:21:03Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"imagesorcery-mcp","identifier":"imagesorcery-mcp","version":"0.11.4","runtime_hint":"imagesorcery-mcp"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"78ed6aa5-3037-4adb-9bda-8dcda12aa9a5","is_latest":true,"published_at":"2025-09-09T10:55:22.907123Z","updated_at":"2025-09-09T10:55:22.907123Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Imagesorcery","homepage_url":"https://imagesorcery.net","is_in_organization":true,"license":"MIT License","name":"imagesorcery-mcp","name_with_owner":"sunriseapps/imagesorcery-mcp","opengraph_image_url":"https://opengraph.githubassets.com/e6ec2855214da077b3d12c49a0c303e959527bfd1ae632fa470fd070e7e55565/sunriseapps/imagesorcery-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/211119887?v=4","preferred_image":"https://avatars.githubusercontent.com/u/211119887?v=4","primary_language":"Python","primary_language_color":"#3572A5","pushed_at":"2025-09-23T21:24:06Z","readme":"# 🪄 ImageSorcery MCP\n**ComputerVision-based 🪄 sorcery of local image recognition and editing tools for AI assistants**\n\nOfficial website: [imagesorcery.net](https://imagesorcery.net?utm_source=readme)\n\n[![License](https://img.shields.io/badge/License-MIT-green)](https://opensource.org/licenses/MIT) [![MCP](https://img.shields.io/badge/Protocol-MCP-lightgrey)](https://github.com/microsoft/mcp)\n[![Claude](https://img.shields.io/badge/Works_with-Claude-orange)](https://claude.ai) [![Cursor](https://img.shields.io/badge/Works_with-Cursor-white)](https://cursor.so) [![Cline](https://img.shields.io/badge/Works_with-Cline-purple)](https://github.com/ClineLabs/cline)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/2620351a-15b1-4840-a93a-cbdbd23a6944) [![PyPI Downloads](https://static.pepy.tech/badge/imagesorcery-mcp)](https://pepy.tech/projects/imagesorcery-mcp)\n\n\u003ca href=\"https://glama.ai/mcp/servers/@sunriseapps/imagesorcery-mcp\"\u003e\n  \u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@sunriseapps/imagesorcery-mcp/badge\" /\u003e\n\u003c/a\u003e\n\n## ✅ With ImageSorcery MCP\n\n`🪄 ImageSorcery` empowers AI assistants with powerful image processing capabilities:\n\n- ✅ Crop, resize, and rotate images with precision\n- ✅ Remove background\n- ✅ Draw text and shapes on images\n- ✅ Add logos and watermarks\n- ✅ Detect objects using state-of-the-art models\n- ✅ Extract text from images with OCR\n- ✅ Use a wide range of pre-trained models for object detection, OCR, and more\n- ✅ Do all of this **locally**, without sending your images to any servers\n\nJust ask your AI to help with image tasks:\n\n\u003e \"copy photos with pets from folder `photos` to folder `pets`\"\n![Copying pets](https://i.imgur.com/wsaDWbf.gif)\n\n\u003e \"Find a cat at the photo.jpg and crop the image in a half in height and width to make the cat be centered\"\n![Centerizing cat](https://i.imgur.com/tD0O3l6.gif)\n😉 _**Hint:** Use full path to your files\"._\n\n\u003e \"Enumerate form fields on this `form.jpg` with `foduucom/web-form-ui-field-detection` model and fill the `form.md` with a list of described fields\"\n![Numerate form fields](https://i.imgur.com/1SNGfaP.gif)\n😉 _**Hint:** Specify the model and the confidence\"._\n\n😉 _**Hint:** Add \"use imagesorcery\" to make sure it will use the proper tool\"._\n\nYour tool will combine multiple tools listed below to achieve your goal.\n\n## 🛠️ Available Tools\n\n| Tool | Description | Example Prompt |\n|------|-------------|----------------|\n| `blur` | Blurs specified rectangular or polygonal areas of an image using OpenCV. Can also invert the provided areas e.g. to blur background. | \"Blur the area from (150, 100) to (250, 200) with a blur strength of 21 in my image 'test_image.png' and save it as 'output.png'\" |\n| `change_color` | Changes the color palette of an image | \"Convert my image 'test_image.png' to sepia and save it as 'output.png'\" |\n| `config` | View and update ImageSorcery MCP configuration settings | \"Show me the current configuration\" or \"Set the default detection confidence to 0.8\" |\n| `crop` | Crops an image using OpenCV's NumPy slicing approach | \"Crop my image 'input.png' from coordinates (10,10) to (200,200) and save it as 'cropped.png'\" |\n| `detect` | Detects objects in an image using models from Ultralytics. Can return segmentation masks (as PNG files) or polygons. | \"Detect objects in my image 'photo.jpg' with a confidence threshold of 0.4\" |\n| `draw_arrows` | Draws arrows on an image using OpenCV | \"Draw a red arrow from (50,50) to (150,100) on my image 'photo.jpg'\" |\n| `draw_circles` | Draws circles on an image using OpenCV | \"Draw a red circle with center (100,100) and radius 50 on my image 'photo.jpg'\" |\n| `draw_lines` | Draws lines on an image using OpenCV | \"Draw a red line from (50,50) to (150,100) on my image 'photo.jpg'\" |\n| `draw_rectangles` | Draws rectangles on an image using OpenCV | \"Draw a red rectangle from (50,50) to (150,100) and a filled blue rectangle from (200,150) to (300,250) on my image 'photo.jpg'\" |\n| `draw_texts` | Draws text on an image using OpenCV | \"Add text 'Hello World' at position (50,50) and 'Copyright 2023' at the bottom right corner of my image 'photo.jpg'\" |\n| `fill` | Fills specified rectangular, polygonal, or mask-based areas of an image with a color and opacity, or makes them transparent. Can also invert the provided areas e.g. to remove background. | \"Fill the area from (150, 100) to (250, 200) with semi-transparent red in my image 'test_image.png'\" |\n| `find` | Finds objects in an image based on a text description. Can return segmentation masks (as PNG files) or polygons. | \"Find all dogs in my image 'photo.jpg' with a confidence threshold of 0.4\" |\n| `get_metainfo` | Gets metadata information about an image file | \"Get metadata information about my image 'photo.jpg'\" |\n| `ocr` | Performs Optical Character Recognition (OCR) on an image using EasyOCR | \"Extract text from my image 'document.jpg' using OCR with English language\" |\n| `overlay` | Overlays one image on top of another, handling transparency | \"Overlay 'logo.png' on top of 'background.jpg' at position (10, 10)\" |\n| `resize` | Resizes an image using OpenCV | \"Resize my image 'photo.jpg' to 800x600 pixels and save it as 'resized_photo.jpg'\" |\n| `rotate` | Rotates an image using imutils.rotate_bound function | \"Rotate my image 'photo.jpg' by 45 degrees and save it as 'rotated_photo.jpg'\" |\n\n😉 _**Hint:** detailed information and usage instructions for each tool can be found in the tool's `/src/imagesorcery_mcp/tools/README.md`._\n\n## 📚 Available Resources\n\n| Resource URI | Description | Example Prompt |\n|--------------|-------------|----------------|\n| `models://list` | Lists all available models in the models directory | \"Which models are available in ImageSorcery?\" |\n\n😉 _**Hint:** detailed information and usage instructions for each resource can be found in the resource's `/src/imagesorcery_mcp/resources/README.md`._\n\n## 💬 Available Prompts\n\n| Prompt Name | Description | Example Usage |\n|-------------|-------------|---------------|\n| `remove-background` | Guides the AI through a comprehensive background removal workflow using object detection and masking tools | \"Use the remove-background prompt to remove the background from my photo 'portrait.jpg', keeping only the person\" |\n\n😉 _**Hint:** detailed information and usage instructions for each prompt can be found in the prompt's `/src/imagesorcery_mcp/prompts/README.md`._\n\n## 🚀 Getting Started\n\n### Requirements\n\n- `Python 3.10` or higher\n- `pipx` (recommended) - for easy installation and virtual environment management\n- `ffmpeg`, `libsm6`, `libxext6`, `libgl1-mesa-glx` - system libraries required by OpenCV\n- `Claude.app`, `Cline`, or another MCP client\n\n\u003cdetails\u003e\n\u003csummary\u003eVirtual environments (e.g. Docker)\u003c/summary\u003e\n\nThese dependencies are typically included with OpenCV installation and don't require separate installation. But they might be missing in some virtual environments like Docker.\n\n**For Ubuntu/Debian systems:**\n```bash\nsudo apt-get update \u0026\u0026 sudo apt-get install -y ffmpeg libsm6 libxext6 libgl1-mesa-glx\n```\n\n**For Docker containers:**\nAdd this line to your Dockerfile:\n```dockerfile\nRUN apt-get update \u0026\u0026 apt-get install -y ffmpeg libsm6 libxext6 libgl1-mesa-glx\n```\n\u003c/details\u003e\n\n### Installation\n\n![ImageSorcery Demo](https://i.imgur.com/pRfTlHV.gif)\n\u003cdetails\u003e\n\u003csummary\u003eFor Cline just copy-paste LLM-INSTALL.md\u003c/summary\u003e\n\n```md\n1.  Load MCP documentation if you have the tool for it. It will give you path to *MCP-configuration file* and other needed instructions.\n2.  Run command `pipx install imagesorcery-mcp`\n3.  Run command `imagesorcery-mcp --post-install`\n4.  Create or modify *MCP-configuration file*.\n5.  Add the `imagesorcery-mcp` server configuration to the `mcpServers` object in the *MCP-configuration file*\n    `\"imagesorcery-mcp\": {\"command\": \"imagesorcery-mcp\",\"timeout\": 100}`\n6.  Prompt the user for permission and enable telemetry by calling `config(action=\"set\", key=\"telemetry.enabled\", value=True, persist=True)`.\n7.  Attempt completion, indicating that the installation and configuration are complete.\n```\n\u003c/details\u003e\n\n#### Method 1: Using pipx (Recommended)\n\n`pipx` is the recommended way to install ImageSorcery MCP as it automatically handles virtual environment creation and management, making the installation process much simpler.\n\n\u003cdetails\u003e\n\u003csummary\u003e0.  Install pipx (if not already installed):\u003c/summary\u003e\n\n0.  **Install pipx (if not already installed):**\n    ```bash\n    # On macOS with Homebrew:\n    brew install pipx\n\n    # On Ubuntu/Debian:\n    sudo apt update \u0026\u0026 sudo apt install pipx\n\n    # On other systems with pip:\n    pip install --user pipx\n    pipx ensurepath\n    ```\n\u003c/details\u003e\n\n1.  **Install ImageSorcery MCP with pipx:**\n    ```bash\n    pipx install imagesorcery-mcp\n    ```\n\n2.  **Run the post-installation script:**\n    This step is crucial. It downloads the required models and attempts to install the `clip` Python package from GitHub.\n    ```bash\n    imagesorcery-mcp --post-install\n    ```\n\n#### Method 2: Manual Virtual Environment (Plan B)\n\n\u003cdetails\u003e\n\u003csummary\u003eIf pipx doesn't work for your system, you can manually create a virtual environment\u003c/summary\u003e\n\nFor reliable installation of all components, especially the `clip` package (installed via the post-install script), it is **strongly recommended to use Python's built-in `venv` module instead of `uv venv`**.\n\n1.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv imagesorcery-mcp\n    source imagesorcery-mcp/bin/activate  # For Linux/macOS\n    # source imagesorcery-mcp\\Scripts\\activate    # For Windows\n    ```\n\n2.  **Install the package into the activated virtual environment:**\n    You can use `pip` or `uv pip`.\n    ```bash\n    pip install imagesorcery-mcp\n    # OR, if you prefer using uv for installation into the venv:\n    # uv pip install imagesorcery-mcp\n    ```\n\n3.  **Run the post-installation script:**\n    This step is crucial. It downloads the required models and attempts to install the `clip` Python package from GitHub into the active virtual environment.\n    ```bash\n    imagesorcery-mcp --post-install\n    ```\n\n**Note:** When using this method, you'll need to provide the full path to the executable in your MCP client configuration (e.g., `/full/path/to/venv/bin/imagesorcery-mcp`).\n\u003c/details\u003e\n\n\n#### Additional Notes\n\u003cdetails\u003e\n\u003csummary\u003eWhat does the post-installation script do?\u003c/summary\u003e\nThe `imagesorcery-mcp --post-install` script performs the following actions:\n\n- **Creates a `config.toml` configuration file** in the current directory, allowing users to customize default tool parameters.\n- Creates a `models` directory (usually within the site-packages directory of your virtual environment, or a user-specific location if installed globally) to store pre-trained models.\n- Generates an initial `models/model_descriptions.json` file there.\n- Downloads default YOLO models (`yoloe-11l-seg-pf.pt`, `yoloe-11s-seg-pf.pt`, `yoloe-11l-seg.pt`, `yoloe-11s-seg.pt`) required by the `detect` tool into this `models` directory.\n- **Attempts to install the `clip` Python package** from Ultralytics' GitHub repository directly into the active Python environment. This is required for text prompt functionality in the `find` tool.\n- Downloads the CLIP model file required by the `find` tool into the `models` directory.\n\nYou can run this process anytime to restore the default models and attempt `clip` installation.\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eImportant Notes for `uv` users (\u003ccode\u003euv venv\u003c/code\u003e and \u003ccode\u003euvx\u003c/code\u003e)\u003c/summary\u003e\n\n-   **Using `uv venv` to create virtual environments:**\n    Based on testing, virtual environments created with `uv venv` may not include `pip` in a way that allows the `imagesorcery-mcp --post-install` script to automatically install the `clip` package from GitHub (it might result in a \"No module named pip\" error during the `clip` installation step).\n    **If you choose to use `uv venv`:**\n    1.  Create and activate your `uv venv`.\n    2.  Install `imagesorcery-mcp`: `uv pip install imagesorcery-mcp`.\n    3.  Manually install the `clip` package into your active `uv venv`:\n        ```bash\n        uv pip install git+https://github.com/ultralytics/CLIP.git\n        ```\n    3.  Run `imagesorcery-mcp --post-install`. This will download models but may fail to install the `clip` Python package.\n    For a smoother automated `clip` installation via the post-install script, using `python -m venv` (as described in step 1 above) is the recommended method for creating the virtual environment.\n\n-   **Using `uvx imagesorcery-mcp --post-install`:**\n    Running the post-installation script directly with `uvx` (e.g., `uvx imagesorcery-mcp --post-install`) will likely fail to install the `clip` Python package. This is because the temporary environment created by `uvx` typically does not have `pip` available in a way the script can use. Models will be downloaded, but the `clip` package won't be installed by this command.\n    If you intend to use `uvx` to run the main `imagesorcery-mcp` server and require `clip` functionality, you'll need to ensure the `clip` package is installed in an accessible Python environment that `uvx` can find, or consider installing `imagesorcery-mcp` into a persistent environment created with `python -m venv`.\n\u003c/details\u003e\n\n## ⚙️ Configure MCP client\n\nAdd to your MCP client these settings.\n\n**For pipx installation (recommended):**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"imagesorcery-mcp\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\n**For manual venv installation:**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"/full/path/to/venv/bin/imagesorcery-mcp\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003cdetails\u003e\n\u003csummary\u003eIf you're using the server in HTTP mode, configure your client to connect to the HTTP endpoint:\u003c/summary\u003e\n\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"url\": \"http://127.0.0.1:8000/mcp\", // Use your custom host, port, and path if specified\n      \"transportType\": \"http\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eFor Windows\u003c/summary\u003e\n\n**For pipx installation (recommended):**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"imagesorcery-mcp.exe\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\n**For manual venv installation:**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"C:\\\\full\\\\path\\\\to\\\\venv\\\\Scripts\\\\imagesorcery-mcp.exe\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003c/details\u003e\n\n## 📦 Additional Models\n\nSome tools require specific models to be available in the `models` directory:\n\n```bash\n# Download models for the detect tool\ndownload-yolo-models --ultralytics yoloe-11l-seg\ndownload-yolo-models --huggingface ultralytics/yolov8:yolov8m.pt\n```\n\n\u003cdetails\u003e\n\u003csummary\u003eAbout Model Descriptions\u003c/summary\u003e\n\nWhen downloading models, the script automatically updates the `models/model_descriptions.json` file:\n\n- For Ultralytics models: Descriptions are predefined in `src/imagesorcery_mcp/scripts/create_model_descriptions.py` and include detailed information about each model's purpose, size, and characteristics.\n\n- For Hugging Face models: Descriptions are automatically extracted from the model card on Hugging Face Hub. The script attempts to use the model name from the model index or the first line of the description.\n\nAfter downloading models, it's recommended to check the descriptions in `models/model_descriptions.json` and adjust them if needed to provide more accurate or detailed information about the models' capabilities and use cases.\n\u003c/details\u003e\n\n### Running the Server\n\nImageSorcery MCP server can be run in different modes:\n- `STDIO` - default\n- `Streamable HTTP` - for web-based deployments\n- `Server-Sent Events (SSE)` - for web-based deployments that rely on SSE\n\n\u003cdetails\u003e\n\u003csummary\u003eAbout different modes:\u003c/summary\u003e\n\n1. **STDIO Mode (Default)** - This is the standard mode for local MCP clients:\n   ```bash\n   imagesorcery-mcp\n   ```\n\n2. **Streamable HTTP Mode** - For web-based deployments:\n   ```bash\n   imagesorcery-mcp --transport=streamable-http\n   ```\n   \n   With custom host, port, and path:\n   ```bash\n   imagesorcery-mcp --transport=streamable-http --host=0.0.0.0 --port=4200 --path=/custom-path\n   ```\n\nAvailable transport options:\n- `--transport`: Choose between \"stdio\" (default), \"streamable-http\", or \"sse\"\n- `--host`: Specify host for HTTP-based transports (default: 127.0.0.1)\n- `--port`: Specify port for HTTP-based transports (default: 8000)\n- `--path`: Specify endpoint path for HTTP-based transports (default: /mcp)\n\u003c/details\u003e\n\n## 🔒 Privacy \u0026 Telemetry\n\nWe are committed to your privacy. ImageSorcery MCP is designed to run locally, ensuring your images and data stay on your machine.\n\nTo help us understand which features are most popular and fix bugs faster, we've included optional, anonymous telemetry.\n\n-   **It is disabled by default.** You must explicitly opt-in to enable it.\n-   **What we collect:** Anonymized usage data, including features used (e.g., `crop`, `detect`), application version, operating system type (e.g., 'linux', 'win32'), and tool failures.\n-   **What we NEVER collect:** We do not collect any personal or sensitive information. This includes image data, file paths, IP addresses, or any other personally identifiable information.\n-   **How to enable/disable:** You can control telemetry by setting `enabled = true` or `enabled = false` in the `[telemetry]` section of your `config.toml` file.\n\n## ⚙️ Configuring the Server\n\nThe server can be configured using a `config.toml` file in the current directory. The file is created automatically during installation with default values. You can customize the default tool parameters in this file. More in [CONFIG.md](CONFIG.md).\n\n## 🤝 Contributing\n\u003cdetails\u003e\n\u003csummary\u003eWhether you're a 👤 human or an 🤖 AI agent, we welcome your contributions to this project!\u003c/summary\u003e\n\n### Directory Structure\n\nThis repository is organized as follows:\n\n```\n.\n├── .gitignore                 # Specifies intentionally untracked files that Git should ignore.\n├── pyproject.toml             # Configuration file for Python projects, including build system, dependencies, and tool settings.\n├── pytest.ini                 # Configuration file for the pytest testing framework.\n├── README.md                  # The main documentation file for the project.\n├── setup.sh                   # A shell script for quick setup (legacy, for reference or local use).\n├── models/                    # This directory stores pre-trained models used by tools like `detect` and `find`. It is typically ignored by Git due to the large file sizes.\n│   ├── model_descriptions.json  # Contains descriptions of the available models.\n│   ├── settings.json            # Contains settings related to model management and training runs.\n│   └── *.pt                     # Pre-trained model.\n├── src/                       # Contains the source code for the 🪄 ImageSorcery MCP server.\n│   └── imagesorcery_mcp/       # The main package directory for the server.\n│       ├── README.md            # High-level overview of the core architecture (server and middleware).\n│       ├── __init__.py          # Makes `imagesorcery_mcp` a Python package.\n│       ├── __main__.py          # Entry point for running the package as a script.\n│       ├── logging_config.py    # Configures the logging for the server.\n│       ├── server.py            # The main server file, responsible for initializing FastMCP and registering tools.\n│       ├── middleware.py        # Custom middleware for improved validation error handling.\n│       ├── logs/                # Directory for storing server logs.\n│       ├── scripts/             # Contains utility scripts for model management.\n│       │   ├── README.md        # Documentation for the scripts.\n│       │   ├── __init__.py      # Makes `scripts` a Python package.\n│       │   ├── create_model_descriptions.py # Script to generate model descriptions.\n│       │   ├── download_clip.py # Script to download CLIP models.\n│       │   ├── post_install.py  # Script to run post-installation tasks.\n│       │   └── download_models.py # Script to download other models (e.g., YOLO).\n│       ├── tools/               # Contains the implementation of individual MCP tools.\n│       │   ├── README.md        # Documentation for the tools.\n│       │   ├── __init__.py      # Makes `tools` a Python package.\n│       │   └── *.py           # Implements the tool.\n│       ├── prompts/             # Contains the implementation of individual MCP prompts.\n│       │   ├── README.md        # Documentation for the prompts.\n│       │   ├── __init__.py      # Makes `prompts` a Python package.\n│       │   └── *.py           # Implements the prompt.\n│       └── resources/           # Contains the implementation of individual MCP resources.\n│           ├── README.md        # Documentation for the resources.\n│           ├── __init__.py      # Makes `resources` a Python package.\n│           └── *.py           # Implements the resource.\n└── tests/                     # Contains test files for the project.\n    ├── test_server.py         # Tests for the main server functionality.\n    ├── data/                  # Contains test data, likely image files used in tests.\n    ├── tools/                 # Contains tests for individual tools.\n    ├── prompts/               # Contains tests for individual prompts.\n    └── resources/             # Contains tests for individual resources.\n```\n\n### Development Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/sunriseapps/imagesorcery-mcp.git # Or your fork\ncd imagesorcery-mcp\n```\n\n2. (Recommended) Create and activate a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate # For Linux/macOS\n# venv\\Scripts\\activate    # For Windows\n```\n\n3. Install the package in editable mode along with development dependencies:\n```bash\npip install -e \".[dev]\"\n```\nThis will install `imagesorcery-mcp` and all dependencies from `[project.dependencies]` and `[project.optional-dependencies].dev` (including `build` and `twine`).\n\n### Rules\n\nThese rules apply to all contributors: humans and AI.\n\n0. Read all the `README.md` files in the project. Understand the project structure and purpose. Understand the guidelines for contributing. Think through how it relates to your task, and how to make changes accordingly.\n1. Read `pyproject.toml`.\nPay attention to sections: `[tool.ruff]`, `[tool.ruff.lint]`, `[project.optional-dependencies]` and `[project]dependencies`.\nStrictly follow code style defined in `pyproject.toml`.\nStick to the stack defined in `pyproject.toml` dependencies and do not add any new dependencies without a good reason.\n2. Write your code in new and existing files.\nIf new dependencies are needed, update `pyproject.toml` and install them via `pip install -e .` or `pip install -e \".[dev]\"`. Do not install them directly via `pip install`.\nCheck out existing source codes for examples (e.g. `src/imagesorcery_mcp/server.py`, `src/imagesorcery_mcp/tools/crop.py`). Stick to the code style, naming conventions, input and output data formats, code structure, architecture, etc. of the existing code.\n3. Update related `README.md` files with your changes.\nStick to the format and structure of the existing `README.md` files.\n4. Write tests for your code.\nCheck out existing tests for examples (e.g. `tests/test_server.py`, `tests/tools/test_crop.py`).\nStick to the code style, naming conventions, input and output data formats, code structure, architecture, etc. of the existing tests.\n\n5. Run tests and linter to ensure everything works:\n```bash\npytest\nruff check .\n```\nIn case of failures - fix the code and tests. It is **strictly required** to have all new code to comply with the linter rules and pass all tests.\n\n\n### Coding hints\n- Use type hints where appropriate\n- Use pydantic for data validation and serialization\n\u003c/details\u003e\n\n## 📝 Questions?\n\nIf you have any questions, issues, or suggestions regarding this project, feel free to reach out to:\n\n- Project Author: [titulus](https://www.linkedin.com/in/titulus/) via LinkedIn\n- Sunrise Apps CEO: [Vlad Karm](https://www.linkedin.com/in/vladkarm/) via LinkedIn\n\nYou can also open an issue in the repository for bug reports or feature requests.\n\n## 📜 License\n\nThis project is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n","stargazer_count":224,"topics":["computer-vision","image-processing","ocr","opencv","image-editing","image-manipulation","mcp","mcp-server"],"uses_custom_opengraph_image":false}}}},{"name":"azure-ai-foundry/mcp-foundry","description":"An experimental MCP server implementation for Azure AI Foundry that exposes unified tools for models, knowledge, evaluation and deployment.","status":"active","repository":{"url":"https://github.com/azure-ai-foundry/mcp-foundry","source":"github","id":"952560918","readme":"# MCP Server that interacts with Azure AI Foundry (experimental)\n\nA Model Context Protocol server for Azure AI Foundry, providing a unified set of tools for models, knowledge, evaluation, and more.\n\n[![GitHub watchers](https://img.shields.io/github/watchers/azure-ai-foundry/mcp-foundry.svg?style=social\u0026label=Watch)](https://github.com/azure-ai-foundry/mcp-foundry/watchers)\n[![GitHub forks](https://img.shields.io/github/forks/azure-ai-foundry/mcp-foundry.svg?style=social\u0026label=Fork)](https://github.com/azure-ai-foundry/mcp-foundry/fork)\n[![GitHub stars](https://img.shields.io/github/stars/azure-ai-foundry/mcp-foundry?style=social\u0026label=Star)](https://github.com/azure-ai-foundry/mcp-foundry/stargazers)\n\n[![Azure AI Community Discord](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/REmjGvvFpW)\n\n## Available Tools\n\n### Capabilities: Models\n\n| Category | Tool | Description |\n|---|---|---|\n| **Explore** | `list_models_from_model_catalog` | Retrieves a list of supported models from the Azure AI Foundry catalog. |\n|  | `list_azure_ai_foundry_labs_projects` | Retrieves a list of state-of-the-art AI models from Microsoft Research available in Azure AI Foundry Labs. |\n| | `get_model_details_and_code_samples` | Retrieves detailed information for a specific model from the Azure AI Foundry catalog. |\n| **Build** | `get_prototyping_instructions_for_github_and_labs` | Provides comprehensive instructions and setup guidance for starting to work with models from Azure AI Foundry and Azure AI Foundry Labs. |\n| **Deploy** | `get_model_quotas` | Get model quotas for a specific Azure location. |\n| | `create_azure_ai_services_account` | Creates an Azure AI Services account. |\n| | `list_deployments_from_azure_ai_services` | Retrieves a list of deployments from Azure AI Services. |\n| | `deploy_model_on_ai_services` | Deploys a model on Azure AI Services. |\n| | `create_foundry_project` | Creates a new Azure AI Foundry project. |\n\n### Capabilities: Knowledge\n\n| Category | Tool | Description |\n|---|---|---|\n| **Index** | `list_index_names` | Retrieve all names of indexes from the AI Search Service |\n|  | `list_index_schemas` | Retrieve all index schemas from the AI Search Service |\n|  | `retrieve_index_schema` | Retrieve the schema for a specific index from the AI Search Service |\n|  | `create_index` | Creates a new index |\n|  | `modify_index` | Modifies the index definition of an existing index |\n|  | `delete_index` | Removes an existing index |\n| **Document** | `add_document` | Adds a document to the index |\n|  | `delete_document` | Removes a document from the index |\n| **Query** | `query_index` | Searches a specific index to retrieve matching documents |\n|  | `get_document_count` | Returns the total number of documents in the index |\n| **Indexer** | `list_indexers` | Retrieve all names of indexers from the AI Search Service |\n|  | `get_indexer` | Retrieve the full definition of a specific indexer from the AI Search Service |\n|  | `create_indexer` | Create a new indexer in the Search Service with the skill, index and data source |\n|  | `delete_indexer` | Delete an indexer from the AI Search Service by name |\n| **Data Source** | `list_data_sources` | Retrieve all names of data sources from the AI Search Service |\n|  | `get_data_source` | Retrieve the full definition of a specific data source |\n| **Skill Set** | `list_skill_sets` | Retrieve all names of skill sets from the AI Search Service |\n|  | `get_skill_set` | Retrieve the full definition of a specific skill set |\n| **Content** | `fk_fetch_local_file_contents` | Retrieves the contents of a local file path (sample JSON, document etc) |\n|  | `fk_fetch_url_contents` | Retrieves the contents of a URL (sample JSON, document etc) |\n\n### Capabilities: Evaluation\n\n| Category | Tool | Description |\n|---|---|---|\n| **Evaluator Utilities** | `list_text_evaluators` | List all available text evaluators. |\n|  | `list_agent_evaluators` | List all available agent evaluators. |\n|  | `get_text_evaluator_requirements` | Show input requirements for each text evaluator. |\n|  | `get_agent_evaluator_requirements` | Show input requirements for each agent evaluator. |\n| **Text Evaluation** | `run_text_eval` | Run one or multiple text evaluators on a JSONL file or content. |\n|  | `format_evaluation_report` | Convert evaluation output into a readable Markdown report. |\n| **Agent Evaluation** | `agent_query_and_evaluate` | Query an agent and evaluate its response using selected evaluators. End-to-End agent evaluation. |\n|  | `run_agent_eval` | Evaluate a single agent interaction with specific data (query, response, tool calls, definitions). |\n| **Agent Service** | `list_agents` | List all Azure AI Agents available in the configured project. |\n|  | `connect_agent` | Send a query to a specified agent. |\n|  | `query_default_agent` | Query the default agent defined in environment variables. |\n\n### Capabilities: Finetuning\n\n| Category | Tool | Description |\n|---|---|---|\n| **Finetuning** | `fetch_finetuning_status` | Retrieves detailed status and metadata for a specific fine-tuning job, including job state, model, creation and finish times, hyperparameters, and any errors. |\n|  | `list_finetuning_jobs` | Lists all fine-tuning jobs in the resource, returning job IDs and their current statuses for easy tracking and management. |\n|  | `get_finetuning_job_events` | Retrieves a chronological list of all events for a specific fine-tuning job, including timestamps and detailed messages for each training step, evaluation, and completion. |\n|  | `get_finetuning_metrics` | Retrieves training and evaluation metrics for a specific fine-tuning job, including loss curves, accuracy, and other relevant performance indicators for monitoring and analysis. |\n|  | `list_finetuning_files` | Lists all files available for fine-tuning in Azure OpenAI, including file IDs, names, purposes, and statuses. |\n|  | `execute_dynamic_swagger_action` | Executes any tool dynamically generated from the Swagger specification, allowing flexible API calls for advanced scenarios. |\n|  | `list_dynamic_swagger_tools` | Lists all dynamically registered tools from the Swagger specification, enabling discovery and automation of available API endpoints. |\n\n\n## Prompt Examples\n\n### Models\n\n#### Explore models\n\n- How can you help me find the right model?\n- What models can I use from Azure AI Foundry?\n- What OpenAI models are available in Azure AI Foundry?\n- What are the most popular models in Azure AI Foundry? Pick me 10 models.\n- What models are good for reasoning? Show me some examples in two buckets, one for large models and one for small models.\n- Can you compare Phi models and explain differences?\n- Show me the model card for Phi-4-reasoning.\n- Can you show me how to test a model?\n- What does free playground in Azure AI Foundry mean?\n- Can I use GitHub token to test models?\n- Show me latest models that support GitHub token.\n- Who are the model publishers for the models in Azure AI Foundry?\n- Show me models from Meta.\n- Show me models with MIT license.\n\n#### Build prototypes\n\n- Can you describe how you can help me build a prototype using the model?\n- Describe how you can build a prototype that uses an OpenAI model with my GitHub token. Don't try to create one yet.\n- Recommend me a few scenarios to build prototypes with models.\n- Tell me about Azure AI Foundry Labs.\n- Tell me more about Magentic One\n- What is Omniparser and what are potential use cases?\n- Can you help me build a prototype using Omniparser?\n\n#### Deploy OpenAI models\n\n- Can you help me deploy OpenAI models?\n- What steps do I need to take to deploy OpenAI models on Azure AI Foundry?\n- Can you help me understand how I can use OpenAI models on Azure AI Foundry using GitHub token? Can I use it for production?\n- I already have an Azure AI services resource. Can I deploy OpenAI models on it?\n- What does quota for OpenAI models mean on Azure AI Foundry?\n- Get me current quota for my AI services resource.\n\n## Quick Start with GitHub Copilot\n\n[![Use The Template](https://img.shields.io/static/v1?style=for-the-badge\u0026label=Use+The+Template\u0026message=GitHub\u0026color=181717\u0026logo=github)](https://github.com/azure-ai-foundry/foundry-models-playground/generate)\n\n\u003e [This GitHub template](https://github.com/azure-ai-foundry/foundry-mcp-playground) has minimal setup with MCP server configuration and all required dependencies, making it easy to get started with your own projects.\n\n[![Install in VS Code](https://img.shields.io/static/v1?style=for-the-badge\u0026label=Install+in+VS+Code\u0026message=Open\u0026color=007ACC\u0026logo=visualstudiocode)](https://insiders.vscode.dev/redirect/mcp/install?name=Azure%20AI%20Foundry%20MCP%20Server\u0026config=%7B%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22--prerelease%3Dallow%22%2C%22--from%22%2C%22git%2Bhttps%3A%2F%2Fgithub.com%2Fazure-ai-foundry%2Fmcp-foundry.git%22%2C%22run-azure-ai-foundry-mcp%22%5D%7D)\n\n\u003e This helps you automatically set up the MCP server in your VS Code environment under user settings.\n\u003e You will need `uvx` installed in your environment to run the server.\n\n## Manual Setup\n\n1. Install `uv` by following [Installing uv](https://docs.astral.sh/uv/getting-started/installation/).\n1. Start a new workspace in VS Code.\n1. (Optional) Create `.env` file in the root of your workspace to set environment variables.\n1. Create `.vscode/mcp.json` in the root of your workspace.\n\n    ```json\n    {\n        \"servers\": {\n            \"mcp_foundry_server\": {\n                \"type\": \"stdio\",\n                \"command\": \"uvx\",\n                \"args\": [\n                    \"--prerelease=allow\",\n                    \"--from\",\n                    \"git+https://github.com/azure-ai-foundry/mcp-foundry.git\",\n                    \"run-azure-ai-foundry-mcp\",\n                    \"--envFile\",\n                    \"${workspaceFolder}/.env\"\n                ]\n            }\n        }\n    }\n    ```\n\n1. Click `Start` button for the server in `.vscode/mcp.json` file.\n1. Open GitHub Copilot chat in Agent mode and start asking questions.\n\nSee [More examples for advanced setup](./clients/README.md) for more details on how to set up the MCP server.\n\n## Setting the Environment Variables\n\nTo securely pass information to the MCP server, such as API keys, endpoints, and other sensitive data, you can use environment variables. This is especially important for tools that require authentication or access to external services.\n\nYou can set these environment variables in a `.env` file in the root of your project. You can pass the location of `.env` file when setting up MCP Server, and the server will automatically load these variables when it starts.\n\nSee [example .env file](./clients/python/pydantic-ai/.env.example) for a sample configuration.\n\n| Category       | Variable                      | Required?                          | Description                                      |\n| -------------- | ----------------------------- | ---------------------------------- | ------------------------------------------------ |\n| **Model**      | `GITHUB_TOKEN`                | No                                 | GitHub token for testing models for free with rate limits. |\n| **Knowledge**  | `AZURE_AI_SEARCH_ENDPOINT`    | Always                             | The endpoint URL for your Azure AI Search service. It should look like this: `https://\u003cyour-search-service-name\u003e.search.windows.net/`. |\n|                | `AZURE_AI_SEARCH_API_VERSION` | No                                 | API Version to use. Defaults to `2025-03-01-preview`. |\n|                | `SEARCH_AUTHENTICATION_METHOD`| Always                             | `service-principal` or `api-search-key`.         |\n|                | `AZURE_TENANT_ID`             | Yes when using `service-principal` | The ID of your Azure Active Directory tenant.    |\n|                | `AZURE_CLIENT_ID`             | Yes when using `service-principal` | The ID of your Service Principal (app registration) |\n|                | `AZURE_CLIENT_SECRET`         | Yes when using `service-principal` | The secret credential for the Service Principal. |\n|                | `AZURE_AI_SEARCH_API_KEY`     | Yes when using `api-search-key`    | The API key for your Azure AI Search service.    |\n| **Evaluation** | `EVAL_DATA_DIR`               | Always                             | Path to the JSONL evaluation dataset             |\n|                | `AZURE_OPENAI_ENDPOINT`       | Text quality evaluators            | Endpoint for Azure OpenAI                        |\n|                | `AZURE_OPENAI_API_KEY`        | Text quality evaluators            | API key for Azure OpenAI                         |\n|                | `AZURE_OPENAI_DEPLOYMENT`     | Text quality evaluators            | Deployment name (e.g., `gpt-4o`)                 |\n|                | `AZURE_OPENAI_API_VERSION`    | Text quality evaluators            | Version of the OpenAI API                        |\n|                | `AZURE_AI_PROJECT_ENDPOINT`   | Agent services                     | Used for Azure AI Agent querying and evaluation  |\n\n\u003e [!NOTE]\n\u003e **Model**\n\u003e - `GITHUB_TOKEN` is used to authenticate with GitHub API for testing models. It is not required if you are exploring models from Foundry catalog.\n\u003e\n\u003e **Knowledge**\n\u003e - See [Create a search service](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) to learn more about provisioning a search service.\n\u003e - Azure AI Search supports multiple authentication methods. You can use either a **Microsoft Entra authentication** or an **Key-based authentication** to authenticate your requests. The choice of authentication method depends on your security requirements and the Azure environment you are working in.\n\u003e - See [Authenication](https://learn.microsoft.com/en-us/azure/search/search-security-overview#authentication) to learn more about authentication methods for a search service.\n\u003e\n\u003e **Evaluation**\n\u003e - If you're using **agent tools or safety evaluators**, make sure the Azure project credentials are valid.\n\u003e - If you're only doing **text quality evaluation**, the OpenAI endpoint and key are sufficient.\n\n## License\n\nMIT License. See LICENSE for details.\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:09Z","updated_at":"2025-10-22T11:21:24Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"pypi","identifier":"run-azure-ai-foundry-mcp","runtime_hint":"uvx","runtime_arguments":[{"format":"string","type":"named","name":"--prerelease=allow"},{"is_required":true,"format":"string","type":"named","name":"--from"},{"is_required":true,"format":"string","value":"git+https://github.com/azure-ai-foundry/mcp-foundry.git","type":"positional","name":"git+https://github.com/azure-ai-foundry/mcp-foundry.git"}],"package_arguments":[{"format":"string","type":"named","name":"--envFile"},{"format":"string","value":"{env_file_path}","variables":{"env_file_path":{"description":"Path to a .env file to load (e.g., ${workspaceFolder}/.env)."}},"type":"positional","name":"env_file_path"}],"environment_variables":[{"value":"{github_token}","variables":{"github_token":{"description":"Optional: GitHub token for testing models for free with rate limits.","is_secret":true}},"name":"GITHUB_TOKEN"},{"value":"{ai_search_endpoint}","variables":{"ai_search_endpoint":{"description":"Your Azure AI Search endpoint (https://\u003cservice\u003e.search.windows.net/). Required for knowledge tools."}},"name":"AZURE_AI_SEARCH_ENDPOINT"},{"value":"{ai_search_api_version}","variables":{"ai_search_api_version":{"description":"API version (defaults to 2025-03-01-preview)."}},"name":"AZURE_AI_SEARCH_API_VERSION"},{"value":"{search_auth_method}","variables":{"search_auth_method":{"description":"Authentication method: 'service-principal' or 'api-search-key'."}},"name":"SEARCH_AUTHENTICATION_METHOD"},{"value":"{azure_tenant_id}","variables":{"azure_tenant_id":{"description":"Required if using service-principal auth."}},"name":"AZURE_TENANT_ID"},{"value":"{azure_client_id}","variables":{"azure_client_id":{"description":"Required if using service-principal auth."}},"name":"AZURE_CLIENT_ID"},{"value":"{azure_client_secret}","variables":{"azure_client_secret":{"description":"Required if using service-principal auth.","is_secret":true}},"name":"AZURE_CLIENT_SECRET"},{"value":"{ai_search_api_key}","variables":{"ai_search_api_key":{"description":"Required if using api-search-key auth.","is_secret":true}},"name":"AZURE_AI_SEARCH_API_KEY"},{"value":"{eval_data_dir}","variables":{"eval_data_dir":{"description":"Path to JSONL datasets for evaluation tools."}},"name":"EVAL_DATA_DIR"},{"value":"{az_openai_endpoint}","variables":{"az_openai_endpoint":{"description":"Endpoint for text-quality evaluators (optional)."}},"name":"AZURE_OPENAI_ENDPOINT"},{"value":"{az_openai_api_key}","variables":{"az_openai_api_key":{"description":"API key for the above endpoint (optional).","is_secret":true}},"name":"AZURE_OPENAI_API_KEY"},{"value":"{az_openai_deployment}","variables":{"az_openai_deployment":{"description":"Deployment name (e.g., gpt-4o) used by evaluators (optional)."}},"name":"AZURE_OPENAI_DEPLOYMENT"},{"value":"{az_openai_api_version}","variables":{"az_openai_api_version":{"description":"API version for Azure OpenAI (optional)."}},"name":"AZURE_OPENAI_API_VERSION"},{"value":"{ai_project_endpoint}","variables":{"ai_project_endpoint":{"description":"Azure AI Project endpoint for agent tools (optional)."}},"name":"AZURE_AI_PROJECT_ENDPOINT"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"edcc494d-9f35-47be-8f64-148e3a21dbc8","is_latest":true,"published_at":"2025-09-09T10:55:22.907291Z","updated_at":"2025-09-09T10:55:22.907291Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Azure AI Foundry","is_in_organization":true,"license":"MIT License","name":"mcp-foundry","name_with_owner":"azure-ai-foundry/mcp-foundry","opengraph_image_url":"https://opengraph.githubassets.com/11bc281b5548a108d55cb6911f9d4ee70dbed068c0169c5c732ff56f3eb89255/azure-ai-foundry/mcp-foundry","owner_avatar_url":"https://avatars.githubusercontent.com/u/202016865?v=4","preferred_image":"https://avatars.githubusercontent.com/u/202016865?v=4","primary_language":"Python","primary_language_color":"#3572A5","pushed_at":"2025-08-01T15:59:25Z","readme":"# MCP Server that interacts with Azure AI Foundry (experimental)\n\nA Model Context Protocol server for Azure AI Foundry, providing a unified set of tools for models, knowledge, evaluation, and more.\n\n[![GitHub watchers](https://img.shields.io/github/watchers/azure-ai-foundry/mcp-foundry.svg?style=social\u0026label=Watch)](https://github.com/azure-ai-foundry/mcp-foundry/watchers)\n[![GitHub forks](https://img.shields.io/github/forks/azure-ai-foundry/mcp-foundry.svg?style=social\u0026label=Fork)](https://github.com/azure-ai-foundry/mcp-foundry/fork)\n[![GitHub stars](https://img.shields.io/github/stars/azure-ai-foundry/mcp-foundry?style=social\u0026label=Star)](https://github.com/azure-ai-foundry/mcp-foundry/stargazers)\n\n[![Azure AI Community Discord](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/REmjGvvFpW)\n\n## Available Tools\n\n### Capabilities: Models\n\n| Category | Tool | Description |\n|---|---|---|\n| **Explore** | `list_models_from_model_catalog` | Retrieves a list of supported models from the Azure AI Foundry catalog. |\n|  | `list_azure_ai_foundry_labs_projects` | Retrieves a list of state-of-the-art AI models from Microsoft Research available in Azure AI Foundry Labs. |\n| | `get_model_details_and_code_samples` | Retrieves detailed information for a specific model from the Azure AI Foundry catalog. |\n| **Build** | `get_prototyping_instructions_for_github_and_labs` | Provides comprehensive instructions and setup guidance for starting to work with models from Azure AI Foundry and Azure AI Foundry Labs. |\n| **Deploy** | `get_model_quotas` | Get model quotas for a specific Azure location. |\n| | `create_azure_ai_services_account` | Creates an Azure AI Services account. |\n| | `list_deployments_from_azure_ai_services` | Retrieves a list of deployments from Azure AI Services. |\n| | `deploy_model_on_ai_services` | Deploys a model on Azure AI Services. |\n| | `create_foundry_project` | Creates a new Azure AI Foundry project. |\n\n### Capabilities: Knowledge\n\n| Category | Tool | Description |\n|---|---|---|\n| **Index** | `list_index_names` | Retrieve all names of indexes from the AI Search Service |\n|  | `list_index_schemas` | Retrieve all index schemas from the AI Search Service |\n|  | `retrieve_index_schema` | Retrieve the schema for a specific index from the AI Search Service |\n|  | `create_index` | Creates a new index |\n|  | `modify_index` | Modifies the index definition of an existing index |\n|  | `delete_index` | Removes an existing index |\n| **Document** | `add_document` | Adds a document to the index |\n|  | `delete_document` | Removes a document from the index |\n| **Query** | `query_index` | Searches a specific index to retrieve matching documents |\n|  | `get_document_count` | Returns the total number of documents in the index |\n| **Indexer** | `list_indexers` | Retrieve all names of indexers from the AI Search Service |\n|  | `get_indexer` | Retrieve the full definition of a specific indexer from the AI Search Service |\n|  | `create_indexer` | Create a new indexer in the Search Service with the skill, index and data source |\n|  | `delete_indexer` | Delete an indexer from the AI Search Service by name |\n| **Data Source** | `list_data_sources` | Retrieve all names of data sources from the AI Search Service |\n|  | `get_data_source` | Retrieve the full definition of a specific data source |\n| **Skill Set** | `list_skill_sets` | Retrieve all names of skill sets from the AI Search Service |\n|  | `get_skill_set` | Retrieve the full definition of a specific skill set |\n| **Content** | `fk_fetch_local_file_contents` | Retrieves the contents of a local file path (sample JSON, document etc) |\n|  | `fk_fetch_url_contents` | Retrieves the contents of a URL (sample JSON, document etc) |\n\n### Capabilities: Evaluation\n\n| Category | Tool | Description |\n|---|---|---|\n| **Evaluator Utilities** | `list_text_evaluators` | List all available text evaluators. |\n|  | `list_agent_evaluators` | List all available agent evaluators. |\n|  | `get_text_evaluator_requirements` | Show input requirements for each text evaluator. |\n|  | `get_agent_evaluator_requirements` | Show input requirements for each agent evaluator. |\n| **Text Evaluation** | `run_text_eval` | Run one or multiple text evaluators on a JSONL file or content. |\n|  | `format_evaluation_report` | Convert evaluation output into a readable Markdown report. |\n| **Agent Evaluation** | `agent_query_and_evaluate` | Query an agent and evaluate its response using selected evaluators. End-to-End agent evaluation. |\n|  | `run_agent_eval` | Evaluate a single agent interaction with specific data (query, response, tool calls, definitions). |\n| **Agent Service** | `list_agents` | List all Azure AI Agents available in the configured project. |\n|  | `connect_agent` | Send a query to a specified agent. |\n|  | `query_default_agent` | Query the default agent defined in environment variables. |\n\n### Capabilities: Finetuning\n\n| Category | Tool | Description |\n|---|---|---|\n| **Finetuning** | `fetch_finetuning_status` | Retrieves detailed status and metadata for a specific fine-tuning job, including job state, model, creation and finish times, hyperparameters, and any errors. |\n|  | `list_finetuning_jobs` | Lists all fine-tuning jobs in the resource, returning job IDs and their current statuses for easy tracking and management. |\n|  | `get_finetuning_job_events` | Retrieves a chronological list of all events for a specific fine-tuning job, including timestamps and detailed messages for each training step, evaluation, and completion. |\n|  | `get_finetuning_metrics` | Retrieves training and evaluation metrics for a specific fine-tuning job, including loss curves, accuracy, and other relevant performance indicators for monitoring and analysis. |\n|  | `list_finetuning_files` | Lists all files available for fine-tuning in Azure OpenAI, including file IDs, names, purposes, and statuses. |\n|  | `execute_dynamic_swagger_action` | Executes any tool dynamically generated from the Swagger specification, allowing flexible API calls for advanced scenarios. |\n|  | `list_dynamic_swagger_tools` | Lists all dynamically registered tools from the Swagger specification, enabling discovery and automation of available API endpoints. |\n\n\n## Prompt Examples\n\n### Models\n\n#### Explore models\n\n- How can you help me find the right model?\n- What models can I use from Azure AI Foundry?\n- What OpenAI models are available in Azure AI Foundry?\n- What are the most popular models in Azure AI Foundry? Pick me 10 models.\n- What models are good for reasoning? Show me some examples in two buckets, one for large models and one for small models.\n- Can you compare Phi models and explain differences?\n- Show me the model card for Phi-4-reasoning.\n- Can you show me how to test a model?\n- What does free playground in Azure AI Foundry mean?\n- Can I use GitHub token to test models?\n- Show me latest models that support GitHub token.\n- Who are the model publishers for the models in Azure AI Foundry?\n- Show me models from Meta.\n- Show me models with MIT license.\n\n#### Build prototypes\n\n- Can you describe how you can help me build a prototype using the model?\n- Describe how you can build a prototype that uses an OpenAI model with my GitHub token. Don't try to create one yet.\n- Recommend me a few scenarios to build prototypes with models.\n- Tell me about Azure AI Foundry Labs.\n- Tell me more about Magentic One\n- What is Omniparser and what are potential use cases?\n- Can you help me build a prototype using Omniparser?\n\n#### Deploy OpenAI models\n\n- Can you help me deploy OpenAI models?\n- What steps do I need to take to deploy OpenAI models on Azure AI Foundry?\n- Can you help me understand how I can use OpenAI models on Azure AI Foundry using GitHub token? Can I use it for production?\n- I already have an Azure AI services resource. Can I deploy OpenAI models on it?\n- What does quota for OpenAI models mean on Azure AI Foundry?\n- Get me current quota for my AI services resource.\n\n## Quick Start with GitHub Copilot\n\n[![Use The Template](https://img.shields.io/static/v1?style=for-the-badge\u0026label=Use+The+Template\u0026message=GitHub\u0026color=181717\u0026logo=github)](https://github.com/azure-ai-foundry/foundry-models-playground/generate)\n\n\u003e [This GitHub template](https://github.com/azure-ai-foundry/foundry-mcp-playground) has minimal setup with MCP server configuration and all required dependencies, making it easy to get started with your own projects.\n\n[![Install in VS Code](https://img.shields.io/static/v1?style=for-the-badge\u0026label=Install+in+VS+Code\u0026message=Open\u0026color=007ACC\u0026logo=visualstudiocode)](https://insiders.vscode.dev/redirect/mcp/install?name=Azure%20AI%20Foundry%20MCP%20Server\u0026config=%7B%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22--prerelease%3Dallow%22%2C%22--from%22%2C%22git%2Bhttps%3A%2F%2Fgithub.com%2Fazure-ai-foundry%2Fmcp-foundry.git%22%2C%22run-azure-ai-foundry-mcp%22%5D%7D)\n\n\u003e This helps you automatically set up the MCP server in your VS Code environment under user settings.\n\u003e You will need `uvx` installed in your environment to run the server.\n\n## Manual Setup\n\n1. Install `uv` by following [Installing uv](https://docs.astral.sh/uv/getting-started/installation/).\n1. Start a new workspace in VS Code.\n1. (Optional) Create `.env` file in the root of your workspace to set environment variables.\n1. Create `.vscode/mcp.json` in the root of your workspace.\n\n    ```json\n    {\n        \"servers\": {\n            \"mcp_foundry_server\": {\n                \"type\": \"stdio\",\n                \"command\": \"uvx\",\n                \"args\": [\n                    \"--prerelease=allow\",\n                    \"--from\",\n                    \"git+https://github.com/azure-ai-foundry/mcp-foundry.git\",\n                    \"run-azure-ai-foundry-mcp\",\n                    \"--envFile\",\n                    \"${workspaceFolder}/.env\"\n                ]\n            }\n        }\n    }\n    ```\n\n1. Click `Start` button for the server in `.vscode/mcp.json` file.\n1. Open GitHub Copilot chat in Agent mode and start asking questions.\n\nSee [More examples for advanced setup](./clients/README.md) for more details on how to set up the MCP server.\n\n## Setting the Environment Variables\n\nTo securely pass information to the MCP server, such as API keys, endpoints, and other sensitive data, you can use environment variables. This is especially important for tools that require authentication or access to external services.\n\nYou can set these environment variables in a `.env` file in the root of your project. You can pass the location of `.env` file when setting up MCP Server, and the server will automatically load these variables when it starts.\n\nSee [example .env file](./clients/python/pydantic-ai/.env.example) for a sample configuration.\n\n| Category       | Variable                      | Required?                          | Description                                      |\n| -------------- | ----------------------------- | ---------------------------------- | ------------------------------------------------ |\n| **Model**      | `GITHUB_TOKEN`                | No                                 | GitHub token for testing models for free with rate limits. |\n| **Knowledge**  | `AZURE_AI_SEARCH_ENDPOINT`    | Always                             | The endpoint URL for your Azure AI Search service. It should look like this: `https://\u003cyour-search-service-name\u003e.search.windows.net/`. |\n|                | `AZURE_AI_SEARCH_API_VERSION` | No                                 | API Version to use. Defaults to `2025-03-01-preview`. |\n|                | `SEARCH_AUTHENTICATION_METHOD`| Always                             | `service-principal` or `api-search-key`.         |\n|                | `AZURE_TENANT_ID`             | Yes when using `service-principal` | The ID of your Azure Active Directory tenant.    |\n|                | `AZURE_CLIENT_ID`             | Yes when using `service-principal` | The ID of your Service Principal (app registration) |\n|                | `AZURE_CLIENT_SECRET`         | Yes when using `service-principal` | The secret credential for the Service Principal. |\n|                | `AZURE_AI_SEARCH_API_KEY`     | Yes when using `api-search-key`    | The API key for your Azure AI Search service.    |\n| **Evaluation** | `EVAL_DATA_DIR`               | Always                             | Path to the JSONL evaluation dataset             |\n|                | `AZURE_OPENAI_ENDPOINT`       | Text quality evaluators            | Endpoint for Azure OpenAI                        |\n|                | `AZURE_OPENAI_API_KEY`        | Text quality evaluators            | API key for Azure OpenAI                         |\n|                | `AZURE_OPENAI_DEPLOYMENT`     | Text quality evaluators            | Deployment name (e.g., `gpt-4o`)                 |\n|                | `AZURE_OPENAI_API_VERSION`    | Text quality evaluators            | Version of the OpenAI API                        |\n|                | `AZURE_AI_PROJECT_ENDPOINT`   | Agent services                     | Used for Azure AI Agent querying and evaluation  |\n\n\u003e [!NOTE]\n\u003e **Model**\n\u003e - `GITHUB_TOKEN` is used to authenticate with GitHub API for testing models. It is not required if you are exploring models from Foundry catalog.\n\u003e\n\u003e **Knowledge**\n\u003e - See [Create a search service](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) to learn more about provisioning a search service.\n\u003e - Azure AI Search supports multiple authentication methods. You can use either a **Microsoft Entra authentication** or an **Key-based authentication** to authenticate your requests. The choice of authentication method depends on your security requirements and the Azure environment you are working in.\n\u003e - See [Authenication](https://learn.microsoft.com/en-us/azure/search/search-security-overview#authentication) to learn more about authentication methods for a search service.\n\u003e\n\u003e **Evaluation**\n\u003e - If you're using **agent tools or safety evaluators**, make sure the Azure project credentials are valid.\n\u003e - If you're only doing **text quality evaluation**, the OpenAI endpoint and key are sufficient.\n\n## License\n\nMIT License. See LICENSE for details.\n","stargazer_count":212,"uses_custom_opengraph_image":false}}}},{"name":"dynatrace-oss/dynatrace-mcp","description":"Manage and interact with the Dynatrace Platform for real-time observability and monitoring.","status":"active","repository":{"url":"https://github.com/dynatrace-oss/dynatrace-mcp","source":"github","id":"971357826","readme":"# Dynatrace MCP Server\n\n\u003ch4 align=\"center\"\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp/releases\"\u003e\n    \u003cimg src=\"https://img.shields.io/github/release/dynatrace-oss/dynatrace-mcp\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/LICENSE\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/license-mit-blue.svg\" alt=\"Dynatrace MCP Server is released under the MIT License\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://www.npmjs.com/package/@dynatrace-oss/dynatrace-mcp-server\"\u003e\n    \u003cimg src=\"https://img.shields.io/npm/dm/@dynatrace-oss/dynatrace-mcp-server?logo=npm\u0026style=flat\u0026color=red\" alt=\"npm\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp\"\u003e\n    \u003cimg src=\"https://img.shields.io/github/stars/dynatrace-oss/dynatrace-mcp\" alt=\"Dynatrace MCP Server Stars on GitHub\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp\"\u003e\n    \u003cimg src=\"https://img.shields.io/github/contributors/dynatrace-oss/dynatrace-mcp?color=green\" alt=\"Dynatrace MCP Server Contributors on GitHub\" /\u003e\n  \u003c/a\u003e\n\u003c/h4\u003e\n\nThe local _Dynatrace MCP server_ allows AI Assistants to interact with the [Dynatrace](https://www.dynatrace.com/) observability platform,\nbringing real-time observability data directly into your development workflow.\n\n\u003e Note: This product is not officially supported by Dynatrace.\n\nIf you need help, please contact us via [GitHub Issues](https://github.com/dynatrace-oss/dynatrace-mcp/issues) if you have feature requests, questions, or need help.\n\nhttps://github.com/user-attachments/assets/25c05db1-8e09-4a7f-add2-ed486ffd4b5a\n\n## Quickstart\n\nYou can add this MCP server to your MCP Client like VSCode, Claude, Cursor, Amazon Q, Windsurf, ChatGPT, or Github Copilot via the command is `npx -y @dynatrace-oss/dynatrace-mcp-server` (type: `stdio`). For more details, please refer to the [configuration section below](#configuration).\n\nFurthermore, you need to configure the URL to a Dynatrace environment:\n\n- `DT_ENVIRONMENT` (string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n\nOnce you are done, we recommend looking into [example prompts](#-example-prompts-), like `Get all details of the entity 'my-service'` or `Show me error logs`. Please mind that these prompts lead to executing DQL statements which may incur [costs](#costs) in accordance to your licence.\n\n## Architecture\n\n![Architecture](https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/assets/dynatrace-mcp-arch.png?raw=true)\n\n## Use cases\n\n- **Real-time observability** - Fetch production-level data for early detection and proactive monitoring\n- **Contextual debugging** - Fix issues with full context from monitored exceptions, logs, and anomalies\n- **Security insights** - Get detailed vulnerability analysis and security problem tracking\n- **Natural language queries** - Use AI-powered DQL generation and explanation\n- **Multi-phase incident investigation** - Systematic 4-phase approach with automated impact assessment\n- **Advanced transaction analysis** - Precise root cause identification with file/line-level accuracy\n- **Cross-data source correlation** - Connect problems → spans → logs with trace ID correlation\n- **DevOps automation** - Deployment health gates with automated promotion/rollback logic\n- **Security compliance monitoring** - Multi-cloud compliance assessment with evidence-based investigation\n\n## Capabilities\n\n- List and get [problem](https://www.dynatrace.com/hub/detail/problems/) details from your services (for example Kubernetes)\n- List and get security problems / [vulnerability](https://www.dynatrace.com/hub/detail/vulnerabilities/) details\n- Execute DQL (Dynatrace Query Language) and retrieve logs, events, spans and metrics\n- Send Slack messages (via Slack Connector)\n- Set up notification Workflow (via Dynatrace [AutomationEngine](https://docs.dynatrace.com/docs/discover-dynatrace/platform/automationengine))\n- Get more information about a monitored entity\n- Get Ownership of an entity\n\n### Costs\n\n**Important:** While this local MCP server is provided for free, using certain capabilities to access data in Dynatrace Grail may incur additional costs based\non your Dynatrace consumption model. This affects `execute_dql` tool and other capabilities that **query** Dynatrace Grail storage, and costs\ndepend on the volume (GB scanned).\n\n**Before using this MCP server extensively, please:**\n\n1. Review your current Dynatrace consumption model and pricing\n2. Understand the cost implications of the specific data you plan to query (logs, events, metrics) - see [Dynatrace Pricing and Rate Card](https://www.dynatrace.com/pricing/)\n3. Start with smaller timeframes (e.g., 12h-24h) and make use of [buckets](https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model#built-in-grail-buckets) to reduce the cost impact\n4. Set an appropriate `DT_GRAIL_QUERY_BUDGET_GB` environment variable (default: 1000 GB) to control and monitor your Grail query consumption\n\n**Grail Budget Tracking:**\n\nThe MCP server includes built-in budget tracking for Grail queries to help you monitor and control costs:\n\n- Set `DT_GRAIL_QUERY_BUDGET_GB` (default: 1000 GB) to define your session budget limit\n- The server tracks bytes scanned across all Grail queries in the current session\n- You'll receive warnings when approaching 80% of your budget\n- Budget exceeded alerts help prevent unexpected high consumption\n- Budget resets when you restart the MCP server session\n\n**To understand costs that occured:**\n\nExecute the following DQL statement in a notebook to see how much bytes have been queried from Grail (Logs, Events, etc...):\n\n```\nfetch dt.system.events\n| filter event.kind == \"QUERY_EXECUTION_EVENT\" and contains(client.client_context, \"dynatrace-mcp\")\n| sort timestamp desc\n| fields timestamp, query_id, query_string, scanned_bytes, table, bucket, user.id, user.email, client.client_context\n| maketimeSeries sum(scanned_bytes), by: { user.email, user.id, table }\n```\n\n### AI-Powered Assistance (Preview)\n\n- **Natural Language to DQL** - Convert plain English queries to Dynatrace Query Language\n- **DQL Explanation** - Get plain English explanations of complex DQL queries\n- **AI Chat Assistant** - Get contextual help and guidance for Dynatrace questions\n- **Feedback System** - Provide feedback to improve AI responses over time\n\n\u003e **Note:** While Davis CoPilot AI is generally available (GA), the Davis CoPilot APIs are currently in preview. For more information, visit the [Davis CoPilot Preview Community](https://dt-url.net/copilot-community).\n\n## Configuration\n\nYou can add this MCP server (using STDIO) to your MCP Client like VS Code, Claude, Cursor, Amazon Q Developer CLI, Windsurf Github Copilot via the package `@dynatrace-oss/dynatrace-mcp-server`.\n\nWe recommend to always set it up for your current workspace instead of using it globally.\n\n**VS Code**\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"cwd\": \"${workspaceFolder}\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"envFile\": \"${workspaceFolder}/.env\"\n    }\n  }\n}\n```\n\nPlease note: In this config, [the `${workspaceFolder}` variable](https://code.visualstudio.com/docs/reference/variables-reference#_predefined-variables) is used.\nThis only works if the config is stored in the current workspaces, e.g., `\u003cyour-repo\u003e/.vscode/mcp.json`. Alternatively, this can also be stored in user-settings, and you can define `env` as follows:\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Claude Desktop**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Amazon Q Developer CLI**\n\nThe [Amazon Q Developer CLI](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) provides an interactive chat experience directly in your terminal. You can ask questions, get help with AWS services, troubleshoot issues, and generate code snippets without leaving your command line environment.\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\nThis configuration should be stored in `\u003cyour-repo\u003e/.amazonq/mcp.json`.\n\n**Google Gemini CLI**\n\nThe [Google Gemini CLI](https://github.com/google-gemini/gemini-cli) is Google's official command-line AI assistant that supports MCP server integration. You can add the Dynatrace MCP server using either the built-in management commands or manual configuration.\n\nUsing `gemini` CLI directly (recommended):\n\n```bash\ngemini extensions install https://github.com/dynatrace-oss/dynatrace-mcp\nexport DT_PLATFORM_TOKEN=... # optional\nexport DT_ENVIRONMENT=https://...\n```\n\nand verify that the server is running via\n\n```bash\ngemini mcp list\n```\n\nOr manually in your `~/.gemini/settings.json` or `.gemini/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace\": {\n      \"command\": \"npx\",\n      \"args\": [\"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      },\n      \"timeout\": 30000,\n      \"trust\": false\n    }\n  }\n}\n```\n\n### HTTP Server Mode (Alternative)\n\nFor scenarios where you need to run the MCP server as an HTTP service instead of using stdio (e.g., for stateful sessions, load balancing, or integration with web clients), you can use the HTTP server mode:\n\n**Running as HTTP server:**\n\n```bash\n# Get help and see all available options\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --help\n\n# Run with HTTP server on default port 3000\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http\n\n# Run with custom port (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --server -p 8080\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --port 3001\n\n# Run with custom host/IP (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --host 127.0.0.1\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http -H 192.168.0.1\n\n# Check version\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --version\n```\n\n**Configuration for MCP clients that support HTTP transport:**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-http\": {\n      \"url\": \"http://localhost:3000\",\n      \"transport\": \"http\"\n    }\n  }\n}\n```\n\n### Rule File\n\nFor efficient result retrieval from Dynatrace, please consider creating a rule file (e.g., [.github/copilot-instructions.md](https://docs.github.com/en/copilot/how-tos/configure-custom-instructions/add-repository-instructions), [.amazonq/rules/](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/context-project-rules.html)), instructing coding agents on how to get more details for your component/app/service. Here is an example for [easytrade](https://github.com/Dynatrace/easytrade), please adapt the names and filters to fit your use-cases and components:\n\n```\n# Observability\n\nWe use Dynatrace as an Observability solution. This document provides instructions on how to get data for easytrade from Dynatrace using DQL.\n\n## How to get any data for my App\n\nDepending on the query and tool used, the following filters can be applied to narrow down results:\n\n* `contains(entity.name, \"easytrade\")`\n* `contains(affected_entity.name, \"easytrade\")`\n* `contains(container.name, \"easytrade\")`\n\nFor best results, you can combine these filters with an `OR` operator.\n\n## Logs\n\nTo fetch logs for easytrade, execute `fetch logs | filter contains(container.name, \"easyatrade\")`.\nFor fetching just error-logs, add `| filter loglevel == \"ERROR\"`.\n```\n\n## Environment Variables\n\n- `DT_ENVIRONMENT` (**required**, string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n- `DT_PLATFORM_TOKEN` (optional, string, e.g., `dt0s16.SAMPLE.abcd1234`) - Dynatrace Platform Token\n- `OAUTH_CLIENT_ID` (optional, string, e.g., `dt0s02.SAMPLE`) - Alternative: Dynatrace OAuth Client ID (for advanced use cases)\n- `OAUTH_CLIENT_SECRET` (optional, string, e.g., `dt0s02.SAMPLE.abcd1234`) - Alternative: Dynatrace OAuth Client Secret (for advanced use cases)\n- `DT_GRAIL_QUERY_BUDGET_GB` (optional, number, default: `1000`) - Budget limit in GB (base 1000) for Grail query bytes scanned per session. The MCP server tracks your Grail usage and warns when approaching or exceeding this limit.\n\nWhen just providing `DT_ENVIRONMENT`, the local MCP server will try to open a browser window to authenticate against the Dynatrace SSO.\n\nFor more information about the other authentication methods, please have a look at the documentation about\n[creating a Platform Token in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/platform-tokens), as well as\n[creating an OAuth Client in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/oauth-clients) for advanced scenarios.\n\nIn addition, depending on the features you use, the following variables can be configured:\n\n- `SLACK_CONNECTION_ID` (string) - connection ID of a [Slack Connection](https://docs.dynatrace.com/docs/analyze-explore-automate/workflows/actions/slack)\n\n### Proxy Configuration\n\nThe MCP server honors system proxy settings for corporate environments:\n\n- `https_proxy` or `HTTPS_PROXY` (optional, string, e.g., `http://proxy.example.com:8080`) - Proxy server URL for HTTPS requests\n- `http_proxy` or `HTTP_PROXY` (optional, string, e.g., `http://proxy.example.com:8080`) - Proxy server URL for HTTP requests\n- `no_proxy` or `NO_PROXY` (optional, string, e.g., `localhost,127.0.0.1,.local`) - Comma-separated list of hostnames or domains that should bypass the proxy\n\n**Note:** The `no_proxy` environment variable is currently logged for informational purposes but not fully enforced by the underlying HTTP client. If you need to bypass the proxy for specific hosts, consider configuring your proxy server to handle these exclusions.\n\nExample configuration with proxy:\n\n```bash\nexport HTTPS_PROXY=http://proxy.company.com:8080\nexport NO_PROXY=localhost,127.0.0.1,.company.local\nexport DT_ENVIRONMENT=https://abc12345.apps.dynatrace.com\n```\n\n### Scopes for Authentication\n\nDepending on the features you are using, the following scopes are needed:\n\n**Available for both Platform Tokens and OAuth Clients:**\n\n- `app-engine:apps:run` - needed for almost all tools\n- `app-engine:functions:run` - needed for for almost all tools\n- `environment-api:entities:read` - for retrieving ownership details from monitored entities (_currently not available for Platform Tokens_)\n- `automation:workflows:read` - read Workflows\n- `automation:workflows:write` - create and update Workflows\n- `automation:workflows:run` - run Workflows\n- `storage:buckets:read` - needed for `execute_dql` tool to read all system data stored on Grail\n- `storage:logs:read` - needed for `execute_dql` tool to read logs for reliability guardian validations\n- `storage:metrics:read` - needed for `execute_dql` tool to read metrics for reliability guardian validations\n- `storage:bizevents:read` - needed for `execute_dql` tool to read bizevents for reliability guardian validations\n- `storage:spans:read` - needed for `execute_dql` tool to read spans from Grail\n- `storage:entities:read` - needed for `execute_dql` tool to read Entities from Grail\n- `storage:events:read` - needed for `execute_dql` tool to read Events from Grail\n- `storage:security.events:read`- needed for `execute_dql` tool to read Security Events from Grail\n- `storage:system:read` - needed for `execute_dql` tool to read System Data from Grail\n- `storage:user.events:read` - needed for `execute_dql` tool to read User events from Grail\n- `storage:user.sessions:read` - needed for `execute_dql` tool to read User sessions from Grail\n- `storage:smartscape:read` - needed for `execute_dql` tool to read Smartscape Data\n- `davis-copilot:conversations:execute` - execute conversational skill (chat with Copilot)\n- `davis-copilot:nl2dql:execute` - execute Davis Copilot Natural Language (NL) to DQL skill\n- `davis-copilot:dql2nl:execute` - execute DQL to Natural Language (NL) skill\n- `email:emails:send` - needed for `send_email` tool to send emails\n- `settings:objects:read` - needed for reading ownership information and Guardians (SRG) from settings\n\n  **Note**: Please ensure that `settings:objects:read` is used, and _not_ the similarly named scope `app-settings:objects:read`.\n\n**Important**: Some features requiring `environment-api:entities:read` will only work with OAuth Clients. For most use cases, Platform Tokens provide all necessary functionality.\n\n## ✨ Example prompts ✨\n\nUse these example prompts as a starting point. Just copy them into your IDE or agent setup, adapt them to your services/stack/architecture,\nand extend them as needed. They're here to help you imagine how real-time observability and automation work together in the MCP context in your IDE.\n\n### **Basic Queries \u0026 AI Assistance**\n\n**Find a monitored entity**\n\n```\nGet all details of the entity 'my-service'\n```\n\n**Find error logs**\n\n```\nShow me error logs\n```\n\n**Write a DQL query from natural language:**\n\n```\nShow me error rates for the payment service in the last hour\n```\n\n**Explain a DQL query:**\n\n```\nWhat does this DQL do?\nfetch logs | filter dt.source_entity == 'SERVICE-123' | summarize count(), by:{severity} | sort count() desc\n```\n\n**Chat with Davis CoPilot:**\n\n```\nHow can I investigate slow database queries in Dynatrace?\n```\n\n**Send email notifications:**\n\n```\nSend an email notification about the incident to the responsible team at team@example.com with CC to manager@example.com\n```\n\n### **Advanced Incident Investigation**\n\n**Multi-phase incident response:**\n\n```\nOur checkout service is experiencing high error rates. Start a systematic 4-phase incident investigation:\n1. Detect and triage the active problems\n2. Assess user impact and affected services\n3. Perform cross-data source analysis (problems → spans → logs)\n4. Identify root cause with file/line-level precision\n```\n\n**Cross-service failure analysis:**\n\n```\nWe have cascading failures across our microservices architecture.\nAnalyze the entity relationships and trace the failure propagation from the initial problem\nthrough all downstream services. Show me the correlation timeline.\n```\n\n### **Security \u0026 Compliance Analysis**\n\n**Latest-scan vulnerability assessment:**\n\n```\nPerform a comprehensive security analysis using the latest scan data:\n- Check for new vulnerabilities in our production environment\n- Focus on critical and high-severity findings\n- Provide evidence-based remediation paths\n- Generate risk scores with team-specific guidance\n```\n\n**Multi-cloud compliance monitoring:**\n\n```\nRun a compliance assessment across our AWS, Azure, and Kubernetes environments.\nCheck for configuration drift and security posture changes in the last 24 hours.\n```\n\n### **DevOps \u0026 SRE Automation**\n\n**Deployment health gate analysis:**\n\n```\nOur latest deployment is showing performance degradation.\nRun deployment health gate analysis with:\n- Golden signals monitoring (Rate, Errors, Duration, Saturation)\n- SLO/SLI validation with error budget calculations\n- Generate automated rollback recommendation if needed\n```\n\n**Infrastructure as Code remediation:**\n\n```\nGenerate Infrastructure as Code templates to remediate the current alert patterns.\nInclude automated scaling policies and resource optimization recommendations.\n```\n\n### **Deep Transaction Analysis**\n\n**Business logic error investigation:**\n\n```\nOur payment processing is showing intermittent failures.\nPerform advanced transaction analysis:\n- Extract exception details with full stack traces\n- Correlate with deployment events and ArgoCD changes\n- Identify the exact code location causing the issue\n```\n\n**Performance correlation analysis:**\n\n```\nAnalyze the performance impact across our distributed system for the slow checkout flow.\nShow me the complete trace analysis with business context and identify bottlenecks.\n```\n\n### **Traditional Use Cases (Enhanced)**\n\n**Find open vulnerabilities on production, setup alert:**\n\n```\nI have this code snippet here in my IDE, where I get a dependency vulnerability warning for my code.\nCheck if I see any open vulnerability/cve on production.\nAnalyze a specific production problem.\nSetup a workflow that sends Slack alerts to the #devops-alerts channel when availability problems occur.\n```\n\n**Debug intermittent 503 errors:**\n\n```\nOur load balancer is intermittently returning 503 errors during peak traffic.\nPull all recent problems detected for our front-end services and\nrun a query to correlate error rates with service instance health indicators.\nI suspect we have circuit breakers triggering, but need confirmation from the telemetry data.\n```\n\n**Correlate memory issue with logs:**\n\n```\nThere's a problem with high memory usage on one of our hosts.\nGet the problem details and then fetch related logs to help understand\nwhat's causing the memory spike? Which file in this repo is this related to?\n```\n\n**Trace request flow analysis:**\n\n```\nOur users are experiencing slow checkout processes.\nCan you execute a DQL query to show me the full request trace for our checkout flow,\nso I can identify which service is causing the bottleneck?\n```\n\n**Analyze Kubernetes cluster events:**\n\n```\nOur application deployments seem to be failing intermittently.\nCan you fetch recent events from our \"production-cluster\"\nto help identify what might be causing these deployment issues?\n```\n\n## Troubleshooting\n\n### Authentication Issues\n\nIn most cases, authentication issues are related to missing scopes or invalid tokens. Please ensure that you have added all required scopes as listed above.\n\n**For Platform Tokens:**\n\n1. Verify your Platform Token has all the necessary scopes listed in the \"Scopes for Authentication\" section\n2. Ensure your token is valid and not expired\n3. Check that your user has the required permissions in your Dynatrace Environment\n\n**For OAuth Clients:**\nIn case of OAuth-related problems, you can troubleshoot SSO/OAuth issues based on our [Dynatrace Developer Documentation](https://developer.dynatrace.com/develop/access-platform-apis-from-outside/#get-bearer-token-and-call-app-function).\n\nIt is recommended to test access with the following API (which requires minimal scopes `app-engine:apps:run` and `app-engine:functions:run`):\n\n1. Use OAuth Client ID and Secret to retrieve a Bearer Token (only valid for a couple of minutes):\n\n```bash\ncurl --request POST 'https://sso.dynatrace.com/sso/oauth2/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode 'grant_type=client_credentials' \\\n  --data-urlencode 'client_id={your-client-id}' \\\n  --data-urlencode 'client_secret={your-client-secret}' \\\n  --data-urlencode 'scope=app-engine:apps:run app-engine:functions:run'\n```\n\n2. Use `access_token` from the response of the above call as the bearer-token in the next call:\n\n```bash\ncurl -X GET https://abc12345.apps.dynatrace.com/platform/management/v1/environment \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer {your-bearer-token}'\n```\n\n3. You should retrieve a result like this:\n\n```json\n{\n  \"environmentId\": \"abc12345\",\n  \"createTime\": \"2023-01-01T00:10:57.123Z\",\n  \"blockTime\": \"2025-12-07T00:00:00Z\",\n  \"state\": \"ACTIVE\"\n}\n```\n\n### Problem accessing data on Grail\n\nGrail has a dedicated section about permissions in the Dynatrace Docs. Please refer to https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model/assign-permissions-in-grail for more details.\n\n## Telemetry\n\nThe Dynatrace MCP Server includes sending Telemetry Data via Dynatrace OpenKit to help improve the product. This includes:\n\n- Server start events\n- Tool usage (which tools are called, success/failure, execution duration)\n- Error tracking for debugging and improvement\n\n**Privacy and Opt-out:**\n\n- Telemetry is **enabled by default** but can be disabled by setting `DT_MCP_DISABLE_TELEMETRY=true`\n- No sensitive data from your Dynatrace environment is tracked\n- Only anonymous usage statistics and error information are collected\n- Usage statistics and error data are transmitted to Dynatrace’s analytics endpoint\n\n**Configuration options:**\n\n- `DT_MCP_DISABLE_TELEMETRY` (boolean, default: `false`) - Disable Telemetry\n- `DT_MCP_TELEMETRY_APPLICATION_ID` (string, default: `dynatrace-mcp-server`) - Application ID for tracking\n- `DT_MCP_TELEMETRY_ENDPOINT_URL` (string, default: Dynatrace endpoint) - OpenKit endpoint URL\n- `DT_MCP_TELEMETRY_DEVICE_ID` (string, default: auto-generated) - Device identifier for tracking\n\nTo disable usage tracking, add this to your environment:\n\n```bash\nDT_MCP_DISABLE_TELEMETRY=true\n```\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:38Z","updated_at":"2025-10-22T11:20:54Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"npm","identifier":"@dynatrace-oss/dynatrace-mcp-server","version":"latest","runtime_hint":"npx","environment_variables":[{"value":"{dt_environment}","variables":{"dt_environment":{"description":"Dynatrace Platform URL (e.g., https://abc12345.apps.dynatrace.com — not classic live.dynatrace.com).","is_required":true}},"name":"DT_ENVIRONMENT"},{"value":"{dt_platform_token}","variables":{"dt_platform_token":{"description":"Dynatrace Platform Token (recommended auth).","is_required":true,"is_secret":true}},"name":"DT_PLATFORM_TOKEN"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"08afd42e-5082-4f1f-b025-1fdcee7a1be1","is_latest":true,"published_at":"2025-09-09T10:55:22.908413Z","updated_at":"2025-09-09T10:55:22.908413Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Dynatrace","is_in_organization":true,"license":"MIT License","name":"dynatrace-mcp","name_with_owner":"dynatrace-oss/dynatrace-mcp","opengraph_image_url":"https://opengraph.githubassets.com/abc56695e49896b50afabc22ae8548cb8854c1152efcda79882acd61369c8e73/dynatrace-oss/dynatrace-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/58178984?v=4","preferred_image":"https://avatars.githubusercontent.com/u/58178984?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-22T07:02:38Z","readme":"# Dynatrace MCP Server\n\n\u003ch4 align=\"center\"\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp/releases\"\u003e\n    \u003cimg src=\"https://img.shields.io/github/release/dynatrace-oss/dynatrace-mcp\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/LICENSE\"\u003e\n    \u003cimg src=\"https://img.shields.io/badge/license-mit-blue.svg\" alt=\"Dynatrace MCP Server is released under the MIT License\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://www.npmjs.com/package/@dynatrace-oss/dynatrace-mcp-server\"\u003e\n    \u003cimg src=\"https://img.shields.io/npm/dm/@dynatrace-oss/dynatrace-mcp-server?logo=npm\u0026style=flat\u0026color=red\" alt=\"npm\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp\"\u003e\n    \u003cimg src=\"https://img.shields.io/github/stars/dynatrace-oss/dynatrace-mcp\" alt=\"Dynatrace MCP Server Stars on GitHub\" /\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp\"\u003e\n    \u003cimg src=\"https://img.shields.io/github/contributors/dynatrace-oss/dynatrace-mcp?color=green\" alt=\"Dynatrace MCP Server Contributors on GitHub\" /\u003e\n  \u003c/a\u003e\n\u003c/h4\u003e\n\nThe local _Dynatrace MCP server_ allows AI Assistants to interact with the [Dynatrace](https://www.dynatrace.com/) observability platform,\nbringing real-time observability data directly into your development workflow.\n\n\u003e Note: This product is not officially supported by Dynatrace.\n\nIf you need help, please contact us via [GitHub Issues](https://github.com/dynatrace-oss/dynatrace-mcp/issues) if you have feature requests, questions, or need help.\n\nhttps://github.com/user-attachments/assets/25c05db1-8e09-4a7f-add2-ed486ffd4b5a\n\n## Quickstart\n\nYou can add this MCP server to your MCP Client like VSCode, Claude, Cursor, Amazon Q, Windsurf, ChatGPT, or Github Copilot via the command is `npx -y @dynatrace-oss/dynatrace-mcp-server` (type: `stdio`). For more details, please refer to the [configuration section below](#configuration).\n\nFurthermore, you need to configure the URL to a Dynatrace environment:\n\n- `DT_ENVIRONMENT` (string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n\nOnce you are done, we recommend looking into [example prompts](#-example-prompts-), like `Get all details of the entity 'my-service'` or `Show me error logs`. Please mind that these prompts lead to executing DQL statements which may incur [costs](#costs) in accordance to your licence.\n\n## Architecture\n\n![Architecture](https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/assets/dynatrace-mcp-arch.png?raw=true)\n\n## Use cases\n\n- **Real-time observability** - Fetch production-level data for early detection and proactive monitoring\n- **Contextual debugging** - Fix issues with full context from monitored exceptions, logs, and anomalies\n- **Security insights** - Get detailed vulnerability analysis and security problem tracking\n- **Natural language queries** - Use AI-powered DQL generation and explanation\n- **Multi-phase incident investigation** - Systematic 4-phase approach with automated impact assessment\n- **Advanced transaction analysis** - Precise root cause identification with file/line-level accuracy\n- **Cross-data source correlation** - Connect problems → spans → logs with trace ID correlation\n- **DevOps automation** - Deployment health gates with automated promotion/rollback logic\n- **Security compliance monitoring** - Multi-cloud compliance assessment with evidence-based investigation\n\n## Capabilities\n\n- List and get [problem](https://www.dynatrace.com/hub/detail/problems/) details from your services (for example Kubernetes)\n- List and get security problems / [vulnerability](https://www.dynatrace.com/hub/detail/vulnerabilities/) details\n- Execute DQL (Dynatrace Query Language) and retrieve logs, events, spans and metrics\n- Send Slack messages (via Slack Connector)\n- Set up notification Workflow (via Dynatrace [AutomationEngine](https://docs.dynatrace.com/docs/discover-dynatrace/platform/automationengine))\n- Get more information about a monitored entity\n- Get Ownership of an entity\n\n### Costs\n\n**Important:** While this local MCP server is provided for free, using certain capabilities to access data in Dynatrace Grail may incur additional costs based\non your Dynatrace consumption model. This affects `execute_dql` tool and other capabilities that **query** Dynatrace Grail storage, and costs\ndepend on the volume (GB scanned).\n\n**Before using this MCP server extensively, please:**\n\n1. Review your current Dynatrace consumption model and pricing\n2. Understand the cost implications of the specific data you plan to query (logs, events, metrics) - see [Dynatrace Pricing and Rate Card](https://www.dynatrace.com/pricing/)\n3. Start with smaller timeframes (e.g., 12h-24h) and make use of [buckets](https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model#built-in-grail-buckets) to reduce the cost impact\n4. Set an appropriate `DT_GRAIL_QUERY_BUDGET_GB` environment variable (default: 1000 GB) to control and monitor your Grail query consumption\n\n**Grail Budget Tracking:**\n\nThe MCP server includes built-in budget tracking for Grail queries to help you monitor and control costs:\n\n- Set `DT_GRAIL_QUERY_BUDGET_GB` (default: 1000 GB) to define your session budget limit\n- The server tracks bytes scanned across all Grail queries in the current session\n- You'll receive warnings when approaching 80% of your budget\n- Budget exceeded alerts help prevent unexpected high consumption\n- Budget resets when you restart the MCP server session\n\n**To understand costs that occured:**\n\nExecute the following DQL statement in a notebook to see how much bytes have been queried from Grail (Logs, Events, etc...):\n\n```\nfetch dt.system.events\n| filter event.kind == \"QUERY_EXECUTION_EVENT\" and contains(client.client_context, \"dynatrace-mcp\")\n| sort timestamp desc\n| fields timestamp, query_id, query_string, scanned_bytes, table, bucket, user.id, user.email, client.client_context\n| maketimeSeries sum(scanned_bytes), by: { user.email, user.id, table }\n```\n\n### AI-Powered Assistance (Preview)\n\n- **Natural Language to DQL** - Convert plain English queries to Dynatrace Query Language\n- **DQL Explanation** - Get plain English explanations of complex DQL queries\n- **AI Chat Assistant** - Get contextual help and guidance for Dynatrace questions\n- **Feedback System** - Provide feedback to improve AI responses over time\n\n\u003e **Note:** While Davis CoPilot AI is generally available (GA), the Davis CoPilot APIs are currently in preview. For more information, visit the [Davis CoPilot Preview Community](https://dt-url.net/copilot-community).\n\n## Configuration\n\nYou can add this MCP server (using STDIO) to your MCP Client like VS Code, Claude, Cursor, Amazon Q Developer CLI, Windsurf Github Copilot via the package `@dynatrace-oss/dynatrace-mcp-server`.\n\nWe recommend to always set it up for your current workspace instead of using it globally.\n\n**VS Code**\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"cwd\": \"${workspaceFolder}\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"envFile\": \"${workspaceFolder}/.env\"\n    }\n  }\n}\n```\n\nPlease note: In this config, [the `${workspaceFolder}` variable](https://code.visualstudio.com/docs/reference/variables-reference#_predefined-variables) is used.\nThis only works if the config is stored in the current workspaces, e.g., `\u003cyour-repo\u003e/.vscode/mcp.json`. Alternatively, this can also be stored in user-settings, and you can define `env` as follows:\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Claude Desktop**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Amazon Q Developer CLI**\n\nThe [Amazon Q Developer CLI](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) provides an interactive chat experience directly in your terminal. You can ask questions, get help with AWS services, troubleshoot issues, and generate code snippets without leaving your command line environment.\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\nThis configuration should be stored in `\u003cyour-repo\u003e/.amazonq/mcp.json`.\n\n**Google Gemini CLI**\n\nThe [Google Gemini CLI](https://github.com/google-gemini/gemini-cli) is Google's official command-line AI assistant that supports MCP server integration. You can add the Dynatrace MCP server using either the built-in management commands or manual configuration.\n\nUsing `gemini` CLI directly (recommended):\n\n```bash\ngemini extensions install https://github.com/dynatrace-oss/dynatrace-mcp\nexport DT_PLATFORM_TOKEN=... # optional\nexport DT_ENVIRONMENT=https://...\n```\n\nand verify that the server is running via\n\n```bash\ngemini mcp list\n```\n\nOr manually in your `~/.gemini/settings.json` or `.gemini/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace\": {\n      \"command\": \"npx\",\n      \"args\": [\"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_ENVIRONMENT\": \"\"\n      },\n      \"timeout\": 30000,\n      \"trust\": false\n    }\n  }\n}\n```\n\n### HTTP Server Mode (Alternative)\n\nFor scenarios where you need to run the MCP server as an HTTP service instead of using stdio (e.g., for stateful sessions, load balancing, or integration with web clients), you can use the HTTP server mode:\n\n**Running as HTTP server:**\n\n```bash\n# Get help and see all available options\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --help\n\n# Run with HTTP server on default port 3000\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http\n\n# Run with custom port (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --server -p 8080\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --port 3001\n\n# Run with custom host/IP (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --host 127.0.0.1\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http -H 192.168.0.1\n\n# Check version\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --version\n```\n\n**Configuration for MCP clients that support HTTP transport:**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-http\": {\n      \"url\": \"http://localhost:3000\",\n      \"transport\": \"http\"\n    }\n  }\n}\n```\n\n### Rule File\n\nFor efficient result retrieval from Dynatrace, please consider creating a rule file (e.g., [.github/copilot-instructions.md](https://docs.github.com/en/copilot/how-tos/configure-custom-instructions/add-repository-instructions), [.amazonq/rules/](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/context-project-rules.html)), instructing coding agents on how to get more details for your component/app/service. Here is an example for [easytrade](https://github.com/Dynatrace/easytrade), please adapt the names and filters to fit your use-cases and components:\n\n```\n# Observability\n\nWe use Dynatrace as an Observability solution. This document provides instructions on how to get data for easytrade from Dynatrace using DQL.\n\n## How to get any data for my App\n\nDepending on the query and tool used, the following filters can be applied to narrow down results:\n\n* `contains(entity.name, \"easytrade\")`\n* `contains(affected_entity.name, \"easytrade\")`\n* `contains(container.name, \"easytrade\")`\n\nFor best results, you can combine these filters with an `OR` operator.\n\n## Logs\n\nTo fetch logs for easytrade, execute `fetch logs | filter contains(container.name, \"easyatrade\")`.\nFor fetching just error-logs, add `| filter loglevel == \"ERROR\"`.\n```\n\n## Environment Variables\n\n- `DT_ENVIRONMENT` (**required**, string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n- `DT_PLATFORM_TOKEN` (optional, string, e.g., `dt0s16.SAMPLE.abcd1234`) - Dynatrace Platform Token\n- `OAUTH_CLIENT_ID` (optional, string, e.g., `dt0s02.SAMPLE`) - Alternative: Dynatrace OAuth Client ID (for advanced use cases)\n- `OAUTH_CLIENT_SECRET` (optional, string, e.g., `dt0s02.SAMPLE.abcd1234`) - Alternative: Dynatrace OAuth Client Secret (for advanced use cases)\n- `DT_GRAIL_QUERY_BUDGET_GB` (optional, number, default: `1000`) - Budget limit in GB (base 1000) for Grail query bytes scanned per session. The MCP server tracks your Grail usage and warns when approaching or exceeding this limit.\n\nWhen just providing `DT_ENVIRONMENT`, the local MCP server will try to open a browser window to authenticate against the Dynatrace SSO.\n\nFor more information about the other authentication methods, please have a look at the documentation about\n[creating a Platform Token in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/platform-tokens), as well as\n[creating an OAuth Client in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/oauth-clients) for advanced scenarios.\n\nIn addition, depending on the features you use, the following variables can be configured:\n\n- `SLACK_CONNECTION_ID` (string) - connection ID of a [Slack Connection](https://docs.dynatrace.com/docs/analyze-explore-automate/workflows/actions/slack)\n\n### Proxy Configuration\n\nThe MCP server honors system proxy settings for corporate environments:\n\n- `https_proxy` or `HTTPS_PROXY` (optional, string, e.g., `http://proxy.example.com:8080`) - Proxy server URL for HTTPS requests\n- `http_proxy` or `HTTP_PROXY` (optional, string, e.g., `http://proxy.example.com:8080`) - Proxy server URL for HTTP requests\n- `no_proxy` or `NO_PROXY` (optional, string, e.g., `localhost,127.0.0.1,.local`) - Comma-separated list of hostnames or domains that should bypass the proxy\n\n**Note:** The `no_proxy` environment variable is currently logged for informational purposes but not fully enforced by the underlying HTTP client. If you need to bypass the proxy for specific hosts, consider configuring your proxy server to handle these exclusions.\n\nExample configuration with proxy:\n\n```bash\nexport HTTPS_PROXY=http://proxy.company.com:8080\nexport NO_PROXY=localhost,127.0.0.1,.company.local\nexport DT_ENVIRONMENT=https://abc12345.apps.dynatrace.com\n```\n\n### Scopes for Authentication\n\nDepending on the features you are using, the following scopes are needed:\n\n**Available for both Platform Tokens and OAuth Clients:**\n\n- `app-engine:apps:run` - needed for almost all tools\n- `app-engine:functions:run` - needed for for almost all tools\n- `environment-api:entities:read` - for retrieving ownership details from monitored entities (_currently not available for Platform Tokens_)\n- `automation:workflows:read` - read Workflows\n- `automation:workflows:write` - create and update Workflows\n- `automation:workflows:run` - run Workflows\n- `storage:buckets:read` - needed for `execute_dql` tool to read all system data stored on Grail\n- `storage:logs:read` - needed for `execute_dql` tool to read logs for reliability guardian validations\n- `storage:metrics:read` - needed for `execute_dql` tool to read metrics for reliability guardian validations\n- `storage:bizevents:read` - needed for `execute_dql` tool to read bizevents for reliability guardian validations\n- `storage:spans:read` - needed for `execute_dql` tool to read spans from Grail\n- `storage:entities:read` - needed for `execute_dql` tool to read Entities from Grail\n- `storage:events:read` - needed for `execute_dql` tool to read Events from Grail\n- `storage:security.events:read`- needed for `execute_dql` tool to read Security Events from Grail\n- `storage:system:read` - needed for `execute_dql` tool to read System Data from Grail\n- `storage:user.events:read` - needed for `execute_dql` tool to read User events from Grail\n- `storage:user.sessions:read` - needed for `execute_dql` tool to read User sessions from Grail\n- `storage:smartscape:read` - needed for `execute_dql` tool to read Smartscape Data\n- `davis-copilot:conversations:execute` - execute conversational skill (chat with Copilot)\n- `davis-copilot:nl2dql:execute` - execute Davis Copilot Natural Language (NL) to DQL skill\n- `davis-copilot:dql2nl:execute` - execute DQL to Natural Language (NL) skill\n- `email:emails:send` - needed for `send_email` tool to send emails\n- `settings:objects:read` - needed for reading ownership information and Guardians (SRG) from settings\n\n  **Note**: Please ensure that `settings:objects:read` is used, and _not_ the similarly named scope `app-settings:objects:read`.\n\n**Important**: Some features requiring `environment-api:entities:read` will only work with OAuth Clients. For most use cases, Platform Tokens provide all necessary functionality.\n\n## ✨ Example prompts ✨\n\nUse these example prompts as a starting point. Just copy them into your IDE or agent setup, adapt them to your services/stack/architecture,\nand extend them as needed. They're here to help you imagine how real-time observability and automation work together in the MCP context in your IDE.\n\n### **Basic Queries \u0026 AI Assistance**\n\n**Find a monitored entity**\n\n```\nGet all details of the entity 'my-service'\n```\n\n**Find error logs**\n\n```\nShow me error logs\n```\n\n**Write a DQL query from natural language:**\n\n```\nShow me error rates for the payment service in the last hour\n```\n\n**Explain a DQL query:**\n\n```\nWhat does this DQL do?\nfetch logs | filter dt.source_entity == 'SERVICE-123' | summarize count(), by:{severity} | sort count() desc\n```\n\n**Chat with Davis CoPilot:**\n\n```\nHow can I investigate slow database queries in Dynatrace?\n```\n\n**Send email notifications:**\n\n```\nSend an email notification about the incident to the responsible team at team@example.com with CC to manager@example.com\n```\n\n### **Advanced Incident Investigation**\n\n**Multi-phase incident response:**\n\n```\nOur checkout service is experiencing high error rates. Start a systematic 4-phase incident investigation:\n1. Detect and triage the active problems\n2. Assess user impact and affected services\n3. Perform cross-data source analysis (problems → spans → logs)\n4. Identify root cause with file/line-level precision\n```\n\n**Cross-service failure analysis:**\n\n```\nWe have cascading failures across our microservices architecture.\nAnalyze the entity relationships and trace the failure propagation from the initial problem\nthrough all downstream services. Show me the correlation timeline.\n```\n\n### **Security \u0026 Compliance Analysis**\n\n**Latest-scan vulnerability assessment:**\n\n```\nPerform a comprehensive security analysis using the latest scan data:\n- Check for new vulnerabilities in our production environment\n- Focus on critical and high-severity findings\n- Provide evidence-based remediation paths\n- Generate risk scores with team-specific guidance\n```\n\n**Multi-cloud compliance monitoring:**\n\n```\nRun a compliance assessment across our AWS, Azure, and Kubernetes environments.\nCheck for configuration drift and security posture changes in the last 24 hours.\n```\n\n### **DevOps \u0026 SRE Automation**\n\n**Deployment health gate analysis:**\n\n```\nOur latest deployment is showing performance degradation.\nRun deployment health gate analysis with:\n- Golden signals monitoring (Rate, Errors, Duration, Saturation)\n- SLO/SLI validation with error budget calculations\n- Generate automated rollback recommendation if needed\n```\n\n**Infrastructure as Code remediation:**\n\n```\nGenerate Infrastructure as Code templates to remediate the current alert patterns.\nInclude automated scaling policies and resource optimization recommendations.\n```\n\n### **Deep Transaction Analysis**\n\n**Business logic error investigation:**\n\n```\nOur payment processing is showing intermittent failures.\nPerform advanced transaction analysis:\n- Extract exception details with full stack traces\n- Correlate with deployment events and ArgoCD changes\n- Identify the exact code location causing the issue\n```\n\n**Performance correlation analysis:**\n\n```\nAnalyze the performance impact across our distributed system for the slow checkout flow.\nShow me the complete trace analysis with business context and identify bottlenecks.\n```\n\n### **Traditional Use Cases (Enhanced)**\n\n**Find open vulnerabilities on production, setup alert:**\n\n```\nI have this code snippet here in my IDE, where I get a dependency vulnerability warning for my code.\nCheck if I see any open vulnerability/cve on production.\nAnalyze a specific production problem.\nSetup a workflow that sends Slack alerts to the #devops-alerts channel when availability problems occur.\n```\n\n**Debug intermittent 503 errors:**\n\n```\nOur load balancer is intermittently returning 503 errors during peak traffic.\nPull all recent problems detected for our front-end services and\nrun a query to correlate error rates with service instance health indicators.\nI suspect we have circuit breakers triggering, but need confirmation from the telemetry data.\n```\n\n**Correlate memory issue with logs:**\n\n```\nThere's a problem with high memory usage on one of our hosts.\nGet the problem details and then fetch related logs to help understand\nwhat's causing the memory spike? Which file in this repo is this related to?\n```\n\n**Trace request flow analysis:**\n\n```\nOur users are experiencing slow checkout processes.\nCan you execute a DQL query to show me the full request trace for our checkout flow,\nso I can identify which service is causing the bottleneck?\n```\n\n**Analyze Kubernetes cluster events:**\n\n```\nOur application deployments seem to be failing intermittently.\nCan you fetch recent events from our \"production-cluster\"\nto help identify what might be causing these deployment issues?\n```\n\n## Troubleshooting\n\n### Authentication Issues\n\nIn most cases, authentication issues are related to missing scopes or invalid tokens. Please ensure that you have added all required scopes as listed above.\n\n**For Platform Tokens:**\n\n1. Verify your Platform Token has all the necessary scopes listed in the \"Scopes for Authentication\" section\n2. Ensure your token is valid and not expired\n3. Check that your user has the required permissions in your Dynatrace Environment\n\n**For OAuth Clients:**\nIn case of OAuth-related problems, you can troubleshoot SSO/OAuth issues based on our [Dynatrace Developer Documentation](https://developer.dynatrace.com/develop/access-platform-apis-from-outside/#get-bearer-token-and-call-app-function).\n\nIt is recommended to test access with the following API (which requires minimal scopes `app-engine:apps:run` and `app-engine:functions:run`):\n\n1. Use OAuth Client ID and Secret to retrieve a Bearer Token (only valid for a couple of minutes):\n\n```bash\ncurl --request POST 'https://sso.dynatrace.com/sso/oauth2/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode 'grant_type=client_credentials' \\\n  --data-urlencode 'client_id={your-client-id}' \\\n  --data-urlencode 'client_secret={your-client-secret}' \\\n  --data-urlencode 'scope=app-engine:apps:run app-engine:functions:run'\n```\n\n2. Use `access_token` from the response of the above call as the bearer-token in the next call:\n\n```bash\ncurl -X GET https://abc12345.apps.dynatrace.com/platform/management/v1/environment \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer {your-bearer-token}'\n```\n\n3. You should retrieve a result like this:\n\n```json\n{\n  \"environmentId\": \"abc12345\",\n  \"createTime\": \"2023-01-01T00:10:57.123Z\",\n  \"blockTime\": \"2025-12-07T00:00:00Z\",\n  \"state\": \"ACTIVE\"\n}\n```\n\n### Problem accessing data on Grail\n\nGrail has a dedicated section about permissions in the Dynatrace Docs. Please refer to https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model/assign-permissions-in-grail for more details.\n\n## Telemetry\n\nThe Dynatrace MCP Server includes sending Telemetry Data via Dynatrace OpenKit to help improve the product. This includes:\n\n- Server start events\n- Tool usage (which tools are called, success/failure, execution duration)\n- Error tracking for debugging and improvement\n\n**Privacy and Opt-out:**\n\n- Telemetry is **enabled by default** but can be disabled by setting `DT_MCP_DISABLE_TELEMETRY=true`\n- No sensitive data from your Dynatrace environment is tracked\n- Only anonymous usage statistics and error information are collected\n- Usage statistics and error data are transmitted to Dynatrace’s analytics endpoint\n\n**Configuration options:**\n\n- `DT_MCP_DISABLE_TELEMETRY` (boolean, default: `false`) - Disable Telemetry\n- `DT_MCP_TELEMETRY_APPLICATION_ID` (string, default: `dynatrace-mcp-server`) - Application ID for tracking\n- `DT_MCP_TELEMETRY_ENDPOINT_URL` (string, default: Dynatrace endpoint) - OpenKit endpoint URL\n- `DT_MCP_TELEMETRY_DEVICE_ID` (string, default: auto-generated) - Device identifier for tracking\n\nTo disable usage tracking, add this to your environment:\n\n```bash\nDT_MCP_DISABLE_TELEMETRY=true\n```\n","stargazer_count":162,"topics":["claude","cline","dynatrace","mcp","monitoring","observability","copilot"],"uses_custom_opengraph_image":false}}}},{"name":"doist/todoist-ai","description":"A set of tools to connect to AI agents, to allow them to use Todoist on a user's behalf.","status":"active","repository":{"url":"https://github.com/Doist/todoist-ai","source":"github","id":"987716578","readme":"# Todoist AI and MCP SDK\n\nLibrary for connecting AI agents to Todoist. Includes tools that can be integrated into LLMs,\nenabling them to access and modify a Todoist account on the user's behalf.\n\nThese tools can be used both through an MCP server, or imported directly in other projects to\nintegrate them to your own AI conversational interfaces.\n\n## Using tools\n\n### 1. Add this repository as a dependency\n\n```sh\nnpm install @doist/todoist-ai\n```\n\n### 2. Import the tools and plug them to an AI\n\nHere's an example using [Vercel's AI SDK](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#streamtext).\n\n```js\nimport { findTasksByDate, addTasks } from \"@doist/todoist-ai\";\nimport { streamText } from \"ai\";\n\nconst result = streamText({\n    model: yourModel,\n    system: \"You are a helpful Todoist assistant\",\n    tools: {\n        findTasksByDate,\n        addTasks,\n    },\n});\n```\n\n## Using as an MCP server\n\n### Quick Start\n\nYou can run the MCP server directly with npx:\n\n```bash\nnpx @doist/todoist-ai\n```\n\n### Setup Guide\n\nThe Todoist AI MCP server is available as a streamable HTTP service for easy integration with various AI clients:\n\n**Primary URL (Streamable HTTP):** `https://ai.todoist.net/mcp`\n\n#### Claude Desktop\n\n1. Open Settings → Connectors → Add custom connector\n2. Enter `https://ai.todoist.net/mcp` and complete OAuth authentication\n\n#### Cursor\n\nCreate a configuration file:\n- **Global:** `~/.cursor/mcp.json`\n- **Project-specific:** `.cursor/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"todoist\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://ai.todoist.net/mcp\"]\n    }\n  }\n}\n```\n\nThen enable the server in Cursor settings if prompted.\n\n#### Claude Code (CLI)\n\nFirstly configure Claude so it has a new MCP available using this command:\n\n```bash\nclaude mcp add --transport http todoist https://ai.todoist.net/mcp\n```\n\nThen launch `claude`, execute `/mcp`, then select the `todoist` MCP server.\n\nThis will take you through a wizard to authenticate using your browser with Todoist. Once complete you will be able to use todoist in `claude`.\n\n\n#### Visual Studio Code\n\n1. Open Command Palette → MCP: Add Server\n2. Select HTTP transport and use:\n\n```json\n{\n  \"servers\": {\n    \"todoist\": {\n      \"type\": \"http\",\n      \"url\": \"https://ai.todoist.net/mcp\"\n    }\n  }\n}\n```\n\n#### Other MCP Clients\n\n```bash\nnpx -y mcp-remote https://ai.todoist.net/mcp\n```\n\nFor more details on setting up and using the MCP server, including creating custom servers, see [docs/mcp-server.md](docs/mcp-server.md).\n\n## Features\n\nA key feature of this project is that tools can be reused, and are not written specifically for use in an MCP server. They can be hooked up as tools to other conversational AI interfaces (e.g. Vercel's AI SDK).\n\nThis project is in its early stages. Expect more and/or better tools soon.\n\nNevertheless, our goal is to provide a small set of tools that enable complete workflows, rather than just atomic actions, striking a balance between flexibility and efficiency for LLMs.\n\nFor our design philosophy, guidelines, and development patterns, see [docs/tool-design.md](docs/tool-design.md).\n\n### Available Tools\n\nFor a complete list of available tools, see the [src/tools](src/tools) directory.\n\n#### OpenAI MCP Compatibility\n\nThis server includes `search` and `fetch` tools that follow the [OpenAI MCP specification](https://platform.openai.com/docs/mcp), enabling seamless integration with OpenAI's MCP protocol. These tools return JSON-encoded results optimized for OpenAI's requirements while maintaining compatibility with the broader MCP ecosystem.\n\n## Dependencies\n\n-   MCP server using the official [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/typescript-sdk?tab=readme-ov-file#installation)\n-   Todoist Typescript API client [@doist/todoist-api-typescript](https://github.com/Doist/todoist-api-typescript)\n\n## MCP Server Setup\n\nSee [docs/mcp-server.md](docs/mcp-server.md) for full instructions on setting up the MCP server.\n\n## Local Development Setup\n\nSee [docs/dev-setup.md](docs/dev-setup.md) for full instructions on setting up this repository locally for development and contributing.\n\n### Quick Start\n\nAfter cloning and setting up the repository:\n\n- `npm start` - Build and run the MCP inspector for testing\n- `npm run dev` - Development mode with auto-rebuild and restart\n\n## Releasing\n\nThis project uses [release-please](https://github.com/googleapis/release-please) to automate version management and package publishing.\n\n### How it works\n\n1. Make your changes using [Conventional Commits](https://www.conventionalcommits.org/):\n\n    - `feat:` for new features (minor version bump)\n    - `fix:` for bug fixes (patch version bump)\n    - `feat!:` or `fix!:` for breaking changes (major version bump)\n    - `docs:` for documentation changes\n    - `chore:` for maintenance tasks\n    - `ci:` for CI changes\n\n2. When commits are pushed to `main`:\n\n    - Release-please automatically creates/updates a release PR\n    - The PR includes version bump and changelog updates\n    - Review the PR and merge when ready\n\n3. After merging the release PR:\n    - A new GitHub release is automatically created\n    - A new tag is created\n    - The `publish` workflow is triggered\n    - The package is published to npm\n"},"version":"0.0.1-seed","created_at":"2025-09-25T18:54:14Z","updated_at":"2025-10-22T11:21:52Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://ai.todoist.net/mcp"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"63b1980f-487d-4ad6-8242-f54ad3b3181c","is_latest":true,"published_at":"2025-09-09T10:55:22.907061Z","updated_at":"2025-09-09T10:55:22.907061Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Todoist","is_in_organization":true,"license":"MIT License","name":"todoist-ai","name_with_owner":"Doist/todoist-ai","opengraph_image_url":"https://opengraph.githubassets.com/e891cfa1d86958b120708aabf416c29ab37f85e05954b495b4eef73280692a8c/Doist/todoist-ai","owner_avatar_url":"https://avatars.githubusercontent.com/u/2565372?v=4","preferred_image":"https://avatars.githubusercontent.com/u/2565372?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-22T10:17:17Z","readme":"# Todoist AI and MCP SDK\n\nLibrary for connecting AI agents to Todoist. Includes tools that can be integrated into LLMs,\nenabling them to access and modify a Todoist account on the user's behalf.\n\nThese tools can be used both through an MCP server, or imported directly in other projects to\nintegrate them to your own AI conversational interfaces.\n\n## Using tools\n\n### 1. Add this repository as a dependency\n\n```sh\nnpm install @doist/todoist-ai\n```\n\n### 2. Import the tools and plug them to an AI\n\nHere's an example using [Vercel's AI SDK](https://ai-sdk.dev/docs/ai-sdk-core/generating-text#streamtext).\n\n```js\nimport { findTasksByDate, addTasks } from \"@doist/todoist-ai\";\nimport { streamText } from \"ai\";\n\nconst result = streamText({\n    model: yourModel,\n    system: \"You are a helpful Todoist assistant\",\n    tools: {\n        findTasksByDate,\n        addTasks,\n    },\n});\n```\n\n## Using as an MCP server\n\n### Quick Start\n\nYou can run the MCP server directly with npx:\n\n```bash\nnpx @doist/todoist-ai\n```\n\n### Setup Guide\n\nThe Todoist AI MCP server is available as a streamable HTTP service for easy integration with various AI clients:\n\n**Primary URL (Streamable HTTP):** `https://ai.todoist.net/mcp`\n\n#### Claude Desktop\n\n1. Open Settings → Connectors → Add custom connector\n2. Enter `https://ai.todoist.net/mcp` and complete OAuth authentication\n\n#### Cursor\n\nCreate a configuration file:\n- **Global:** `~/.cursor/mcp.json`\n- **Project-specific:** `.cursor/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"todoist\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://ai.todoist.net/mcp\"]\n    }\n  }\n}\n```\n\nThen enable the server in Cursor settings if prompted.\n\n#### Claude Code (CLI)\n\nFirstly configure Claude so it has a new MCP available using this command:\n\n```bash\nclaude mcp add --transport http todoist https://ai.todoist.net/mcp\n```\n\nThen launch `claude`, execute `/mcp`, then select the `todoist` MCP server.\n\nThis will take you through a wizard to authenticate using your browser with Todoist. Once complete you will be able to use todoist in `claude`.\n\n\n#### Visual Studio Code\n\n1. Open Command Palette → MCP: Add Server\n2. Select HTTP transport and use:\n\n```json\n{\n  \"servers\": {\n    \"todoist\": {\n      \"type\": \"http\",\n      \"url\": \"https://ai.todoist.net/mcp\"\n    }\n  }\n}\n```\n\n#### Other MCP Clients\n\n```bash\nnpx -y mcp-remote https://ai.todoist.net/mcp\n```\n\nFor more details on setting up and using the MCP server, including creating custom servers, see [docs/mcp-server.md](docs/mcp-server.md).\n\n## Features\n\nA key feature of this project is that tools can be reused, and are not written specifically for use in an MCP server. They can be hooked up as tools to other conversational AI interfaces (e.g. Vercel's AI SDK).\n\nThis project is in its early stages. Expect more and/or better tools soon.\n\nNevertheless, our goal is to provide a small set of tools that enable complete workflows, rather than just atomic actions, striking a balance between flexibility and efficiency for LLMs.\n\nFor our design philosophy, guidelines, and development patterns, see [docs/tool-design.md](docs/tool-design.md).\n\n### Available Tools\n\nFor a complete list of available tools, see the [src/tools](src/tools) directory.\n\n#### OpenAI MCP Compatibility\n\nThis server includes `search` and `fetch` tools that follow the [OpenAI MCP specification](https://platform.openai.com/docs/mcp), enabling seamless integration with OpenAI's MCP protocol. These tools return JSON-encoded results optimized for OpenAI's requirements while maintaining compatibility with the broader MCP ecosystem.\n\n## Dependencies\n\n-   MCP server using the official [@modelcontextprotocol/sdk](https://github.com/modelcontextprotocol/typescript-sdk?tab=readme-ov-file#installation)\n-   Todoist Typescript API client [@doist/todoist-api-typescript](https://github.com/Doist/todoist-api-typescript)\n\n## MCP Server Setup\n\nSee [docs/mcp-server.md](docs/mcp-server.md) for full instructions on setting up the MCP server.\n\n## Local Development Setup\n\nSee [docs/dev-setup.md](docs/dev-setup.md) for full instructions on setting up this repository locally for development and contributing.\n\n### Quick Start\n\nAfter cloning and setting up the repository:\n\n- `npm start` - Build and run the MCP inspector for testing\n- `npm run dev` - Development mode with auto-rebuild and restart\n\n## Releasing\n\nThis project uses [release-please](https://github.com/googleapis/release-please) to automate version management and package publishing.\n\n### How it works\n\n1. Make your changes using [Conventional Commits](https://www.conventionalcommits.org/):\n\n    - `feat:` for new features (minor version bump)\n    - `fix:` for bug fixes (patch version bump)\n    - `feat!:` or `fix!:` for breaking changes (major version bump)\n    - `docs:` for documentation changes\n    - `chore:` for maintenance tasks\n    - `ci:` for CI changes\n\n2. When commits are pushed to `main`:\n\n    - Release-please automatically creates/updates a release PR\n    - The PR includes version bump and changelog updates\n    - Review the PR and merge when ready\n\n3. After merging the release PR:\n    - A new GitHub release is automatically created\n    - A new tag is created\n    - The `publish` workflow is triggered\n    - The package is published to npm\n","stargazer_count":144,"uses_custom_opengraph_image":false}}}},{"name":"pydantic/logfire-mcp","description":"Provides access to OpenTelemetry traces and metrics through Logfire.","status":"active","repository":{"url":"https://github.com/pydantic/logfire-mcp","source":"github","id":"943883428","readme":"\u003c!-- DO NOT MODIFY THIS FILE DIRECTLY, IT IS GENERATED BY THE TESTS! --\u003e\n\n# Pydantic Logfire MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server with tools that can access the OpenTelemetry traces and\nmetrics you've sent to Pydantic Logfire.\n\n\u003ca href=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp\"\u003e\n  \u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp/badge\" alt=\"Pydantic Logfire Server MCP server\" /\u003e\n\u003c/a\u003e\n\nThis MCP server enables LLMs to retrieve your application's telemetry data, analyze distributed\ntraces, and make use of the results of arbitrary SQL queries executed using the Pydantic Logfire APIs.\n\n## Available Tools\n\n* `find_exceptions_in_file` - Get the details about the 10 most recent exceptions on the file.\n  * Arguments:\n    * `filepath` (string) - The path to the file to find exceptions in.\n    * `age` (integer) - Number of minutes to look back, e.g. 30 for last 30 minutes. Maximum allowed value is 7 days.\n\n* `arbitrary_query` - Run an arbitrary query on the Pydantic Logfire database.\n  * Arguments:\n    * `query` (string) - The query to run, as a SQL string.\n    * `age` (integer) - Number of minutes to look back, e.g. 30 for last 30 minutes. Maximum allowed value is 7 days.\n\n* `logfire_link` - Creates a link to help the user to view the trace in the Logfire UI.\n  * Arguments:\n    * `trace_id` (string) - The trace ID to link to.\n\n* `schema_reference` - The database schema for the Logfire DataFusion database.\n\n\n## Setup\n\n### Install `uv`\n\nThe first thing to do is make sure `uv` is installed, as `uv` is used to run the MCP server.\n\nFor installation instructions, see the [`uv` installation docs](https://docs.astral.sh/uv/getting-started/installation/).\n\nIf you already have an older version of `uv` installed, you might need to update it with `uv self update`.\n\n### Obtain a Pydantic Logfire read token\nIn order to make requests to the Pydantic Logfire APIs, the Pydantic Logfire MCP server requires a \"read token\".\n\nYou can create one under the \"Read Tokens\" section of your project settings in Pydantic Logfire:\nhttps://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n\u003e [!IMPORTANT]\n\u003e Pydantic Logfire read tokens are project-specific, so you need to create one for the specific project you want to expose to the Pydantic Logfire MCP server.\n\n### Manually run the server\n\nOnce you have `uv` installed and have a Pydantic Logfire read token, you can manually run the MCP server using `uvx` (which is provided by `uv`).\n\nYou can specify your read token using the `LOGFIRE_READ_TOKEN` environment variable:\n\n```bash\nLOGFIRE_READ_TOKEN=YOUR_READ_TOKEN uvx logfire-mcp@latest\n```\n\nYou can also set `LOGFIRE_READ_TOKEN` in a `.env` file:\n\n```bash\nLOGFIRE_READ_TOKEN=pylf_v1_us_...\n```\n\n**NOTE:** for this to work, the MCP server needs to run with the directory containing the `.env` file in its working directory.\n\nor using the `--read-token` flag:\n\n```bash\nuvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n\u003e [!NOTE]\n\u003e If you are using Cursor, Claude Desktop, Cline, or other MCP clients that manage your MCP servers for you, you **_do\n    NOT_** need to manually run the server yourself. The next section will show you how to configure these clients to make\n    use of the Pydantic Logfire MCP server.\n\n### Base URL\n\nIf you are running Logfire in a self hosted environment, you need to specify the base URL.\nThis can be done using the `LOGFIRE_BASE_URL` environment variable:\n\n```bash\nLOGFIRE_BASE_URL=https://logfire.my-company.com uvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n\nYou can also use the `--base-url` argument:\n\n```bash\nuvx logfire-mcp@latest --base-url=https://logfire.my-company.com --read-token=YOUR_READ_TOKEN\n```\n\n## Configuration with well-known MCP clients\n\n### Configure for Cursor\n\nCreate a `.cursor/mcp.json` file in your project root:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\", \"--read-token=YOUR-TOKEN\"]\n    }\n  }\n}\n```\n\nThe Cursor doesn't accept the `env` field, so you need to use the `--read-token` flag instead.\n\n### Configure for Claude code\n\nRun the following command:\n\n```bash\nclaude mcp add logfire -e LOGFIRE_READ_TOKEN=YOUR_TOKEN -- uvx logfire-mcp@latest\n```\n\n### Configure for Claude Desktop\n\nAdd to your Claude settings:\n\n```json\n{\n  \"command\": [\"uvx\"],\n  \"args\": [\"logfire-mcp@latest\"],\n  \"type\": \"stdio\",\n  \"env\": {\n    \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n  }\n}\n```\n\n### Configure for Cline\n\nAdd to your Cline settings in `cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Configure for VS Code\n\nMake sure you [enabled MCP support in VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_enable-mcp-support-in-vs-code).\n\nCreate a `.vscode/mcp.json` file in your project's root directory:\n\n```json\n{\n  \"servers\": {\n    \"logfire\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\", // or the absolute /path/to/uvx\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n### Configure for Zed\n\nCreate a `.zed/settings.json` file in your project's root directory:\n\n```json\n{\n  \"context_servers\": {\n    \"logfire\": {\n      \"source\": \"custom\",\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"enabled\": true\n    }\n  }\n}\n```\n\n## Example Interactions\n\n1. Get details about exceptions from traces in a specific file:\n```json\n{\n  \"name\": \"find_exceptions_in_file\",\n  \"arguments\": {\n    \"filepath\": \"app/api.py\",\n    \"age\": 1440\n  }\n}\n```\n\nResponse:\n```json\n[\n  {\n    \"created_at\": \"2024-03-20T10:30:00Z\",\n    \"message\": \"Failed to process request\",\n    \"exception_type\": \"ValueError\",\n    \"exception_message\": \"Invalid input format\",\n    \"function_name\": \"process_request\",\n    \"line_number\": \"42\",\n    \"attributes\": {\n      \"service.name\": \"api-service\",\n      \"code.filepath\": \"app/api.py\"\n    },\n    \"trace_id\": \"1234567890abcdef\"\n  }\n]\n```\n\n2. Run a custom query on traces:\n```json\n{\n  \"name\": \"arbitrary_query\",\n  \"arguments\": {\n    \"query\": \"SELECT trace_id, message, created_at, attributes-\u003e\u003e'service.name' as service FROM records WHERE severity_text = 'ERROR' ORDER BY created_at DESC LIMIT 10\",\n    \"age\": 1440\n  }\n}\n```\n\n## Examples of Questions for Claude\n\n1. \"What exceptions occurred in traces from the last hour across all services?\"\n2. \"Show me the recent errors in the file 'app/api.py' with their trace context\"\n3. \"How many errors were there in the last 24 hours per service?\"\n4. \"What are the most common exception types in my traces, grouped by service name?\"\n5. \"Get me the OpenTelemetry schema for traces and metrics\"\n6. \"Find all errors from yesterday and show their trace contexts\"\n\n## Getting Started\n\n1. First, obtain a Pydantic Logfire read token from:\n   https://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n2. Run the MCP server:\n   ```bash\n   uvx logfire-mcp@latest --read-token=YOUR_TOKEN\n   ```\n\n3. Configure your preferred client (Cursor, Claude Desktop, or Cline) using the configuration examples above\n\n4. Start using the MCP server to analyze your OpenTelemetry traces and metrics!\n\n## Contributing\n\nWe welcome contributions to help improve the Pydantic Logfire MCP server. Whether you want to add new trace analysis tools, enhance metrics querying functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see the [Model Context Protocol servers repository](https://github.com/modelcontextprotocol/servers).\n\n## License\n\nPydantic Logfire MCP is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n"},"version":"1.0.0","created_at":"2025-09-23T17:40:36Z","updated_at":"2025-10-22T11:20:53Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"pypi","identifier":"logfire-mcp","version":"v0.6.0","runtime_hint":"uvx","package_arguments":[{"is_required":true,"format":"string","value":"{logfire_token}","variables":{"logfire_token":{"description":"Pydantic Logfire read token (project-specific)","is_required":true,"is_secret":true}},"type":"named","name":"--read-token","value_hint":"flag"},{"format":"string","value":"{logfire_base_url}","variables":{"logfire_base_url":{"description":"Self-hosted Logfire base URL (omit for cloud)"}},"type":"named","name":"--base-url","value_hint":"flag"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"89ee8051-ae2e-49b6-8372-d9d8f5133d9c","is_latest":true,"published_at":"2025-09-09T10:55:22.909183Z","updated_at":"2025-09-09T10:55:22.909183Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Logfire","is_in_organization":true,"license":"MIT License","name":"logfire-mcp","name_with_owner":"pydantic/logfire-mcp","opengraph_image_url":"https://opengraph.githubassets.com/2b3b088288e3e3d0ed02d100c91e0d11acd09b9c0f6bc5213759fc18de1d7228/pydantic/logfire-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/110818415?v=4","preferred_image":"https://avatars.githubusercontent.com/u/110818415?v=4","primary_language":"Python","primary_language_color":"#3572A5","pushed_at":"2025-09-24T22:55:15Z","readme":"\u003c!-- DO NOT MODIFY THIS FILE DIRECTLY, IT IS GENERATED BY THE TESTS! --\u003e\n\n# Pydantic Logfire MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server with tools that can access the OpenTelemetry traces and\nmetrics you've sent to Pydantic Logfire.\n\n\u003ca href=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp\"\u003e\n  \u003cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp/badge\" alt=\"Pydantic Logfire Server MCP server\" /\u003e\n\u003c/a\u003e\n\nThis MCP server enables LLMs to retrieve your application's telemetry data, analyze distributed\ntraces, and make use of the results of arbitrary SQL queries executed using the Pydantic Logfire APIs.\n\n## Available Tools\n\n* `find_exceptions_in_file` - Get the details about the 10 most recent exceptions on the file.\n  * Arguments:\n    * `filepath` (string) - The path to the file to find exceptions in.\n    * `age` (integer) - Number of minutes to look back, e.g. 30 for last 30 minutes. Maximum allowed value is 7 days.\n\n* `arbitrary_query` - Run an arbitrary query on the Pydantic Logfire database.\n  * Arguments:\n    * `query` (string) - The query to run, as a SQL string.\n    * `age` (integer) - Number of minutes to look back, e.g. 30 for last 30 minutes. Maximum allowed value is 7 days.\n\n* `logfire_link` - Creates a link to help the user to view the trace in the Logfire UI.\n  * Arguments:\n    * `trace_id` (string) - The trace ID to link to.\n\n* `schema_reference` - The database schema for the Logfire DataFusion database.\n\n\n## Setup\n\n### Install `uv`\n\nThe first thing to do is make sure `uv` is installed, as `uv` is used to run the MCP server.\n\nFor installation instructions, see the [`uv` installation docs](https://docs.astral.sh/uv/getting-started/installation/).\n\nIf you already have an older version of `uv` installed, you might need to update it with `uv self update`.\n\n### Obtain a Pydantic Logfire read token\nIn order to make requests to the Pydantic Logfire APIs, the Pydantic Logfire MCP server requires a \"read token\".\n\nYou can create one under the \"Read Tokens\" section of your project settings in Pydantic Logfire:\nhttps://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n\u003e [!IMPORTANT]\n\u003e Pydantic Logfire read tokens are project-specific, so you need to create one for the specific project you want to expose to the Pydantic Logfire MCP server.\n\n### Manually run the server\n\nOnce you have `uv` installed and have a Pydantic Logfire read token, you can manually run the MCP server using `uvx` (which is provided by `uv`).\n\nYou can specify your read token using the `LOGFIRE_READ_TOKEN` environment variable:\n\n```bash\nLOGFIRE_READ_TOKEN=YOUR_READ_TOKEN uvx logfire-mcp@latest\n```\n\nYou can also set `LOGFIRE_READ_TOKEN` in a `.env` file:\n\n```bash\nLOGFIRE_READ_TOKEN=pylf_v1_us_...\n```\n\n**NOTE:** for this to work, the MCP server needs to run with the directory containing the `.env` file in its working directory.\n\nor using the `--read-token` flag:\n\n```bash\nuvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n\u003e [!NOTE]\n\u003e If you are using Cursor, Claude Desktop, Cline, or other MCP clients that manage your MCP servers for you, you **_do\n    NOT_** need to manually run the server yourself. The next section will show you how to configure these clients to make\n    use of the Pydantic Logfire MCP server.\n\n### Base URL\n\nIf you are running Logfire in a self hosted environment, you need to specify the base URL.\nThis can be done using the `LOGFIRE_BASE_URL` environment variable:\n\n```bash\nLOGFIRE_BASE_URL=https://logfire.my-company.com uvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n\nYou can also use the `--base-url` argument:\n\n```bash\nuvx logfire-mcp@latest --base-url=https://logfire.my-company.com --read-token=YOUR_READ_TOKEN\n```\n\n## Configuration with well-known MCP clients\n\n### Configure for Cursor\n\nCreate a `.cursor/mcp.json` file in your project root:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\", \"--read-token=YOUR-TOKEN\"]\n    }\n  }\n}\n```\n\nThe Cursor doesn't accept the `env` field, so you need to use the `--read-token` flag instead.\n\n### Configure for Claude code\n\nRun the following command:\n\n```bash\nclaude mcp add logfire -e LOGFIRE_READ_TOKEN=YOUR_TOKEN -- uvx logfire-mcp@latest\n```\n\n### Configure for Claude Desktop\n\nAdd to your Claude settings:\n\n```json\n{\n  \"command\": [\"uvx\"],\n  \"args\": [\"logfire-mcp@latest\"],\n  \"type\": \"stdio\",\n  \"env\": {\n    \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n  }\n}\n```\n\n### Configure for Cline\n\nAdd to your Cline settings in `cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Configure for VS Code\n\nMake sure you [enabled MCP support in VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_enable-mcp-support-in-vs-code).\n\nCreate a `.vscode/mcp.json` file in your project's root directory:\n\n```json\n{\n  \"servers\": {\n    \"logfire\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\", // or the absolute /path/to/uvx\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n### Configure for Zed\n\nCreate a `.zed/settings.json` file in your project's root directory:\n\n```json\n{\n  \"context_servers\": {\n    \"logfire\": {\n      \"source\": \"custom\",\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"enabled\": true\n    }\n  }\n}\n```\n\n## Example Interactions\n\n1. Get details about exceptions from traces in a specific file:\n```json\n{\n  \"name\": \"find_exceptions_in_file\",\n  \"arguments\": {\n    \"filepath\": \"app/api.py\",\n    \"age\": 1440\n  }\n}\n```\n\nResponse:\n```json\n[\n  {\n    \"created_at\": \"2024-03-20T10:30:00Z\",\n    \"message\": \"Failed to process request\",\n    \"exception_type\": \"ValueError\",\n    \"exception_message\": \"Invalid input format\",\n    \"function_name\": \"process_request\",\n    \"line_number\": \"42\",\n    \"attributes\": {\n      \"service.name\": \"api-service\",\n      \"code.filepath\": \"app/api.py\"\n    },\n    \"trace_id\": \"1234567890abcdef\"\n  }\n]\n```\n\n2. Run a custom query on traces:\n```json\n{\n  \"name\": \"arbitrary_query\",\n  \"arguments\": {\n    \"query\": \"SELECT trace_id, message, created_at, attributes-\u003e\u003e'service.name' as service FROM records WHERE severity_text = 'ERROR' ORDER BY created_at DESC LIMIT 10\",\n    \"age\": 1440\n  }\n}\n```\n\n## Examples of Questions for Claude\n\n1. \"What exceptions occurred in traces from the last hour across all services?\"\n2. \"Show me the recent errors in the file 'app/api.py' with their trace context\"\n3. \"How many errors were there in the last 24 hours per service?\"\n4. \"What are the most common exception types in my traces, grouped by service name?\"\n5. \"Get me the OpenTelemetry schema for traces and metrics\"\n6. \"Find all errors from yesterday and show their trace contexts\"\n\n## Getting Started\n\n1. First, obtain a Pydantic Logfire read token from:\n   https://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n2. Run the MCP server:\n   ```bash\n   uvx logfire-mcp@latest --read-token=YOUR_TOKEN\n   ```\n\n3. Configure your preferred client (Cursor, Claude Desktop, or Cline) using the configuration examples above\n\n4. Start using the MCP server to analyze your OpenTelemetry traces and metrics!\n\n## Contributing\n\nWe welcome contributions to help improve the Pydantic Logfire MCP server. Whether you want to add new trace analysis tools, enhance metrics querying functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see the [Model Context Protocol servers repository](https://github.com/modelcontextprotocol/servers).\n\n## License\n\nPydantic Logfire MCP is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n","stargazer_count":117,"uses_custom_opengraph_image":false}}}},{"name":"evalstate/hf-mcp-server","description":"Access Hugging Face models, datasets, Spaces, papers, collections via MCP.","status":"active","repository":{"url":"https://github.com/evalstate/hf-mcp-server","source":"github","id":"978636614","readme":"# Hugging Face Official MCP Server \n\n\u003cimg src='https://github.com/evalstate/hf-mcp-server/blob/main/hf-logo.svg' width='100'\u003e\n\nWelcome to the official Hugging Face MCP Server 🤗. Connect your LLM to the Hugging Face Hub and thousands of Gradio AI Applications.\n\n## Installing the MCP Server\n\nFollow the instructions below to get started:\n\n\u003cdetails\u003e\n\u003csummary\u003eInstall in \u003cb\u003eClaude Desktop\u003c/b\u003e or \u003cb\u003eclaude.ai\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nClick [here](https://claude.ai/redirect/website.v1.67274164-23df-4883-8166-3c93ced276be/directory/37ed56d5-9d61-4fd4-ad00-b9134c694296) to add the Hugging Face connector to your account. \n\nAlternatively, navigate to [https://claude.ai/settings/connectors](https://claude.ai/settings/connectors), and add \"Hugging Face\" from the gallery.\n\n\u003cimg src='docs/claude-badge.png' width='50%' align='center' /\u003e\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eInstall in \u003cb\u003eClaude Code\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nEnter the command below to install in \u003cb\u003eClaude Code\u003c/b\u003e:\n\n```bash\nclaude mcp add hf-mcp-server -t http https://huggingface.co/mcp?login\n```\n\nThen start `claude` and follow the instructions to complete authentication.\n\n```bash\nclaude mcp add hf-mcp-server \\\n  -t http https://huggingface.co/mcp \\\n  -H \"Authorization: Bearer \u003cYOUR_HF_TOKEN\u003e\"\n```\n\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eInstall in \u003cb\u003eVSCode\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nClick \u003ca href=\"vscode:mcp/install?%7B%22name%22%3A%22huggingface%22%2C%22gallery%22%3Atrue%2C%22url%22%3A%22https%3A%2F%2Fhuggingface.co%2Fmcp%3Flogin%22%7D\"\u003ehere\u003c/a\u003e to add the Hugging Face connector directly to VSCode. Alternatively, install from the gallery at [https://code.visualstudio.com/mcp](https://code.visualstudio.com/mcp): \n\n\u003cimg src='docs/vscode-badge.png' width='50%' align='center' /\u003e\n\nIf you prefer to configure manually or use an auth token, add the snippet below to your `mcp.json` configuration:\n\n\n```JSON\n\"huggingface\": {\n    \"url\": \"https://huggingface.co/mcp\",\n    \"headers\": {\n        \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n    }\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eInstall in \u003cb\u003eCursor\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nClick \u003ca href=\"https://cursor.com/en/install-mcp?name=Hugging%20Face\u0026config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcD9sb2dpbiJ9\"\u003ehere\u003c/a\u003e to install the Hugging Face MCP Server directly in \u003cb\u003eCursor\u003c/b\u003e. \n\nIf you prefer to use configure manually or specify an Authorization Token, use the snippet below:\n\n```JSON\n\"huggingface\": {\n    \"url\": \"https://huggingface.co/mcp\",\n    \"headers\": {\n        \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n    }\n```\n\u003c/details\u003e\n\nOnce installed, navigate to https://huggingface.co/settings/mcp to configure your Tools and Spaces.\n\n\u003e [!TIP]\n\u003e Add ?no_image_content=true to the URL to remove ImageContent blocks from Gradio Servers.\n\n\n![hf_mcp_server_small](https://github.com/user-attachments/assets/d30f9f56-b08c-4dfc-a68f-a164a93db564)\n\n\n## Quick Guide (Repository Packages)\n\nThis repo contains:\n\n - (`/mcp`) MCP Implementations of Hub API and Search endpoints for integration with MCP Servers. \n - (`/app`) An MCP Server and Web Application for deploying endpoints.\n\n### MCP Server\n\nThe following transports are supported:\n\n- STDIO \n- SSE (To be deprecated, but still commonly deployed).\n- StreamableHTTP\n- StreamableHTTP in Stateless JSON Mode (**StreamableHTTPJson**)\n\nThe Web Application and HTTP Transports start by default on Port 3000. \n\nSSE and StreamableHTTP services are available at `/sse` and `/mcp` respectively. Although though not strictly enforced by the specification this is common convention.\n\n\u003e [!TIP]\n\u003e The Web Application allows you to switch tools on and off. For STDIO, SSE and StreamableHTTP this will send a ToolListChangedNotification to the MCP Client. In StreamableHTTPJSON mode the tool will not be listed when the client next requests the tool lists.\n\n### Running Locally\n\nYou can run the MCP Server locally with either `npx` or `docker`. \n\n```bash\nnpx @llmindset/hf-mcp-server       # Start in STDIO mode\nnpx @llmindset/hf-mcp-server-http  # Start in Streamable HTTP mode\nnpx @llmindset/hf-mcp-server-json  # Start in Streamable HTTP (JSON RPC) mode\n```\n\nTo run with docker: \n\n```bash\ndocker pull ghcr.io/evalstate/hf-mcp-server:latest\ndocker run --rm -p 3000:3000 ghcr.io/evalstate/hf-mcp-server:latest\n```\n![image](https://github.com/user-attachments/assets/2fc0ef58-2c7a-4fae-82b5-e6442bfcbd99)\n\nAll commands above start the Management Web interface on http://localhost:3000/. The Streamable HTTP server is accessible on  http://localhost:3000/mcp. See [Environment Variables](#Environment Variables) for configuration options. Docker defaults to Streamable HTTP (JSON RPC) mode.\n\n### Developing OpenAI Apps SDK Components\n\nTo build and test the Apps SDK component, run \n\n```bash\ncd packages/app\nnpm run dev:widget\n```\n\nThen open `http://localhost:5173/gradio-widget-dev.html`. This will bring up a browser with HMR where you can send Structured Content to the components for testing. \n\n![skybridge-viewer](./docs/skybridge-dev.png)\n\n\n## Development\n\nThis project uses `pnpm` for build and development. Corepack is used to ensure everyone uses the same pnpm version (10.12.3).\n\n```bash\n# Install dependencies\npnpm install\n\n# Build all packages\npnpm build\n```\n\n### Build Commands\n\n`pnpm run clean` -\u003e clean build artifacts\n\n`pnpm run build` -\u003e build packages\n\n`pnpm run start` -\u003e start the mcp server application\n\n`pnpm run buildrun` -\u003e clean, build and start\n\n`pnpm run dev` -\u003e concurrently watch `mcp` and start dev server with HMR\n\n\n## Docker Build\n\nBuild the image:\n```bash\ndocker build -t hf-mcp-server .\n```\n\nRun with default settings (Streaming HTTP JSON Mode), Dashboard on Port 3000:\n```bash\ndocker run --rm -p 3000:3000 -e DEFAULT_HF_TOKEN=hf_xxx hf-mcp-server\n```\n\nRun STDIO MCP Server:\n```bash\ndocker run -i --rm -e TRANSPORT=stdio -p 3000:3000 -e DEFAULT_HF_TOKEN=hf_xxx hf-mcp-server\n```\n\n`TRANSPORT` can be `stdio`, `sse`, `streamingHttp` or `streamingHttpJson` (default).\n\n### Transport Endpoints\n\nThe different transport types use the following endpoints:\n- SSE: `/sse` (with message endpoint at `/message`)\n- Streamable HTTP: `/mcp` (regular or JSON mode)\n- STDIO: Uses stdin/stdout directly, no HTTP endpoint\n\n### Stateful Connection Management\n\nThe `sse` and `streamingHttp` transports are both _stateful_ - they maintain a connection with the MCP Client through an SSE connection. When using these transports, the following configuration options take effect:\n\n| Environment Variable              | Default | Description |\n|-----------------------------------|---------|-------------|\n| `MCP_CLIENT_HEARTBEAT_INTERVAL`   | 30000ms | How often to check SSE connection health |\n| `MCP_CLIENT_CONNECTION_CHECK`     | 90000ms | How often to check for stale sessions |\n| `MCP_CLIENT_CONNECTION_TIMEOUT`   | 300000ms | Remove sessions inactive for this duration |\n| `MCP_PING_ENABLED`                | true    | Enable ping keep-alive for sessions |\n| `MCP_PING_INTERVAL`               | 30000ms | Interval between ping cycles | \n\n\n### Environment Variables\n\nThe server respects the following environment variables:\n- `TRANSPORT`: The transport type to use (stdio, sse, streamableHttp, or streamableHttpJson)\n- `DEFAULT_HF_TOKEN`: ⚠️ Requests are serviced with the HF_TOKEN received in the Authorization: Bearer header. The DEFAULT_HF_TOKEN is used if no header was sent. Only set this in Development / Test environments or for local STDIO Deployments. ⚠️\n- If running with `stdio` transport, `HF_TOKEN` is used if `DEFAULT_HF_TOKEN` is not set.\n- `HF_API_TIMEOUT`: Timeout for Hugging Face API requests in milliseconds (default: 12500ms / 12.5 seconds)\n- `USER_CONFIG_API`: URL to use for User settings (defaults to Local front-end)\n- `MCP_STRICT_COMPLIANCE`: set to True for GET 405 rejects in JSON Mode (default serves a welcome page).\n- `AUTHENTICATE_TOOL`: whether to include an `Authenticate` tool to issue an OAuth challenge when called\n- `SEARCH_ENABLES_FETCH`: When set to `true`, automatically enables the `hf_doc_fetch` tool whenever `hf_doc_search` is enabled\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:12Z","updated_at":"2025-10-22T11:21:27Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://huggingface.co/mcp?login"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"1094d85c-b89b-4a7b-a4da-74718f67d839","is_latest":true,"published_at":"2025-08-20T00:00:00Z","updated_at":"2025-08-20T00:00:00Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Hugging Face","homepage_url":"https://huggingface.co/mcp","is_in_organization":true,"license":"MIT License","name":"hf-mcp-server","name_with_owner":"huggingface/hf-mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/40aeed7583ef7bb97ac550654d4857375735ad780741b2120a30d7b1fd2bdc38/huggingface/hf-mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/25720743?v=4","preferred_image":"https://avatars.githubusercontent.com/u/25720743?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-21T20:38:49Z","readme":"# Hugging Face Official MCP Server \n\n\u003cimg src='https://github.com/evalstate/hf-mcp-server/blob/main/hf-logo.svg' width='100'\u003e\n\nWelcome to the official Hugging Face MCP Server 🤗. Connect your LLM to the Hugging Face Hub and thousands of Gradio AI Applications.\n\n## Installing the MCP Server\n\nFollow the instructions below to get started:\n\n\u003cdetails\u003e\n\u003csummary\u003eInstall in \u003cb\u003eClaude Desktop\u003c/b\u003e or \u003cb\u003eclaude.ai\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nClick [here](https://claude.ai/redirect/website.v1.67274164-23df-4883-8166-3c93ced276be/directory/37ed56d5-9d61-4fd4-ad00-b9134c694296) to add the Hugging Face connector to your account. \n\nAlternatively, navigate to [https://claude.ai/settings/connectors](https://claude.ai/settings/connectors), and add \"Hugging Face\" from the gallery.\n\n\u003cimg src='docs/claude-badge.png' width='50%' align='center' /\u003e\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eInstall in \u003cb\u003eClaude Code\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nEnter the command below to install in \u003cb\u003eClaude Code\u003c/b\u003e:\n\n```bash\nclaude mcp add hf-mcp-server -t http https://huggingface.co/mcp?login\n```\n\nThen start `claude` and follow the instructions to complete authentication.\n\n```bash\nclaude mcp add hf-mcp-server \\\n  -t http https://huggingface.co/mcp \\\n  -H \"Authorization: Bearer \u003cYOUR_HF_TOKEN\u003e\"\n```\n\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\n\u003csummary\u003eInstall in \u003cb\u003eVSCode\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nClick \u003ca href=\"vscode:mcp/install?%7B%22name%22%3A%22huggingface%22%2C%22gallery%22%3Atrue%2C%22url%22%3A%22https%3A%2F%2Fhuggingface.co%2Fmcp%3Flogin%22%7D\"\u003ehere\u003c/a\u003e to add the Hugging Face connector directly to VSCode. Alternatively, install from the gallery at [https://code.visualstudio.com/mcp](https://code.visualstudio.com/mcp): \n\n\u003cimg src='docs/vscode-badge.png' width='50%' align='center' /\u003e\n\nIf you prefer to configure manually or use an auth token, add the snippet below to your `mcp.json` configuration:\n\n\n```JSON\n\"huggingface\": {\n    \"url\": \"https://huggingface.co/mcp\",\n    \"headers\": {\n        \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n    }\n```\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eInstall in \u003cb\u003eCursor\u003c/b\u003e\u003c/summary\u003e\n\u003cbr /\u003e\n\nClick \u003ca href=\"https://cursor.com/en/install-mcp?name=Hugging%20Face\u0026config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcD9sb2dpbiJ9\"\u003ehere\u003c/a\u003e to install the Hugging Face MCP Server directly in \u003cb\u003eCursor\u003c/b\u003e. \n\nIf you prefer to use configure manually or specify an Authorization Token, use the snippet below:\n\n```JSON\n\"huggingface\": {\n    \"url\": \"https://huggingface.co/mcp\",\n    \"headers\": {\n        \"Authorization\": \"Bearer \u003cYOUR_HF_TOKEN\u003e\"\n    }\n```\n\u003c/details\u003e\n\nOnce installed, navigate to https://huggingface.co/settings/mcp to configure your Tools and Spaces.\n\n\u003e [!TIP]\n\u003e Add ?no_image_content=true to the URL to remove ImageContent blocks from Gradio Servers.\n\n\n![hf_mcp_server_small](https://github.com/user-attachments/assets/d30f9f56-b08c-4dfc-a68f-a164a93db564)\n\n\n## Quick Guide (Repository Packages)\n\nThis repo contains:\n\n - (`/mcp`) MCP Implementations of Hub API and Search endpoints for integration with MCP Servers. \n - (`/app`) An MCP Server and Web Application for deploying endpoints.\n\n### MCP Server\n\nThe following transports are supported:\n\n- STDIO \n- SSE (To be deprecated, but still commonly deployed).\n- StreamableHTTP\n- StreamableHTTP in Stateless JSON Mode (**StreamableHTTPJson**)\n\nThe Web Application and HTTP Transports start by default on Port 3000. \n\nSSE and StreamableHTTP services are available at `/sse` and `/mcp` respectively. Although though not strictly enforced by the specification this is common convention.\n\n\u003e [!TIP]\n\u003e The Web Application allows you to switch tools on and off. For STDIO, SSE and StreamableHTTP this will send a ToolListChangedNotification to the MCP Client. In StreamableHTTPJSON mode the tool will not be listed when the client next requests the tool lists.\n\n### Running Locally\n\nYou can run the MCP Server locally with either `npx` or `docker`. \n\n```bash\nnpx @llmindset/hf-mcp-server       # Start in STDIO mode\nnpx @llmindset/hf-mcp-server-http  # Start in Streamable HTTP mode\nnpx @llmindset/hf-mcp-server-json  # Start in Streamable HTTP (JSON RPC) mode\n```\n\nTo run with docker: \n\n```bash\ndocker pull ghcr.io/evalstate/hf-mcp-server:latest\ndocker run --rm -p 3000:3000 ghcr.io/evalstate/hf-mcp-server:latest\n```\n![image](https://github.com/user-attachments/assets/2fc0ef58-2c7a-4fae-82b5-e6442bfcbd99)\n\nAll commands above start the Management Web interface on http://localhost:3000/. The Streamable HTTP server is accessible on  http://localhost:3000/mcp. See [Environment Variables](#Environment Variables) for configuration options. Docker defaults to Streamable HTTP (JSON RPC) mode.\n\n### Developing OpenAI Apps SDK Components\n\nTo build and test the Apps SDK component, run \n\n```bash\ncd packages/app\nnpm run dev:widget\n```\n\nThen open `http://localhost:5173/gradio-widget-dev.html`. This will bring up a browser with HMR where you can send Structured Content to the components for testing. \n\n![skybridge-viewer](./docs/skybridge-dev.png)\n\n\n## Development\n\nThis project uses `pnpm` for build and development. Corepack is used to ensure everyone uses the same pnpm version (10.12.3).\n\n```bash\n# Install dependencies\npnpm install\n\n# Build all packages\npnpm build\n```\n\n### Build Commands\n\n`pnpm run clean` -\u003e clean build artifacts\n\n`pnpm run build` -\u003e build packages\n\n`pnpm run start` -\u003e start the mcp server application\n\n`pnpm run buildrun` -\u003e clean, build and start\n\n`pnpm run dev` -\u003e concurrently watch `mcp` and start dev server with HMR\n\n\n## Docker Build\n\nBuild the image:\n```bash\ndocker build -t hf-mcp-server .\n```\n\nRun with default settings (Streaming HTTP JSON Mode), Dashboard on Port 3000:\n```bash\ndocker run --rm -p 3000:3000 -e DEFAULT_HF_TOKEN=hf_xxx hf-mcp-server\n```\n\nRun STDIO MCP Server:\n```bash\ndocker run -i --rm -e TRANSPORT=stdio -p 3000:3000 -e DEFAULT_HF_TOKEN=hf_xxx hf-mcp-server\n```\n\n`TRANSPORT` can be `stdio`, `sse`, `streamingHttp` or `streamingHttpJson` (default).\n\n### Transport Endpoints\n\nThe different transport types use the following endpoints:\n- SSE: `/sse` (with message endpoint at `/message`)\n- Streamable HTTP: `/mcp` (regular or JSON mode)\n- STDIO: Uses stdin/stdout directly, no HTTP endpoint\n\n### Stateful Connection Management\n\nThe `sse` and `streamingHttp` transports are both _stateful_ - they maintain a connection with the MCP Client through an SSE connection. When using these transports, the following configuration options take effect:\n\n| Environment Variable              | Default | Description |\n|-----------------------------------|---------|-------------|\n| `MCP_CLIENT_HEARTBEAT_INTERVAL`   | 30000ms | How often to check SSE connection health |\n| `MCP_CLIENT_CONNECTION_CHECK`     | 90000ms | How often to check for stale sessions |\n| `MCP_CLIENT_CONNECTION_TIMEOUT`   | 300000ms | Remove sessions inactive for this duration |\n| `MCP_PING_ENABLED`                | true    | Enable ping keep-alive for sessions |\n| `MCP_PING_INTERVAL`               | 30000ms | Interval between ping cycles | \n\n\n### Environment Variables\n\nThe server respects the following environment variables:\n- `TRANSPORT`: The transport type to use (stdio, sse, streamableHttp, or streamableHttpJson)\n- `DEFAULT_HF_TOKEN`: ⚠️ Requests are serviced with the HF_TOKEN received in the Authorization: Bearer header. The DEFAULT_HF_TOKEN is used if no header was sent. Only set this in Development / Test environments or for local STDIO Deployments. ⚠️\n- If running with `stdio` transport, `HF_TOKEN` is used if `DEFAULT_HF_TOKEN` is not set.\n- `HF_API_TIMEOUT`: Timeout for Hugging Face API requests in milliseconds (default: 12500ms / 12.5 seconds)\n- `USER_CONFIG_API`: URL to use for User settings (defaults to Local front-end)\n- `MCP_STRICT_COMPLIANCE`: set to True for GET 405 rejects in JSON Mode (default serves a welcome page).\n- `AUTHENTICATE_TOOL`: whether to include an `Authenticate` tool to issue an OAuth challenge when called\n- `SEARCH_ENABLES_FETCH`: When set to `true`, automatically enables the `hf_doc_fetch` tool whenever `hf_doc_search` is enabled\n","stargazer_count":108,"uses_custom_opengraph_image":false}}}},{"name":"azure/aks-mcp","description":"Interact with Azure Kubernetes Service (AKS) from MCP clients.","status":"active","repository":{"url":"https://github.com/Azure/aks-mcp","source":"github","id":"972374392","readme":"# AKS-MCP\n\nThe AKS-MCP is a Model Context Protocol (MCP) server that enables AI assistants\nto interact with Azure Kubernetes Service (AKS) clusters. It serves as a bridge\nbetween AI tools (like GitHub Copilot, Claude, and other MCP-compatible AI\nassistants) and AKS, translating natural language requests into AKS operations\nand returning the results in a format the AI tools can understand.\n\nIt allows AI tools to:\n\n- Operate (CRUD) AKS resources\n- Retrieve details related to AKS clusters (VNets, Subnets, NSGs, Route Tables, etc.)\n- Manage Azure Fleet operations for multi-cluster scenarios\n\n## How it works\n\nAKS-MCP connects to Azure using the Azure SDK and provides a set of tools that\nAI assistants can use to interact with AKS resources. It leverages the Model\nContext Protocol (MCP) to facilitate this communication, enabling AI tools to\nmake API calls to Azure and interpret the responses.\n\n## Azure CLI Authentication\n\nAKS-MCP uses Azure CLI (az) for AKS operations. Azure CLI authentication is attempted in this order:\n\n1. Service Principal (client secret): When `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID` environment variables are present, a service principal login is performed using the following command: `az login --service-principal -u CLIENT_ID -p CLIENT_SECRET --tenant TENANT_ID`\n\n1. Workload Identity (federated token): When `AZURE_CLIENT_ID`, `AZURE_TENANT_ID`, `AZURE_FEDERATED_TOKEN_FILE` environment variables are present, a federated token login is performed using the following command: `az login --service-principal -u CLIENT_ID --tenant TENANT_ID --federated-token TOKEN`\n\n1. User-assigned Managed Identity (managed identity client ID): When only `AZURE_CLIENT_ID` environment variable is present, a user-assigned managed identity login is performed using the following command: `az login --identity -u CLIENT_ID`\n\n1. System-assigned Managed Identity: When `AZURE_MANAGED_IDENTITY` is set to `system`, a system-assigned managed identity login is performed using the following command: `az login --identity`\n\n1. Existing Login: When none of the above environment variables are set, AKS-MCP assumes you have already authenticated (for example, via `az login`) and uses the existing session.\n\nOptional subscription selection:\n\n- If `AZURE_SUBSCRIPTION_ID` is set, AKS-MCP will run `az account set --subscription SUBSCRIPTION_ID` after login.\n\nNotes and security:\n\n- The federated token file must be exactly `/var/run/secrets/azure/tokens/azure-identity-token` and is strictly validated; other paths are rejected.\n- After each login, AKS-MCP verifies authentication with `az account show --query id -o tsv`.\n- Ensure the Azure CLI is installed and on PATH.\n\nEnvironment variables used:\n\n- `AZURE_TENANT_ID`\n- `AZURE_CLIENT_ID`\n- `AZURE_CLIENT_SECRET`\n- `AZURE_FEDERATED_TOKEN_FILE`\n- `AZURE_SUBSCRIPTION_ID`\n- `AZURE_MANAGED_IDENTITY` (set to `system` to opt into system-assigned managed identity)\n\n## Available Tools\n\nThe AKS-MCP server provides consolidated tools for interacting with AKS\nclusters. Some tools will require read-write or admin permissions to run debugging pods on your cluster. To enable read-write or admin permissions for the AKS-MCP server, add the **access level** parameter to your MCP configuration file:\n\n1. Navigate to your **mcp.json** file, or go to MCP: List Servers -\u003e AKS-MCP -\u003e Show Configuration Details in the **Command Palette** (`Ctrl+Shift+P` on Windows/Linux or `Cmd+Shift+P` on macOS).\n2. In the \"args\" section of AKS-MCP, add the following parameters: \"--access-level\", \"readwrite\" / \"admin\"\n\nFor example:\n```\n\"args\": [\n  \"--transport\",\n  \"stdio\",\n  \"--access-level\",\n  \"readwrite\"\n]\n```\n\nThese tools have been designed to provide comprehensive functionality\nthrough unified interfaces:\n\n\u003cdetails\u003e\n\u003csummary\u003eAKS Cluster Management\u003c/summary\u003e\n\n**Tool:** `az_aks_operations`\n\nUnified tool for managing Azure Kubernetes Service (AKS) clusters and related operations.\n\n**Available Operations:**\n\n- **Read-Only** (all access levels):\n  - `show`: Show cluster details\n  - `list`: List clusters in subscription/resource group\n  - `get-versions`: Get available Kubernetes versions\n  - `check-network`: Perform outbound network connectivity check\n  - `nodepool-list`: List node pools in cluster\n  - `nodepool-show`: Show node pool details\n  - `account-list`: List Azure subscriptions\n\n- **Read-Write** (`readwrite`/`admin` access levels):\n  - `create`: Create new cluster\n  - `delete`: Delete cluster\n  - `scale`: Scale cluster node count\n  - `start`: Start a stopped cluster\n  - `stop`: Stop a running cluster\n  - `update`: Update cluster configuration\n  - `upgrade`: Upgrade Kubernetes version\n  - `nodepool-add`: Add node pool to cluster\n  - `nodepool-delete`: Delete node pool\n  - `nodepool-scale`: Scale node pool\n  - `nodepool-upgrade`: Upgrade node pool\n  - `account-set`: Set active subscription\n  - `login`: Azure authentication\n\n- **Admin-Only** (`admin` access level):\n  - `get-credentials`: Get cluster credentials for kubectl access\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eNetwork Resource Management\u003c/summary\u003e\n\n**Tool:** `az_network_resources`\n\nUnified tool for getting Azure network resource information used by AKS clusters.\n\n**Available Resource Types:**\n\n- `all`: Get information about all network resources\n- `vnet`: Virtual Network information\n- `subnet`: Subnet information\n- `nsg`: Network Security Group information\n- `route_table`: Route Table information\n- `load_balancer`: Load Balancer information\n- `private_endpoint`: Private endpoint information\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eMonitoring and Diagnostics\u003c/summary\u003e\n\n**Tool:** `az_monitoring`\n\nUnified tool for Azure monitoring and diagnostics operations for AKS clusters.\n\n**Available Operations:**\n\n- `metrics`: List metric values for resources\n- `resource_health`: Retrieve resource health events for AKS clusters\n- `app_insights`: Execute KQL queries against Application Insights telemetry data\n- `diagnostics`: Check if AKS cluster has diagnostic settings configured\n- `control_plane_logs`: Query AKS control plane logs with safety constraints\n  and time range validation\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eCompute Resources\u003c/summary\u003e\n\n**Tool:** `get_aks_vmss_info`\n\n- Get detailed VMSS configuration for node pools in the AKS cluster\n\n**Tool:** `az_compute_operations`\n\nUnified tool for managing Azure Virtual Machines (VMs) and Virtual Machine Scale Sets (VMSS) used by AKS.\n\n**Available Operations:**\n\n- `show`: Get details of a VM/VMSS\n- `list`: List VMs/VMSS in subscription or resource group\n- `get-instance-view`: Get runtime status\n- `start`: Start VM\n- `stop`: Stop VM\n- `restart`: Restart VM/VMSS instances\n- `reimage`: Reimage VMSS instances (VM not supported for reimage)\n\n**Resource Types:** `vm` (single virtual machines), `vmss` (virtual machine scale sets)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eFleet Management\u003c/summary\u003e\n\n**Tool:** `az_fleet`\n\nComprehensive Azure Fleet management for multi-cluster scenarios.\n\n**Available Operations:**\n\n- **Fleet Operations**: list, show, create, update, delete, get-credentials\n- **Member Operations**: list, show, create, update, delete\n- **Update Run Operations**: list, show, create, start, stop, delete\n- **Update Strategy Operations**: list, show, create, delete\n- **ClusterResourcePlacement Operations**: list, show, get, create, delete\n\nSupports both Azure Fleet management and Kubernetes ClusterResourcePlacement\nCRD operations.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eDiagnostic Detectors\u003c/summary\u003e\n\n**Tool:** `list_detectors`\n\n- List all available AKS cluster detectors\n\n**Tool:** `run_detector`\n\n- Run a specific AKS diagnostic detector\n\n**Tool:** `run_detectors_by_category`\n\n- Run all detectors in a specific category\n- **Categories**: Best Practices, Cluster and Control Plane Availability and\n  Performance, Connectivity Issues, Create/Upgrade/Delete and Scale,\n  Deprecations, Identity and Security, Node Health, Storage\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eAzure Advisor\u003c/summary\u003e\n\n**Tool:** `az_advisor_recommendation`\n\nRetrieve and manage Azure Advisor recommendations for AKS clusters.\n\n**Available Operations:**\n\n- `list`: List recommendations with filtering options\n- `report`: Generate recommendation reports\n- **Filter Options**: resource_group, cluster_names, category (Cost,\n  HighAvailability, Performance, Security), severity (High, Medium, Low)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eKubernetes Tools\u003c/summary\u003e\n\n*Note: kubectl commands are available with all access levels. Additional tools\nrequire explicit enablement via `--additional-tools`*\n\n**kubectl Tools (Unified Interface):**\n\n- **Read-Only** (all access levels):\n  - `kubectl_resources`: View resources (get, describe) - filtered to read-only operations in readonly mode\n  - `kubectl_diagnostics`: Debug and diagnose (logs, events, top, exec, cp)\n  - `kubectl_cluster`: Cluster information (cluster-info, api-resources, api-versions, explain)\n  - `kubectl_config`: Configuration management (diff, auth, config) - filtered to read-only operations in readonly mode\n\n- **Read-Write/Admin** (`readwrite`/`admin` access levels):\n  - `kubectl_resources`: Full resource management (get, describe, create, delete, apply, patch, replace, cordon, uncordon, drain, taint)\n  - `kubectl_workloads`: Workload lifecycle (run, expose, scale, autoscale, rollout)\n  - `kubectl_metadata`: Metadata management (label, annotate, set)\n  - `kubectl_config`: Full configuration management (diff, auth, certificate, config)\n\n**Additional Tools (Optional):**\n\n- `helm`: Helm package manager (requires `--additional-tools helm`)\n- `cilium`: Cilium CLI for eBPF networking (requires `--additional-tools cilium`)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eReal-time Observability\u003c/summary\u003e\n\n**Tool:** `inspektor_gadget_observability`\n\nReal-time observability tool for Azure Kubernetes Service (AKS) clusters using\neBPF.\n\n**Available Actions:**\n\n- `deploy`: Deploy Inspektor Gadget to cluster (requires `readwrite`/`admin` access)\n- `undeploy`: Remove Inspektor Gadget from cluster (requires `readwrite`/`admin` access)\n- `is_deployed`: Check deployment status\n- `run`: Run one-shot gadgets\n- `start`: Start continuous gadgets\n- `stop`: Stop running gadgets\n- `get_results`: Retrieve gadget results\n- `list_gadgets`: List available gadgets\n\n**Available Gadgets:**\n\n- `observe_dns`: Monitor DNS requests and responses\n- `observe_tcp`: Monitor TCP connections\n- `observe_file_open`: Monitor file system operations\n- `observe_process_execution`: Monitor process execution\n- `observe_signal`: Monitor signal delivery\n- `observe_system_calls`: Monitor system calls\n- `top_file`: Top files by I/O operations\n- `top_tcp`: Top TCP connections by traffic\n\n\u003c/details\u003e\n\n## How to install\n\n### Prerequisites\n\n1. Set up [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) and authenticate:\n\n   ```bash\n   az login\n   ```\n\n### VS Code with GitHub Copilot (Recommended)\n\n#### 🚀 One-Click Installation with the AKS Extension\n\nThe easiest way to get started with AKS-MCP is through the **Azure Kubernetes Service Extension for VS Code**.\n\n#### Step 1: Install the AKS Extension\n\n1. Open VS Code and go to Extensions (`Ctrl+Shift+X` on Windows/Linux or `Cmd+Shift+X` on macOS).\n1. Search for [Azure Kubernetes Service](https://marketplace.visualstudio.com/items?itemName=ms-kubernetes-tools.vscode-aks-tools).\n1. Install the official Microsoft AKS extension.\n\n#### Step 2: Launch the AKS-MCP Server\n\n1. Open the **Command Palette** (`Ctrl+Shift+P` on Windows/Linux or `Cmd+Shift+P` on macOS).\n2. Search and run: **AKS: Setup AKS MCP Server**.\n\nUpon successful installation, the server will now be visible in **MCP: List Servers** (via Command Palette). From there, you can start the MCP server or view its status.\n\n#### Step 3: Start Using AKS-MCP\n\nOnce started, the MCP server will appear in the **Copilot Chat: Configure Tools** dropdown under `MCP Server: AKS MCP`, ready to enhance contextual prompts based on your AKS environment. By default, all AKS-MCP server tools are enabled. You can review the list of available tools and disable any that are not required for your specific scenario.\n\nTry a prompt like *\"List all my AKS clusters\"*, which will start using tools from the AKS-MCP server.\n\n#### WSL Configuration\n\nThe MCP configuration differs depending on whether VS Code is running on Windows or inside WSL:\n\n**🪟 Windows Host (VS Code on Windows)**: Use `\"command\": \"wsl\"` to invoke the WSL binary from Windows:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"wsl\",\n      \"args\": [\n        \"--\",\n        \"/home/you/.vs-kubernetes/tools/aks-mcp/aks-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n**🐧 Remote-WSL (VS Code running inside WSL)**: Call the binary directly or use a shell wrapper:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"/home/you/.vs-kubernetes/tools/aks-mcp/aks-mcp --transport stdio\"\n      ]\n    }\n  }\n}\n```\n\n**🔧 Troubleshooting ENOENT Errors**\n\nIf you see \"spawn ENOENT\" errors, verify your VS Code environment:\n- **Windows host**: Check if the WSL binary path is correct and accessible via `wsl -- ls /path/to/aks-mcp`\n- **Remote-WSL**: Do NOT use `\"command\": \"wsl\"` - use direct paths or bash wrapper as shown above\n\n\n\u003e **💡 Benefits**: The AKS extension handles binary downloads, updates, and configuration automatically, ensuring you always have the latest version with optimal settings.\n\n### Alternative Installation Methods\n\n\u003cdetails\u003e\n\u003csummary\u003eManual Binary Installation\u003c/summary\u003e\n\n#### Step 1: Download the Binary\n\nChoose your platform and download the latest AKS-MCP binary:\n\n| Platform | Architecture | Download Link |\n|----------|-------------|---------------|\n| **Windows** | AMD64 | [📥 aks-mcp-windows-amd64.exe](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-amd64.exe) |\n| | ARM64 | [📥 aks-mcp-windows-arm64.exe](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-arm64.exe) |\n| **macOS** | Intel (AMD64) | [📥 aks-mcp-darwin-amd64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-darwin-amd64) |\n| | Apple Silicon (ARM64) | [📥 aks-mcp-darwin-arm64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-darwin-arm64) |\n| **Linux** | AMD64 | [📥 aks-mcp-linux-amd64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-amd64) |\n| | ARM64 | [📥 aks-mcp-linux-arm64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-arm64) |\n\n#### Step 2: Configure VS Code\n\nAfter downloading, create a `.vscode/mcp.json` file in your workspace root with the path to your downloaded binary.\n\n##### Option A: Automated Setup Script\n\nFor quick setup, you can use these one-liner scripts that download the binary\nand create the configuration:\n\n*Windows (PowerShell):*\n\n```powershell\n# Download binary and create VS Code configuration\nmkdir -p .vscode ; Invoke-WebRequest -Uri \"https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-amd64.exe\" -OutFile \"aks-mcp.exe\" ; @{servers=@{\"aks-mcp-server\"=@{type=\"stdio\";command=\"$PWD\\aks-mcp.exe\";args=@(\"--transport\",\"stdio\")}}} | ConvertTo-Json -Depth 3 | Out-File \".vscode/mcp.json\" -Encoding UTF8\n```\n\n*macOS/Linux (Bash):*\n\n```bash\n# Download binary and create VS Code configuration\nmkdir -p .vscode \u0026\u0026 curl -sL https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-amd64 -o aks-mcp \u0026\u0026 chmod +x aks-mcp \u0026\u0026 echo '{\"servers\":{\"aks-mcp-server\":{\"type\":\"stdio\",\"command\":\"'$PWD'/aks-mcp\",\"args\":[\"--transport\",\"stdio\"]}}}' \u003e .vscode/mcp.json\n```\n\n##### Option B: Manual Configuration\n\n\u003e **✨ Simple Setup**: Download the binary for your platform, then use the manual configuration below to set up the MCP server in VS Code.\n\n#### Manual VS Code Configuration\n\nYou can configure the AKS-MCP server in two ways:\n\n**1. Workspace-specific configuration** (recommended for project-specific usage):\n\nCreate a `.vscode/mcp.json` file in your workspace with the path to your downloaded binary:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"\u003center the file path\u003e\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n**2. User-level configuration** (persistent across all workspaces):\n\nFor a persistent configuration that works across all your VS Code workspaces, add the MCP server to your VS Code user settings:\n\n1. Open VS Code Settings (Ctrl+, or Cmd+,)\n2. Search for \"mcp\" in the settings\n3. Add the following to your User Settings JSON:\n\n```json\n{\n  \"github.copilot.chat.mcp.servers\": {\n    \"aks-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"\u003center the file path\u003e\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n#### Step 3: Load the AKS-MCP server tools to Github Copilot\n\n1. If running on an older version of VS Code: restart VS Code i.e. close and\n   reopen VS Code to load the new MCP server configuration.\n2. Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n3. Click the **Tools** button or run /list in the Github Copilot window to see the list of available tools\n4. You should see the AKS-MCP tools in the list\n5. Try a prompt like: *\"List all my AKS clusters in subscription xxx\"*\n6. The agent will automatically use AKS-MCP tools to complete your request\n\n\u003e **💡 Tip**: If you don't see the AKS-MCP tools after restarting, check the VS Code output panel for any MCP server connection errors and verify your binary path in `.vscode/mcp.json`.\n\n**Note**: Ensure you have authenticated with Azure CLI (`az login`) for the server to access your Azure resources.\n\n\u003c/details\u003e\n\n### Other MCP-Compatible Clients\n\n\u003cdetails\u003e\n\u003csummary\u003eDocker and Custom Client Installation\u003c/summary\u003e\n\nFor other MCP-compatible AI clients like [Claude Desktop](https://claude.ai/), configure the server in your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"command\": \"\u003cpath of binary aks-mcp\u003e\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n#### 🐳 Docker MCP Toolkit\n\nYou can enable the [AKS-MCP server directly from MCP Toolkit](https://hub.docker.com/mcp/server/aks/overview):\n\n1. Open Docker Desktop\n2. Click \"MCP Toolkit\" in the left sidebar\n3. Search for \"aks\" in Catalog tab\n4. Click on the AKS-MCP server card\n5. Enable the server by clicking \"+\" in the top right corner\n6. Configure the server using \"Configuration\" tab:\n   - **azure_dir** `[REQUIRED]`: Path to your Azure credentials directory e.g `/home/user/.azure` (must be absolute – without `$HOME` or `~`)\n   - **kubeconfig** `[REQUIRED]`: Path to your kubeconfig file e.g `/home/user/.kube/config` (must be absolute – without `$HOME` or `~`)\n   - **access_level** `[REQUIRED]`: Set to `readonly`, `readwrite`, or `admin` as needed\n   - **container_user** `[OPTIONAL]`: Username or UID to run the container as (default is `mcp`), e.g. use `1000` to match your host user ID (see note below). Only needed if you are using docker engine on Linux.\n7. You are now ready to use the AKS-MCP server with your [preferred MCP client](https://hub.docker.com/mcp/server/aks/manual), see an example [here](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/#install-an-mcp-client). (requires `\u003e= v0.16.0` for MCP gateway)\n\n\u003e **Note**: When running the MCP gateway using Docker Engine, you have to set the `container_user` to match your host user ID (e.g using `id -u`) to ensure proper file permissions for accessing mounted volumes.\n\u003e On Docker Desktop, this is handled automatically if you use `desktop-*` contexts, confirmed by running `docker context ls`.\n\nOn **Windows**, the Azure credentials won't work by default, but you have two options:\n\n1. **Long-lived servers**: Configure the [MCP gateway](https://docs.docker.com/ai/mcp-gateway/) to use long-lived servers using `--long-lived` flag and then authenticate with Azure CLI in the container, see option B in Containerized MCP configuration below on how to fetch credentials inside the container. \n2. **Custom Azure Directory**: Set up a custom Azure directory:\n    ```powershell\n    # Set custom Azure config directory\n    $env:AZURE_CONFIG_DIR = \"$env:USERPROFILE\\.azure-for-docker\"\n    \n    # Disable token cache encryption (to match behavior with Linux/macOS)\n    $env:AZURE_CORE_ENCRYPT_TOKEN_CACHE = \"false\"\n    \n    # Login to Azure CLI\n    az login\n    ```\n\n   This will store the credentials in `$env:USERPROFILE\\.azure-for-docker` (e.g. `C:\\Users\\\u003cusername\u003e\\.azure-for-docker`),\n   use this path in the AKS-MCP server configuration `azure_dir`.\n\nYou can also use the [MCP Gateway](https://docs.docker.com/ai/mcp-gateway/) to enable the AKS-MCP server directly using:\n\n```bash\n# Enable AKS-MCP server in Docker MCP Gateway\ndocker mcp server enable aks\n```\n\nNote: You still need to configure the server (e.g. using `docker mcp config`) with your Azure credentials, kubeconfig file, and access level.\n\n#### 🐋 Containerized MCP configuration\n\nFor containerized deployment, you can run AKS-MCP server using the official Docker image:\n\nOption A: Mount credentials from host (recommended):\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"--user\",\n          \"\u003cyour-user-id (e.g. id -u)\u003e\",\n          \"-v\",\n          \"~/.azure:/home/mcp/.azure\",\n          \"-v\",\n          \"~/.kube:/home/mcp/.kube\",\n          \"ghcr.io/azure/aks-mcp:latest\",\n          \"--transport\",\n          \"stdio\"\n        ]\n    }\n  }\n}\n```\n\nOption B: fetch the credentials inside the container:\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"ghcr.io/azure/aks-mcp:latest\",\n          \"--transport\",\n          \"stdio\"\n        ]\n    }\n  }\n}\n```\n\nStart the MCP server container first per above command, and then run the following commands to fetch the credentials:\n- Login to Azure CLI: `docker exec -it \u003ccontainer-id\u003e az login --use-device-code`\n- Get kubeconfig: `docker exec -it \u003ccontainer-id\u003e az aks get-credentials -g \u003cresource-group\u003e -n \u003ccluster-name\u003e`\n\nNote that:\n\n- Host Azure CLI logins don’t automatically propagate into containers without mounting `~/.azure`.\n- User ID should be set for option A, orelse the mcp user inside container won't be able to access the mounted files.\n\n### 🤖 Custom MCP Client Installation\n\nYou can configure any MCP-compatible client to use the AKS-MCP server by running the binary directly:\n\n```bash\n# Run the server directly\n./aks-mcp --transport stdio\n```\n\n### 🔧 Manual Binary Installation\n\nFor direct binary usage without package managers:\n\n1. Download the latest release from the [releases page](https://github.com/Azure/aks-mcp/releases)\n2. Extract the binary to your preferred location\n3. Make it executable (on Unix systems):\n   ```bash\n   chmod +x aks-mcp\n   ```\n4. Configure your MCP client to use the binary path\n\n\u003c/details\u003e\n\n### Options\n\nCommand line arguments:\n\n```sh\nUsage of ./aks-mcp:\n      --access-level string       Access level (readonly, readwrite, admin) (default \"readonly\")\n      --additional-tools string   Comma-separated list of additional Kubernetes tools to support (kubectl is always enabled). Available: helm,cilium,hubble\n      --allow-namespaces string   Comma-separated list of allowed Kubernetes namespaces (empty means all namespaces)\n      --host string               Host to listen for the server (only used with transport sse or streamable-http) (default \"127.0.0.1\")\n      --otlp-endpoint string      OTLP endpoint for OpenTelemetry traces (e.g. localhost:4317, default \"\")\n      --port int                  Port to listen for the server (only used with transport sse or streamable-http) (default 8000)\n      --timeout int               Timeout for command execution in seconds, default is 600s (default 600)\n      --transport string          Transport mechanism to use (stdio, sse or streamable-http) (default \"stdio\")\n      --log-level string          Log level (debug, info, warn, error) (default \"info\")\n```\n\n**Environment variables:**\n- Standard Azure authentication environment variables are supported (`AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_SUBSCRIPTION_ID`)\n\n## Development\n\n### Prerequisites\n\n- **Go** ≥ `1.24.x` installed on your local machine\n- **Bash** available as `/usr/bin/env bash` (Makefile targets use multi-line recipes with fail-fast mode)\n- **GNU Make** `4.x` or later\n- **Docker** *(optional, for container builds and testing)*\n\n\u003e **Note:** If your login shell is different (e.g., `zsh` on **macOS**), you do **not** need to change it — the Makefile sets variables to run all recipes in `bash` for consistent behavior across platforms.\n\n### Building from Source\n\nThis project includes a Makefile for convenient development, building, and testing. To see all available targets:\n\n```bash\nmake help\n```\n\n#### Quick Start\n\n```bash\n# Build the binary\nmake build\n\n# Run tests\nmake test\n\n# Run tests with coverage\nmake test-coverage\n\n# Format and lint code\nmake check\n\n# Build for all platforms\nmake release\n```\n\n#### Common Development Tasks\n\n```bash\n# Install dependencies\nmake deps\n\n# Build and run with --help\nmake run\n\n# Clean build artifacts\nmake clean\n\n# Install binary to GOBIN\nmake install\n```\n\n#### Docker\n\n```bash\n# Build Docker image\nmake docker-build\n\n# Run Docker container\nmake docker-run\n```\n\n### Manual Build\n\nIf you prefer to build without the Makefile:\n\n```bash\ngo build -o aks-mcp ./cmd/aks-mcp\n```\n\n## Usage\n\nAsk any questions about your AKS clusters in your AI client, for example:\n\n```\nList all my AKS clusters in my subscription xxx.\n\nWhat is the network configuration of my AKS cluster?\n\nShow me the network security groups associated with my cluster.\n\nCreate a new Azure Fleet named prod-fleet in eastus region.\n\nList all members in my fleet.\n\nCreate a placement to deploy nginx workloads to clusters with app=frontend label.\n\nShow me all ClusterResourcePlacements in my fleet.\n```\n\n## Telemetry\n\nTelemetry collection is on by default.\n\nTo opt out, set the environment variable `AKS_MCP_COLLECT_TELEMETRY=false`.\n\n## Contributing\n\nWe welcome contributions to AKS-MCP! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n**📖 [Read our detailed Contributing Guide](CONTRIBUTING.md)** for comprehensive information on:\n\n- Setting up your development environment\n- Running AKS-MCP locally and testing with AI agents\n- Understanding the codebase architecture\n- Adding new MCP tools and features\n- Testing guidelines and best practices\n- Submitting pull requests\n\n### Quick Start for Contributors\n\n1. **Prerequisites**: Go ≥ 1.24.x, Azure CLI, Git\n2. **Setup**: Fork the repo, clone locally, run `make deps \u0026\u0026 make build`\n3. **Test**: Run `make test` and `make check`\n4. **Develop**: Follow the component-based architecture in [CONTRIBUTING.md](CONTRIBUTING.md)\n\n### Contributor License Agreement\n\nMost contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark \u0026 Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:05Z","updated_at":"2025-10-22T11:21:20Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","packages":[{"registry_type":"docker","identifier":"ghcr.io/azure/aks-mcp","version":"latest","runtime_hint":"docker","runtime_arguments":[{"is_required":true,"format":"string","value":"run","type":"positional","name":"docker_cmd"},{"is_required":true,"format":"string","value":"-i","type":"positional","name":"-i"},{"format":"string","value":"--rm","type":"positional","name":"--rm"},{"is_required":true,"format":"string","value":"ghcr.io/azure/aks-mcp:latest","type":"positional","name":"ghcr.io/azure/aks-mcp:latest","value_hint":"image"}],"package_arguments":[{"format":"string","value":"stdio","type":"named","name":"--transport"}],"environment_variables":[{"value":"{azure_tenant_id}","variables":{"azure_tenant_id":{"description":"For service principal / WI auth."}},"name":"AZURE_TENANT_ID"},{"value":"{azure_client_id}","variables":{"azure_client_id":{"description":"Service principal or managed identity client ID."}},"name":"AZURE_CLIENT_ID"},{"value":"{azure_client_secret}","variables":{"azure_client_secret":{"description":"Service principal client secret.","is_secret":true}},"name":"AZURE_CLIENT_SECRET"},{"value":"{azure_federated_token_file}","variables":{"azure_federated_token_file":{"description":"Workload Identity token file path."}},"name":"AZURE_FEDERATED_TOKEN_FILE"},{"value":"{azure_subscription_id}","variables":{"azure_subscription_id":{"description":"Optional: sets active subscription."}},"name":"AZURE_SUBSCRIPTION_ID"},{"value":"{azure_managed_identity}","variables":{"azure_managed_identity":{"description":"Set to 'system' to use system-assigned MI."}},"name":"AZURE_MANAGED_IDENTITY"}]}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"52720d16-d942-4f99-8599-3af03cb1b4a0","is_latest":true,"published_at":"2025-09-09T10:55:22.910285Z","updated_at":"2025-09-09T10:55:22.910285Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Azure Kubernetes Service","is_in_organization":true,"license":"MIT License","name":"aks-mcp","name_with_owner":"Azure/aks-mcp","opengraph_image_url":"https://opengraph.githubassets.com/7ab7a9abe9f68a50d4cca008801532319369ef2d4a4c3e96fae2e99158dde509/Azure/aks-mcp","owner_avatar_url":"https://avatars.githubusercontent.com/u/6844498?v=4","preferred_image":"https://avatars.githubusercontent.com/u/6844498?v=4","primary_language":"Go","primary_language_color":"#00ADD8","pushed_at":"2025-10-21T08:27:14Z","readme":"# AKS-MCP\n\nThe AKS-MCP is a Model Context Protocol (MCP) server that enables AI assistants\nto interact with Azure Kubernetes Service (AKS) clusters. It serves as a bridge\nbetween AI tools (like GitHub Copilot, Claude, and other MCP-compatible AI\nassistants) and AKS, translating natural language requests into AKS operations\nand returning the results in a format the AI tools can understand.\n\nIt allows AI tools to:\n\n- Operate (CRUD) AKS resources\n- Retrieve details related to AKS clusters (VNets, Subnets, NSGs, Route Tables, etc.)\n- Manage Azure Fleet operations for multi-cluster scenarios\n\n## How it works\n\nAKS-MCP connects to Azure using the Azure SDK and provides a set of tools that\nAI assistants can use to interact with AKS resources. It leverages the Model\nContext Protocol (MCP) to facilitate this communication, enabling AI tools to\nmake API calls to Azure and interpret the responses.\n\n## Azure CLI Authentication\n\nAKS-MCP uses Azure CLI (az) for AKS operations. Azure CLI authentication is attempted in this order:\n\n1. Service Principal (client secret): When `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID` environment variables are present, a service principal login is performed using the following command: `az login --service-principal -u CLIENT_ID -p CLIENT_SECRET --tenant TENANT_ID`\n\n1. Workload Identity (federated token): When `AZURE_CLIENT_ID`, `AZURE_TENANT_ID`, `AZURE_FEDERATED_TOKEN_FILE` environment variables are present, a federated token login is performed using the following command: `az login --service-principal -u CLIENT_ID --tenant TENANT_ID --federated-token TOKEN`\n\n1. User-assigned Managed Identity (managed identity client ID): When only `AZURE_CLIENT_ID` environment variable is present, a user-assigned managed identity login is performed using the following command: `az login --identity -u CLIENT_ID`\n\n1. System-assigned Managed Identity: When `AZURE_MANAGED_IDENTITY` is set to `system`, a system-assigned managed identity login is performed using the following command: `az login --identity`\n\n1. Existing Login: When none of the above environment variables are set, AKS-MCP assumes you have already authenticated (for example, via `az login`) and uses the existing session.\n\nOptional subscription selection:\n\n- If `AZURE_SUBSCRIPTION_ID` is set, AKS-MCP will run `az account set --subscription SUBSCRIPTION_ID` after login.\n\nNotes and security:\n\n- The federated token file must be exactly `/var/run/secrets/azure/tokens/azure-identity-token` and is strictly validated; other paths are rejected.\n- After each login, AKS-MCP verifies authentication with `az account show --query id -o tsv`.\n- Ensure the Azure CLI is installed and on PATH.\n\nEnvironment variables used:\n\n- `AZURE_TENANT_ID`\n- `AZURE_CLIENT_ID`\n- `AZURE_CLIENT_SECRET`\n- `AZURE_FEDERATED_TOKEN_FILE`\n- `AZURE_SUBSCRIPTION_ID`\n- `AZURE_MANAGED_IDENTITY` (set to `system` to opt into system-assigned managed identity)\n\n## Available Tools\n\nThe AKS-MCP server provides consolidated tools for interacting with AKS\nclusters. Some tools will require read-write or admin permissions to run debugging pods on your cluster. To enable read-write or admin permissions for the AKS-MCP server, add the **access level** parameter to your MCP configuration file:\n\n1. Navigate to your **mcp.json** file, or go to MCP: List Servers -\u003e AKS-MCP -\u003e Show Configuration Details in the **Command Palette** (`Ctrl+Shift+P` on Windows/Linux or `Cmd+Shift+P` on macOS).\n2. In the \"args\" section of AKS-MCP, add the following parameters: \"--access-level\", \"readwrite\" / \"admin\"\n\nFor example:\n```\n\"args\": [\n  \"--transport\",\n  \"stdio\",\n  \"--access-level\",\n  \"readwrite\"\n]\n```\n\nThese tools have been designed to provide comprehensive functionality\nthrough unified interfaces:\n\n\u003cdetails\u003e\n\u003csummary\u003eAKS Cluster Management\u003c/summary\u003e\n\n**Tool:** `az_aks_operations`\n\nUnified tool for managing Azure Kubernetes Service (AKS) clusters and related operations.\n\n**Available Operations:**\n\n- **Read-Only** (all access levels):\n  - `show`: Show cluster details\n  - `list`: List clusters in subscription/resource group\n  - `get-versions`: Get available Kubernetes versions\n  - `check-network`: Perform outbound network connectivity check\n  - `nodepool-list`: List node pools in cluster\n  - `nodepool-show`: Show node pool details\n  - `account-list`: List Azure subscriptions\n\n- **Read-Write** (`readwrite`/`admin` access levels):\n  - `create`: Create new cluster\n  - `delete`: Delete cluster\n  - `scale`: Scale cluster node count\n  - `start`: Start a stopped cluster\n  - `stop`: Stop a running cluster\n  - `update`: Update cluster configuration\n  - `upgrade`: Upgrade Kubernetes version\n  - `nodepool-add`: Add node pool to cluster\n  - `nodepool-delete`: Delete node pool\n  - `nodepool-scale`: Scale node pool\n  - `nodepool-upgrade`: Upgrade node pool\n  - `account-set`: Set active subscription\n  - `login`: Azure authentication\n\n- **Admin-Only** (`admin` access level):\n  - `get-credentials`: Get cluster credentials for kubectl access\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eNetwork Resource Management\u003c/summary\u003e\n\n**Tool:** `az_network_resources`\n\nUnified tool for getting Azure network resource information used by AKS clusters.\n\n**Available Resource Types:**\n\n- `all`: Get information about all network resources\n- `vnet`: Virtual Network information\n- `subnet`: Subnet information\n- `nsg`: Network Security Group information\n- `route_table`: Route Table information\n- `load_balancer`: Load Balancer information\n- `private_endpoint`: Private endpoint information\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eMonitoring and Diagnostics\u003c/summary\u003e\n\n**Tool:** `az_monitoring`\n\nUnified tool for Azure monitoring and diagnostics operations for AKS clusters.\n\n**Available Operations:**\n\n- `metrics`: List metric values for resources\n- `resource_health`: Retrieve resource health events for AKS clusters\n- `app_insights`: Execute KQL queries against Application Insights telemetry data\n- `diagnostics`: Check if AKS cluster has diagnostic settings configured\n- `control_plane_logs`: Query AKS control plane logs with safety constraints\n  and time range validation\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eCompute Resources\u003c/summary\u003e\n\n**Tool:** `get_aks_vmss_info`\n\n- Get detailed VMSS configuration for node pools in the AKS cluster\n\n**Tool:** `az_compute_operations`\n\nUnified tool for managing Azure Virtual Machines (VMs) and Virtual Machine Scale Sets (VMSS) used by AKS.\n\n**Available Operations:**\n\n- `show`: Get details of a VM/VMSS\n- `list`: List VMs/VMSS in subscription or resource group\n- `get-instance-view`: Get runtime status\n- `start`: Start VM\n- `stop`: Stop VM\n- `restart`: Restart VM/VMSS instances\n- `reimage`: Reimage VMSS instances (VM not supported for reimage)\n\n**Resource Types:** `vm` (single virtual machines), `vmss` (virtual machine scale sets)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eFleet Management\u003c/summary\u003e\n\n**Tool:** `az_fleet`\n\nComprehensive Azure Fleet management for multi-cluster scenarios.\n\n**Available Operations:**\n\n- **Fleet Operations**: list, show, create, update, delete, get-credentials\n- **Member Operations**: list, show, create, update, delete\n- **Update Run Operations**: list, show, create, start, stop, delete\n- **Update Strategy Operations**: list, show, create, delete\n- **ClusterResourcePlacement Operations**: list, show, get, create, delete\n\nSupports both Azure Fleet management and Kubernetes ClusterResourcePlacement\nCRD operations.\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eDiagnostic Detectors\u003c/summary\u003e\n\n**Tool:** `list_detectors`\n\n- List all available AKS cluster detectors\n\n**Tool:** `run_detector`\n\n- Run a specific AKS diagnostic detector\n\n**Tool:** `run_detectors_by_category`\n\n- Run all detectors in a specific category\n- **Categories**: Best Practices, Cluster and Control Plane Availability and\n  Performance, Connectivity Issues, Create/Upgrade/Delete and Scale,\n  Deprecations, Identity and Security, Node Health, Storage\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eAzure Advisor\u003c/summary\u003e\n\n**Tool:** `az_advisor_recommendation`\n\nRetrieve and manage Azure Advisor recommendations for AKS clusters.\n\n**Available Operations:**\n\n- `list`: List recommendations with filtering options\n- `report`: Generate recommendation reports\n- **Filter Options**: resource_group, cluster_names, category (Cost,\n  HighAvailability, Performance, Security), severity (High, Medium, Low)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eKubernetes Tools\u003c/summary\u003e\n\n*Note: kubectl commands are available with all access levels. Additional tools\nrequire explicit enablement via `--additional-tools`*\n\n**kubectl Tools (Unified Interface):**\n\n- **Read-Only** (all access levels):\n  - `kubectl_resources`: View resources (get, describe) - filtered to read-only operations in readonly mode\n  - `kubectl_diagnostics`: Debug and diagnose (logs, events, top, exec, cp)\n  - `kubectl_cluster`: Cluster information (cluster-info, api-resources, api-versions, explain)\n  - `kubectl_config`: Configuration management (diff, auth, config) - filtered to read-only operations in readonly mode\n\n- **Read-Write/Admin** (`readwrite`/`admin` access levels):\n  - `kubectl_resources`: Full resource management (get, describe, create, delete, apply, patch, replace, cordon, uncordon, drain, taint)\n  - `kubectl_workloads`: Workload lifecycle (run, expose, scale, autoscale, rollout)\n  - `kubectl_metadata`: Metadata management (label, annotate, set)\n  - `kubectl_config`: Full configuration management (diff, auth, certificate, config)\n\n**Additional Tools (Optional):**\n\n- `helm`: Helm package manager (requires `--additional-tools helm`)\n- `cilium`: Cilium CLI for eBPF networking (requires `--additional-tools cilium`)\n\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003eReal-time Observability\u003c/summary\u003e\n\n**Tool:** `inspektor_gadget_observability`\n\nReal-time observability tool for Azure Kubernetes Service (AKS) clusters using\neBPF.\n\n**Available Actions:**\n\n- `deploy`: Deploy Inspektor Gadget to cluster (requires `readwrite`/`admin` access)\n- `undeploy`: Remove Inspektor Gadget from cluster (requires `readwrite`/`admin` access)\n- `is_deployed`: Check deployment status\n- `run`: Run one-shot gadgets\n- `start`: Start continuous gadgets\n- `stop`: Stop running gadgets\n- `get_results`: Retrieve gadget results\n- `list_gadgets`: List available gadgets\n\n**Available Gadgets:**\n\n- `observe_dns`: Monitor DNS requests and responses\n- `observe_tcp`: Monitor TCP connections\n- `observe_file_open`: Monitor file system operations\n- `observe_process_execution`: Monitor process execution\n- `observe_signal`: Monitor signal delivery\n- `observe_system_calls`: Monitor system calls\n- `top_file`: Top files by I/O operations\n- `top_tcp`: Top TCP connections by traffic\n\n\u003c/details\u003e\n\n## How to install\n\n### Prerequisites\n\n1. Set up [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) and authenticate:\n\n   ```bash\n   az login\n   ```\n\n### VS Code with GitHub Copilot (Recommended)\n\n#### 🚀 One-Click Installation with the AKS Extension\n\nThe easiest way to get started with AKS-MCP is through the **Azure Kubernetes Service Extension for VS Code**.\n\n#### Step 1: Install the AKS Extension\n\n1. Open VS Code and go to Extensions (`Ctrl+Shift+X` on Windows/Linux or `Cmd+Shift+X` on macOS).\n1. Search for [Azure Kubernetes Service](https://marketplace.visualstudio.com/items?itemName=ms-kubernetes-tools.vscode-aks-tools).\n1. Install the official Microsoft AKS extension.\n\n#### Step 2: Launch the AKS-MCP Server\n\n1. Open the **Command Palette** (`Ctrl+Shift+P` on Windows/Linux or `Cmd+Shift+P` on macOS).\n2. Search and run: **AKS: Setup AKS MCP Server**.\n\nUpon successful installation, the server will now be visible in **MCP: List Servers** (via Command Palette). From there, you can start the MCP server or view its status.\n\n#### Step 3: Start Using AKS-MCP\n\nOnce started, the MCP server will appear in the **Copilot Chat: Configure Tools** dropdown under `MCP Server: AKS MCP`, ready to enhance contextual prompts based on your AKS environment. By default, all AKS-MCP server tools are enabled. You can review the list of available tools and disable any that are not required for your specific scenario.\n\nTry a prompt like *\"List all my AKS clusters\"*, which will start using tools from the AKS-MCP server.\n\n#### WSL Configuration\n\nThe MCP configuration differs depending on whether VS Code is running on Windows or inside WSL:\n\n**🪟 Windows Host (VS Code on Windows)**: Use `\"command\": \"wsl\"` to invoke the WSL binary from Windows:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"wsl\",\n      \"args\": [\n        \"--\",\n        \"/home/you/.vs-kubernetes/tools/aks-mcp/aks-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n**🐧 Remote-WSL (VS Code running inside WSL)**: Call the binary directly or use a shell wrapper:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"/home/you/.vs-kubernetes/tools/aks-mcp/aks-mcp --transport stdio\"\n      ]\n    }\n  }\n}\n```\n\n**🔧 Troubleshooting ENOENT Errors**\n\nIf you see \"spawn ENOENT\" errors, verify your VS Code environment:\n- **Windows host**: Check if the WSL binary path is correct and accessible via `wsl -- ls /path/to/aks-mcp`\n- **Remote-WSL**: Do NOT use `\"command\": \"wsl\"` - use direct paths or bash wrapper as shown above\n\n\n\u003e **💡 Benefits**: The AKS extension handles binary downloads, updates, and configuration automatically, ensuring you always have the latest version with optimal settings.\n\n### Alternative Installation Methods\n\n\u003cdetails\u003e\n\u003csummary\u003eManual Binary Installation\u003c/summary\u003e\n\n#### Step 1: Download the Binary\n\nChoose your platform and download the latest AKS-MCP binary:\n\n| Platform | Architecture | Download Link |\n|----------|-------------|---------------|\n| **Windows** | AMD64 | [📥 aks-mcp-windows-amd64.exe](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-amd64.exe) |\n| | ARM64 | [📥 aks-mcp-windows-arm64.exe](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-arm64.exe) |\n| **macOS** | Intel (AMD64) | [📥 aks-mcp-darwin-amd64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-darwin-amd64) |\n| | Apple Silicon (ARM64) | [📥 aks-mcp-darwin-arm64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-darwin-arm64) |\n| **Linux** | AMD64 | [📥 aks-mcp-linux-amd64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-amd64) |\n| | ARM64 | [📥 aks-mcp-linux-arm64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-arm64) |\n\n#### Step 2: Configure VS Code\n\nAfter downloading, create a `.vscode/mcp.json` file in your workspace root with the path to your downloaded binary.\n\n##### Option A: Automated Setup Script\n\nFor quick setup, you can use these one-liner scripts that download the binary\nand create the configuration:\n\n*Windows (PowerShell):*\n\n```powershell\n# Download binary and create VS Code configuration\nmkdir -p .vscode ; Invoke-WebRequest -Uri \"https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-amd64.exe\" -OutFile \"aks-mcp.exe\" ; @{servers=@{\"aks-mcp-server\"=@{type=\"stdio\";command=\"$PWD\\aks-mcp.exe\";args=@(\"--transport\",\"stdio\")}}} | ConvertTo-Json -Depth 3 | Out-File \".vscode/mcp.json\" -Encoding UTF8\n```\n\n*macOS/Linux (Bash):*\n\n```bash\n# Download binary and create VS Code configuration\nmkdir -p .vscode \u0026\u0026 curl -sL https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-amd64 -o aks-mcp \u0026\u0026 chmod +x aks-mcp \u0026\u0026 echo '{\"servers\":{\"aks-mcp-server\":{\"type\":\"stdio\",\"command\":\"'$PWD'/aks-mcp\",\"args\":[\"--transport\",\"stdio\"]}}}' \u003e .vscode/mcp.json\n```\n\n##### Option B: Manual Configuration\n\n\u003e **✨ Simple Setup**: Download the binary for your platform, then use the manual configuration below to set up the MCP server in VS Code.\n\n#### Manual VS Code Configuration\n\nYou can configure the AKS-MCP server in two ways:\n\n**1. Workspace-specific configuration** (recommended for project-specific usage):\n\nCreate a `.vscode/mcp.json` file in your workspace with the path to your downloaded binary:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"\u003center the file path\u003e\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n**2. User-level configuration** (persistent across all workspaces):\n\nFor a persistent configuration that works across all your VS Code workspaces, add the MCP server to your VS Code user settings:\n\n1. Open VS Code Settings (Ctrl+, or Cmd+,)\n2. Search for \"mcp\" in the settings\n3. Add the following to your User Settings JSON:\n\n```json\n{\n  \"github.copilot.chat.mcp.servers\": {\n    \"aks-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"\u003center the file path\u003e\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n#### Step 3: Load the AKS-MCP server tools to Github Copilot\n\n1. If running on an older version of VS Code: restart VS Code i.e. close and\n   reopen VS Code to load the new MCP server configuration.\n2. Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n3. Click the **Tools** button or run /list in the Github Copilot window to see the list of available tools\n4. You should see the AKS-MCP tools in the list\n5. Try a prompt like: *\"List all my AKS clusters in subscription xxx\"*\n6. The agent will automatically use AKS-MCP tools to complete your request\n\n\u003e **💡 Tip**: If you don't see the AKS-MCP tools after restarting, check the VS Code output panel for any MCP server connection errors and verify your binary path in `.vscode/mcp.json`.\n\n**Note**: Ensure you have authenticated with Azure CLI (`az login`) for the server to access your Azure resources.\n\n\u003c/details\u003e\n\n### Other MCP-Compatible Clients\n\n\u003cdetails\u003e\n\u003csummary\u003eDocker and Custom Client Installation\u003c/summary\u003e\n\nFor other MCP-compatible AI clients like [Claude Desktop](https://claude.ai/), configure the server in your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"command\": \"\u003cpath of binary aks-mcp\u003e\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n#### 🐳 Docker MCP Toolkit\n\nYou can enable the [AKS-MCP server directly from MCP Toolkit](https://hub.docker.com/mcp/server/aks/overview):\n\n1. Open Docker Desktop\n2. Click \"MCP Toolkit\" in the left sidebar\n3. Search for \"aks\" in Catalog tab\n4. Click on the AKS-MCP server card\n5. Enable the server by clicking \"+\" in the top right corner\n6. Configure the server using \"Configuration\" tab:\n   - **azure_dir** `[REQUIRED]`: Path to your Azure credentials directory e.g `/home/user/.azure` (must be absolute – without `$HOME` or `~`)\n   - **kubeconfig** `[REQUIRED]`: Path to your kubeconfig file e.g `/home/user/.kube/config` (must be absolute – without `$HOME` or `~`)\n   - **access_level** `[REQUIRED]`: Set to `readonly`, `readwrite`, or `admin` as needed\n   - **container_user** `[OPTIONAL]`: Username or UID to run the container as (default is `mcp`), e.g. use `1000` to match your host user ID (see note below). Only needed if you are using docker engine on Linux.\n7. You are now ready to use the AKS-MCP server with your [preferred MCP client](https://hub.docker.com/mcp/server/aks/manual), see an example [here](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/#install-an-mcp-client). (requires `\u003e= v0.16.0` for MCP gateway)\n\n\u003e **Note**: When running the MCP gateway using Docker Engine, you have to set the `container_user` to match your host user ID (e.g using `id -u`) to ensure proper file permissions for accessing mounted volumes.\n\u003e On Docker Desktop, this is handled automatically if you use `desktop-*` contexts, confirmed by running `docker context ls`.\n\nOn **Windows**, the Azure credentials won't work by default, but you have two options:\n\n1. **Long-lived servers**: Configure the [MCP gateway](https://docs.docker.com/ai/mcp-gateway/) to use long-lived servers using `--long-lived` flag and then authenticate with Azure CLI in the container, see option B in Containerized MCP configuration below on how to fetch credentials inside the container. \n2. **Custom Azure Directory**: Set up a custom Azure directory:\n    ```powershell\n    # Set custom Azure config directory\n    $env:AZURE_CONFIG_DIR = \"$env:USERPROFILE\\.azure-for-docker\"\n    \n    # Disable token cache encryption (to match behavior with Linux/macOS)\n    $env:AZURE_CORE_ENCRYPT_TOKEN_CACHE = \"false\"\n    \n    # Login to Azure CLI\n    az login\n    ```\n\n   This will store the credentials in `$env:USERPROFILE\\.azure-for-docker` (e.g. `C:\\Users\\\u003cusername\u003e\\.azure-for-docker`),\n   use this path in the AKS-MCP server configuration `azure_dir`.\n\nYou can also use the [MCP Gateway](https://docs.docker.com/ai/mcp-gateway/) to enable the AKS-MCP server directly using:\n\n```bash\n# Enable AKS-MCP server in Docker MCP Gateway\ndocker mcp server enable aks\n```\n\nNote: You still need to configure the server (e.g. using `docker mcp config`) with your Azure credentials, kubeconfig file, and access level.\n\n#### 🐋 Containerized MCP configuration\n\nFor containerized deployment, you can run AKS-MCP server using the official Docker image:\n\nOption A: Mount credentials from host (recommended):\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"--user\",\n          \"\u003cyour-user-id (e.g. id -u)\u003e\",\n          \"-v\",\n          \"~/.azure:/home/mcp/.azure\",\n          \"-v\",\n          \"~/.kube:/home/mcp/.kube\",\n          \"ghcr.io/azure/aks-mcp:latest\",\n          \"--transport\",\n          \"stdio\"\n        ]\n    }\n  }\n}\n```\n\nOption B: fetch the credentials inside the container:\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"ghcr.io/azure/aks-mcp:latest\",\n          \"--transport\",\n          \"stdio\"\n        ]\n    }\n  }\n}\n```\n\nStart the MCP server container first per above command, and then run the following commands to fetch the credentials:\n- Login to Azure CLI: `docker exec -it \u003ccontainer-id\u003e az login --use-device-code`\n- Get kubeconfig: `docker exec -it \u003ccontainer-id\u003e az aks get-credentials -g \u003cresource-group\u003e -n \u003ccluster-name\u003e`\n\nNote that:\n\n- Host Azure CLI logins don’t automatically propagate into containers without mounting `~/.azure`.\n- User ID should be set for option A, orelse the mcp user inside container won't be able to access the mounted files.\n\n### 🤖 Custom MCP Client Installation\n\nYou can configure any MCP-compatible client to use the AKS-MCP server by running the binary directly:\n\n```bash\n# Run the server directly\n./aks-mcp --transport stdio\n```\n\n### 🔧 Manual Binary Installation\n\nFor direct binary usage without package managers:\n\n1. Download the latest release from the [releases page](https://github.com/Azure/aks-mcp/releases)\n2. Extract the binary to your preferred location\n3. Make it executable (on Unix systems):\n   ```bash\n   chmod +x aks-mcp\n   ```\n4. Configure your MCP client to use the binary path\n\n\u003c/details\u003e\n\n### Options\n\nCommand line arguments:\n\n```sh\nUsage of ./aks-mcp:\n      --access-level string       Access level (readonly, readwrite, admin) (default \"readonly\")\n      --additional-tools string   Comma-separated list of additional Kubernetes tools to support (kubectl is always enabled). Available: helm,cilium,hubble\n      --allow-namespaces string   Comma-separated list of allowed Kubernetes namespaces (empty means all namespaces)\n      --host string               Host to listen for the server (only used with transport sse or streamable-http) (default \"127.0.0.1\")\n      --otlp-endpoint string      OTLP endpoint for OpenTelemetry traces (e.g. localhost:4317, default \"\")\n      --port int                  Port to listen for the server (only used with transport sse or streamable-http) (default 8000)\n      --timeout int               Timeout for command execution in seconds, default is 600s (default 600)\n      --transport string          Transport mechanism to use (stdio, sse or streamable-http) (default \"stdio\")\n      --log-level string          Log level (debug, info, warn, error) (default \"info\")\n```\n\n**Environment variables:**\n- Standard Azure authentication environment variables are supported (`AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_SUBSCRIPTION_ID`)\n\n## Development\n\n### Prerequisites\n\n- **Go** ≥ `1.24.x` installed on your local machine\n- **Bash** available as `/usr/bin/env bash` (Makefile targets use multi-line recipes with fail-fast mode)\n- **GNU Make** `4.x` or later\n- **Docker** *(optional, for container builds and testing)*\n\n\u003e **Note:** If your login shell is different (e.g., `zsh` on **macOS**), you do **not** need to change it — the Makefile sets variables to run all recipes in `bash` for consistent behavior across platforms.\n\n### Building from Source\n\nThis project includes a Makefile for convenient development, building, and testing. To see all available targets:\n\n```bash\nmake help\n```\n\n#### Quick Start\n\n```bash\n# Build the binary\nmake build\n\n# Run tests\nmake test\n\n# Run tests with coverage\nmake test-coverage\n\n# Format and lint code\nmake check\n\n# Build for all platforms\nmake release\n```\n\n#### Common Development Tasks\n\n```bash\n# Install dependencies\nmake deps\n\n# Build and run with --help\nmake run\n\n# Clean build artifacts\nmake clean\n\n# Install binary to GOBIN\nmake install\n```\n\n#### Docker\n\n```bash\n# Build Docker image\nmake docker-build\n\n# Run Docker container\nmake docker-run\n```\n\n### Manual Build\n\nIf you prefer to build without the Makefile:\n\n```bash\ngo build -o aks-mcp ./cmd/aks-mcp\n```\n\n## Usage\n\nAsk any questions about your AKS clusters in your AI client, for example:\n\n```\nList all my AKS clusters in my subscription xxx.\n\nWhat is the network configuration of my AKS cluster?\n\nShow me the network security groups associated with my cluster.\n\nCreate a new Azure Fleet named prod-fleet in eastus region.\n\nList all members in my fleet.\n\nCreate a placement to deploy nginx workloads to clusters with app=frontend label.\n\nShow me all ClusterResourcePlacements in my fleet.\n```\n\n## Telemetry\n\nTelemetry collection is on by default.\n\nTo opt out, set the environment variable `AKS_MCP_COLLECT_TELEMETRY=false`.\n\n## Contributing\n\nWe welcome contributions to AKS-MCP! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n**📖 [Read our detailed Contributing Guide](CONTRIBUTING.md)** for comprehensive information on:\n\n- Setting up your development environment\n- Running AKS-MCP locally and testing with AI agents\n- Understanding the codebase architecture\n- Adding new MCP tools and features\n- Testing guidelines and best practices\n- Submitting pull requests\n\n### Quick Start for Contributors\n\n1. **Prerequisites**: Go ≥ 1.24.x, Azure CLI, Git\n2. **Setup**: Fork the repo, clone locally, run `make deps \u0026\u0026 make build`\n3. **Test**: Run `make test` and `make check`\n4. **Develop**: Follow the component-based architecture in [CONTRIBUTING.md](CONTRIBUTING.md)\n\n### Contributor License Agreement\n\nMost contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark \u0026 Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n","stargazer_count":95,"topics":["kubernetes","mcp-server","model-context-protocol"],"uses_custom_opengraph_image":false}}}},{"name":"webflow/mcp-server","description":"Enable AI agents to interact with Webflow APIs.","status":"active","repository":{"url":"https://github.com/webflow/mcp-server","source":"github","id":"950272413","readme":"# Webflow's MCP server\n\nA Node.js server implementing Model Context Protocol (MCP) for Webflow using the [Webflow JavaScript SDK](https://github.com/webflow/js-webflow-api). Enable AI agents to interact with Webflow APIs. Learn more about Webflow's Data API in the [developer documentation](https://developers.webflow.com/data/reference).\n\n[![npm shield](https://img.shields.io/npm/v/webflow-mcp-server)](https://www.npmjs.com/package/webflow-mcp-server)\n![Webflow](https://img.shields.io/badge/webflow-%23146EF5.svg?style=for-the-badge\u0026logo=webflow\u0026logoColor=white)\n\n## Prerequisites\n\n- [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [A Webflow Account](https://webflow.com/signup)\n\n## 🚀 Remote installation\n\nGet started by installing Webflow's remote MCP server. The remote server uses OAuth to authenticate with your Webflow sites, and a companion app that syncs your live canvas with your AI agent.\n\n### Requirements\n\n- Node.js 22.3.0 or higher\n\n\u003e Note: The MCP server currently supports Node.js 22.3.0 or higher. If you run into version issues, see the [Node.js compatibility guidance.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n### Cursor\n\n#### Add MCP server to Cursor\n\n1. Go to `Settings → Cursor Settings → MCP \u0026 Integrations`.\n2. Under MCP Tools, click `+ New MCP Server`.\n3. Paste the following configuration into `.cursor/mcp.json` (or add the `webflow` part to your existing configuration):\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"url\": \"https://mcp.webflow.com/sse\"\n    }\n  }\n}\n```\n\n\u003e Tip: You can create a project-level `mcp.json` to avoid repeated auth prompts across multiple Cursor windows. See Cursor’s docs on [configuration locations.](https://docs.cursor.com/en/context/mcp#configuration-locations)\n\n4. Save and close the file. Cursor will automatically open an OAuth login page where you can authorize Webflow sites to use with the MCP server.\n\n#### Open the Webflow Designer\n\n- Open your site in the Webflow Designer, or ask your AI agent:\n\n```text\nGive me a link to open \u003cMY_SITE_NAME\u003e in the Webflow Designer\n```\n\n#### Open the MCP Webflow App\n\n1. In the Designer, open the Apps panel (press `E`).\n2. Launch your published \"Webflow MCP Bridge App\".\n3. Wait for the app to connect to the MCP server.\n\n#### Write your first prompt\n\nTry these in your AI chat:\n\n```text\nAnalyze my last 5 blog posts and suggest 3 new topic ideas with SEO keywords\n```\n\n```text\nFind older blog posts that mention similar topics and add internal links to my latest post\n```\n\n```text\nCreate a hero section card on my home page with a CTA button and responsive design\n```\n\n### Claude desktop\n\n#### Add MCP server to Claude desktop\n\n1. Enable developer mode: `Help → Troubleshooting → Enable Developer Mode`.\n2. Open developer settings: `File → Settings → Developer`.\n3. Click `Get Started` or edit the configuration to open `claude_desktop_config.json` and add:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.webflow.com/sse\"]\n    }\n  }\n}\n```\n\n4. Save and restart Claude Desktop (`Cmd/Ctrl + R`). An OAuth login page will open to authorize sites.\n\n#### Open the Webflow Designer\n\n- Open your site in the Webflow Designer, or ask your AI agent:\n\n```text\nGive me a link to open \u003cMY_SITE_NAME\u003e in the Webflow Designer\n```\n\n#### Open the MCP Webflow App\n\n1. In the Designer, open the Apps panel (press `E`).\n2. Launch your published \"Webflow MCP Bridge App\".\n3. Wait for the app to connect to the MCP server.\n\n#### Write your first prompt\n\n```text\nAnalyze my last 5 blog posts and suggest 3 new topic ideas with SEO keywords\n```\n\n```text\nFind older blog posts that mention similar topics and add internal links to my latest post\n```\n\n```text\nCreate a hero section card on my home page with a CTA button and responsive design\n```\n\n### Reset your OAuth token\n\nTo reset your OAuth token, run the following command in your terminal.\n\n```bash\nrm -rf ~/.mcp-auth\n```\n\n### Node.js compatibility\n\nPlease see the Node.js [compatibility guidance on Webflow's developer docs.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n---\n\n\n## Local Installation\n\nYou can also configure the MCP server to run locally. This requires:\n\n- Creating and registering your own MCP Bridge App in a Webflow workspace with Admin permissions\n- Configuring your AI client to start the local MCP server with a Webflow API token\n\n### 1. Create and publish the MCP bridge app\n\nBefore connecting the local MCP server to your AI client, you must create and publish the **Webflow MCP Bridge App** in your workspace.\n\n### Steps\n\n1. **Register a Webflow App**\n   - Go to your Webflow Workspace and register a new app.  \n   - Follow the official guide: [Register an App](https://developers.webflow.com/data/v2.0.0/docs/register-an-app).\n\n2. **Get the MCP Bridge App code**\n   - Option A: Download the latest `bundle.zip` from the [releases page](https://github.com/virat21/webflow-mcp-bridge-app/releases).\n   - Option B: Clone the repository and build it:\n     ```bash\n     git clone https://github.com/virat21/webflow-mcp-bridge-app\n     cd webflow-mcp-bridge-app\n     ```\n     - Then build the project following the repository instructions.\n\n3. **Publish the Designer Extension**\n   - Go to **Webflow Dashboard → Workspace settings → Apps \u0026 Integrations → Develop → Your App**.\n   - Click **“Publish Extension Version”**.\n   - Upload your built `bundle.zip` file.\n\n4. **Open the App in Designer**\n   - Once published, open the MCP Bridge App from the **Designer → Apps panel** in a site within your workspace.\n\n### 2. Configure your AI client\n\n#### Cursor\n\nAdd to `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webflow-mcp-server@latest\"],\n      \"env\": {\n        \"WEBFLOW_TOKEN\": \"\u003cYOUR_WEBFLOW_TOKEN\u003e\"\n      }\n    }\n  }\n}\n```\n\n#### Claude desktop\n\nAdd to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webflow-mcp-server@latest\"],\n      \"env\": {\n        \"WEBFLOW_TOKEN\": \"\u003cYOUR_WEBFLOW_TOKEN\u003e\"\n      }\n    }\n  }\n}\n```\n\n### 3. Use the MCP server with the Webflow Designer\n\n- Open your site in the Webflow Designer.\n- Open the Apps panel (press `E`) and launch your published “Webflow MCP Bridge App”.\n- Wait for the app to connect to the MCP server, then use tools from your AI client.\n- If the Bridge App prompts for a local connection URL, call the `get_designer_app_connection_info` tool from your AI client and paste the returned `http://localhost:\u003cport\u003e` URL.\n\n### Optional: Run locally via shell\n\n```bash\nWEBFLOW_TOKEN=\"\u003cYOUR_WEBFLOW_TOKEN\u003e\" npx -y webflow-mcp-server@latest\n```\n\n```powershell\n# PowerShell\n$env:WEBFLOW_TOKEN=\"\u003cYOUR_WEBFLOW_TOKEN\u003e\"\nnpx -y webflow-mcp-server@latest\n```\n\n### Reset your OAuth Token\n\nTo reset your OAuth token, run the following command in your terminal.\n\n```bash\nrm -rf ~/.mcp-auth\n```\n\n### Node.js compatibility\n\nPlease see the Node.js [compatibility guidance on Webflow's developer docs.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n## ❓ Troubleshooting\n\nIf you are having issues starting the server in your MCP client e.g. Cursor or Claude Desktop, please try the following.\n\n### Make sure you have a valid Webflow API token\n\n1. Go to [Webflow's API Playground](https://developers.webflow.com/data/reference/token/authorized-by), log in and generate a token, then copy the token from the Request Generator\n2. Replace `YOUR_WEBFLOW_TOKEN` in your MCP client configuration with the token you copied\n3. Save and **restart** your MCP client\n\n### Make sure you have the Node and NPM installed\n\n- [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n\nRun the following commands to confirm you have Node and NPM installed:\n\n```shell\nnode -v\nnpm -v\n```\n\n### Clear your NPM cache\n\nSometimes clearing your [NPM cache](https://docs.npmjs.com/cli/v8/commands/npm-cache) can resolve issues with `npx`.\n\n```shell\nnpm cache clean --force\n```\n\n### Fix NPM global package permissions\n\nIf `npm -v` doesn't work for you but `sudo npm -v` does, you may need to fix NPM global package permissions. See the official [NPM docs](https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally) for more information.\n\nNote: if you are making changes to your shell configuration, you may need to restart your shell for changes to take effect.\n\n## 🛠️ Available tools\n\nSee the `./tools` directory for a list of available tools\n\n# 🗣️ Prompts \u0026 resources\n\nThis implementation **doesn't** include `prompts` or `resources` from the MCP specification. However, this may change in the future when there is broader support across popular MCP clients.\n\n## 📄 Webflow developer resources\n\n- [Webflow API Documentation](https://developers.webflow.com/data/reference)\n- [Webflow JavaScript SDK](https://github.com/webflow/js-webflow-api)\n\n## ⚠️ Known limitations\n\n### Static page content updates\n\nThe `pages_update_static_content` endpoint currently only supports updates to localized static pages in secondary locales. Updates to static content in the default locale aren't supported and will result in errors.\n"},"version":"1.0.0","created_at":"2025-09-23T17:41:25Z","updated_at":"2025-10-22T11:21:39Z","$schema":"https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json","remotes":[{"transport_type":"sse","url":"https://mcp.webflow.com/sse"}],"_meta":{"io.modelcontextprotocol.registry/official":{"id":"d74444a2-c698-42ee-856c-03da0d434343","is_latest":true,"published_at":"2025-08-22T00:00:00Z","updated_at":"2025-08-22T00:00:00Z"},"io.modelcontextprotocol.registry/publisher-provided":{"github":{"display_name":"Webflow","is_in_organization":true,"license":"MIT License","name":"mcp-server","name_with_owner":"webflow/mcp-server","opengraph_image_url":"https://opengraph.githubassets.com/7b4eb720b5973714a91eba1350636470d5f280268f2804bb3d001fbc9f2cfa5e/webflow/mcp-server","owner_avatar_url":"https://avatars.githubusercontent.com/u/1229663?v=4","preferred_image":"https://avatars.githubusercontent.com/u/1229663?v=4","primary_language":"TypeScript","primary_language_color":"#3178c6","pushed_at":"2025-10-17T15:21:48Z","readme":"# Webflow's MCP server\n\nA Node.js server implementing Model Context Protocol (MCP) for Webflow using the [Webflow JavaScript SDK](https://github.com/webflow/js-webflow-api). Enable AI agents to interact with Webflow APIs. Learn more about Webflow's Data API in the [developer documentation](https://developers.webflow.com/data/reference).\n\n[![npm shield](https://img.shields.io/npm/v/webflow-mcp-server)](https://www.npmjs.com/package/webflow-mcp-server)\n![Webflow](https://img.shields.io/badge/webflow-%23146EF5.svg?style=for-the-badge\u0026logo=webflow\u0026logoColor=white)\n\n## Prerequisites\n\n- [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [A Webflow Account](https://webflow.com/signup)\n\n## 🚀 Remote installation\n\nGet started by installing Webflow's remote MCP server. The remote server uses OAuth to authenticate with your Webflow sites, and a companion app that syncs your live canvas with your AI agent.\n\n### Requirements\n\n- Node.js 22.3.0 or higher\n\n\u003e Note: The MCP server currently supports Node.js 22.3.0 or higher. If you run into version issues, see the [Node.js compatibility guidance.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n### Cursor\n\n#### Add MCP server to Cursor\n\n1. Go to `Settings → Cursor Settings → MCP \u0026 Integrations`.\n2. Under MCP Tools, click `+ New MCP Server`.\n3. Paste the following configuration into `.cursor/mcp.json` (or add the `webflow` part to your existing configuration):\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"url\": \"https://mcp.webflow.com/sse\"\n    }\n  }\n}\n```\n\n\u003e Tip: You can create a project-level `mcp.json` to avoid repeated auth prompts across multiple Cursor windows. See Cursor’s docs on [configuration locations.](https://docs.cursor.com/en/context/mcp#configuration-locations)\n\n4. Save and close the file. Cursor will automatically open an OAuth login page where you can authorize Webflow sites to use with the MCP server.\n\n#### Open the Webflow Designer\n\n- Open your site in the Webflow Designer, or ask your AI agent:\n\n```text\nGive me a link to open \u003cMY_SITE_NAME\u003e in the Webflow Designer\n```\n\n#### Open the MCP Webflow App\n\n1. In the Designer, open the Apps panel (press `E`).\n2. Launch your published \"Webflow MCP Bridge App\".\n3. Wait for the app to connect to the MCP server.\n\n#### Write your first prompt\n\nTry these in your AI chat:\n\n```text\nAnalyze my last 5 blog posts and suggest 3 new topic ideas with SEO keywords\n```\n\n```text\nFind older blog posts that mention similar topics and add internal links to my latest post\n```\n\n```text\nCreate a hero section card on my home page with a CTA button and responsive design\n```\n\n### Claude desktop\n\n#### Add MCP server to Claude desktop\n\n1. Enable developer mode: `Help → Troubleshooting → Enable Developer Mode`.\n2. Open developer settings: `File → Settings → Developer`.\n3. Click `Get Started` or edit the configuration to open `claude_desktop_config.json` and add:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.webflow.com/sse\"]\n    }\n  }\n}\n```\n\n4. Save and restart Claude Desktop (`Cmd/Ctrl + R`). An OAuth login page will open to authorize sites.\n\n#### Open the Webflow Designer\n\n- Open your site in the Webflow Designer, or ask your AI agent:\n\n```text\nGive me a link to open \u003cMY_SITE_NAME\u003e in the Webflow Designer\n```\n\n#### Open the MCP Webflow App\n\n1. In the Designer, open the Apps panel (press `E`).\n2. Launch your published \"Webflow MCP Bridge App\".\n3. Wait for the app to connect to the MCP server.\n\n#### Write your first prompt\n\n```text\nAnalyze my last 5 blog posts and suggest 3 new topic ideas with SEO keywords\n```\n\n```text\nFind older blog posts that mention similar topics and add internal links to my latest post\n```\n\n```text\nCreate a hero section card on my home page with a CTA button and responsive design\n```\n\n### Reset your OAuth token\n\nTo reset your OAuth token, run the following command in your terminal.\n\n```bash\nrm -rf ~/.mcp-auth\n```\n\n### Node.js compatibility\n\nPlease see the Node.js [compatibility guidance on Webflow's developer docs.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n---\n\n\n## Local Installation\n\nYou can also configure the MCP server to run locally. This requires:\n\n- Creating and registering your own MCP Bridge App in a Webflow workspace with Admin permissions\n- Configuring your AI client to start the local MCP server with a Webflow API token\n\n### 1. Create and publish the MCP bridge app\n\nBefore connecting the local MCP server to your AI client, you must create and publish the **Webflow MCP Bridge App** in your workspace.\n\n### Steps\n\n1. **Register a Webflow App**\n   - Go to your Webflow Workspace and register a new app.  \n   - Follow the official guide: [Register an App](https://developers.webflow.com/data/v2.0.0/docs/register-an-app).\n\n2. **Get the MCP Bridge App code**\n   - Option A: Download the latest `bundle.zip` from the [releases page](https://github.com/virat21/webflow-mcp-bridge-app/releases).\n   - Option B: Clone the repository and build it:\n     ```bash\n     git clone https://github.com/virat21/webflow-mcp-bridge-app\n     cd webflow-mcp-bridge-app\n     ```\n     - Then build the project following the repository instructions.\n\n3. **Publish the Designer Extension**\n   - Go to **Webflow Dashboard → Workspace settings → Apps \u0026 Integrations → Develop → Your App**.\n   - Click **“Publish Extension Version”**.\n   - Upload your built `bundle.zip` file.\n\n4. **Open the App in Designer**\n   - Once published, open the MCP Bridge App from the **Designer → Apps panel** in a site within your workspace.\n\n### 2. Configure your AI client\n\n#### Cursor\n\nAdd to `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webflow-mcp-server@latest\"],\n      \"env\": {\n        \"WEBFLOW_TOKEN\": \"\u003cYOUR_WEBFLOW_TOKEN\u003e\"\n      }\n    }\n  }\n}\n```\n\n#### Claude desktop\n\nAdd to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webflow-mcp-server@latest\"],\n      \"env\": {\n        \"WEBFLOW_TOKEN\": \"\u003cYOUR_WEBFLOW_TOKEN\u003e\"\n      }\n    }\n  }\n}\n```\n\n### 3. Use the MCP server with the Webflow Designer\n\n- Open your site in the Webflow Designer.\n- Open the Apps panel (press `E`) and launch your published “Webflow MCP Bridge App”.\n- Wait for the app to connect to the MCP server, then use tools from your AI client.\n- If the Bridge App prompts for a local connection URL, call the `get_designer_app_connection_info` tool from your AI client and paste the returned `http://localhost:\u003cport\u003e` URL.\n\n### Optional: Run locally via shell\n\n```bash\nWEBFLOW_TOKEN=\"\u003cYOUR_WEBFLOW_TOKEN\u003e\" npx -y webflow-mcp-server@latest\n```\n\n```powershell\n# PowerShell\n$env:WEBFLOW_TOKEN=\"\u003cYOUR_WEBFLOW_TOKEN\u003e\"\nnpx -y webflow-mcp-server@latest\n```\n\n### Reset your OAuth Token\n\nTo reset your OAuth token, run the following command in your terminal.\n\n```bash\nrm -rf ~/.mcp-auth\n```\n\n### Node.js compatibility\n\nPlease see the Node.js [compatibility guidance on Webflow's developer docs.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n## ❓ Troubleshooting\n\nIf you are having issues starting the server in your MCP client e.g. Cursor or Claude Desktop, please try the following.\n\n### Make sure you have a valid Webflow API token\n\n1. Go to [Webflow's API Playground](https://developers.webflow.com/data/reference/token/authorized-by), log in and generate a token, then copy the token from the Request Generator\n2. Replace `YOUR_WEBFLOW_TOKEN` in your MCP client configuration with the token you copied\n3. Save and **restart** your MCP client\n\n### Make sure you have the Node and NPM installed\n\n- [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n\nRun the following commands to confirm you have Node and NPM installed:\n\n```shell\nnode -v\nnpm -v\n```\n\n### Clear your NPM cache\n\nSometimes clearing your [NPM cache](https://docs.npmjs.com/cli/v8/commands/npm-cache) can resolve issues with `npx`.\n\n```shell\nnpm cache clean --force\n```\n\n### Fix NPM global package permissions\n\nIf `npm -v` doesn't work for you but `sudo npm -v` does, you may need to fix NPM global package permissions. See the official [NPM docs](https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally) for more information.\n\nNote: if you are making changes to your shell configuration, you may need to restart your shell for changes to take effect.\n\n## 🛠️ Available tools\n\nSee the `./tools` directory for a list of available tools\n\n# 🗣️ Prompts \u0026 resources\n\nThis implementation **doesn't** include `prompts` or `resources` from the MCP specification. However, this may change in the future when there is broader support across popular MCP clients.\n\n## 📄 Webflow developer resources\n\n- [Webflow API Documentation](https://developers.webflow.com/data/reference)\n- [Webflow JavaScript SDK](https://github.com/webflow/js-webflow-api)\n\n## ⚠️ Known limitations\n\n### Static page content updates\n\nThe `pages_update_static_content` endpoint currently only supports updates to localized static pages in secondary locales. Updates to static content in the default locale aren't supported and will result in errors.\n","stargazer_count":84,"topics":["built-with-fern","generated-from-openapi","mcp-server","model-context-protocol","business-critical-yes"],"uses_custom_opengraph_image":false}}}}],"metadata":{"count":44,"next_cursor":"d74444a2-c698-42ee-856c-03da0d434343","total_pages":2}}
